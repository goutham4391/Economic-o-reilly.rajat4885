<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"

	>

<channel>
	<title>Commentary &#8211; Radar</title>
	<atom:link href="https://www.oreilly.com/radar/tag/commentary/feed/" rel="self" type="application/rss+xml" />
	<link>https://www.oreilly.com/radar</link>
	<description>Now, next, and beyond: Tracking need-to-know trends at the intersection of business and technology</description>
	<lastBuildDate>Tue, 13 Sep 2022 17:18:46 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.3.13</generator>
	<item>
		<title>The Problem with Intelligence</title>
		<link>https://www.oreilly.com/radar/the-problem-with-intelligence/</link>
				<comments>https://www.oreilly.com/radar/the-problem-with-intelligence/#respond</comments>
				<pubDate>Tue, 13 Sep 2022 11:21:40 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides]]></dc:creator>
				<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Commentary]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=14718</guid>
				<description><![CDATA[Projects like OpenAI’s DALL-E and DeepMind’s Gato and LaMDA have stirred up many discussions of artificial general intelligence (AGI). These discussions tend not to go anywhere, largely because we don’t really know what intelligence is. We have some ideas–I’ve suggested that intelligence and consciousness are deeply connected to the ability to disobey, and others have [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>Projects like OpenAI’s DALL-E and DeepMind’s Gato and LaMDA have stirred up many discussions of artificial general intelligence (AGI). These discussions tend not to go anywhere, largely because we don’t really know what intelligence is. We have some ideas–I’ve suggested that intelligence and consciousness are deeply connected to <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.oreilly.com/radar/intelligence-and-comprehension/" target="_blank">the ability to disobey</a>, and others have suggested that intelligence can’t exist outside of embodiment (some sort of connection between the intelligence and the physical world). But we really don’t have a definition. We have a lot of partial definitions, all of which are bound to specific contexts.</p>



<p>For example, we often say that dogs are intelligent. But what do we mean by that? Some dogs, like sheep dogs, are very good at performing certain tasks. Most dogs can be trained to sit, fetch, and do other things. And they can disobey. The same is true of children, though we’d never compare a child’s intelligence to a dog’s. And cats won’t do any of those things, though we never refer to cats as unintelligent.</p>



<p>I’m very impressed with <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://alexfoundation.org/about/dr-irene-pepperberg/" target="_blank">Irene Pepperberg’s work on parrot intelligence</a>. She’s shown that her parrots can have an understanding of numbers, can use language intelligently, and can even invent new vocabulary. (“Banerry” for apple, probably because birds don’t have lips and can’t say Ps very well. And apples look like giant cherries and taste like bananas, at least to parrots.) But I wonder if even this is getting the question wrong. (I think Dr. Pepperberg would agree.) We ask birds to be intelligent about things humans are intelligent about. We never ask humans to be intelligent about things birds are intelligent about: navigating in three-dimensional space, storing food for use during winter (a boreal chickadee will <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://knowablemagazine.org/article/mind/2019/chickadee-memory-food" target="_blank">store as many as 80,000 seeds</a> in different places, and remember where they’re all located), making use of the many colors birds see that we can’t (<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://academic.oup.com/bioscience/article/50/10/854/233996" target="_blank">their vision extends well into the ultraviolet</a>). It’s easy to imagine a bird thinking, “Those poor humans. They can’t find their home without taking out that strange little black box (which is actually colored octarine).”</p>



<p>In a similar vein, we often say that dolphins and elephants are intelligent, but it’s never clear what exactly we mean by that. We’ve demonstrated that dolphins can recognize patterns and that they recognize themselves in mirrors, and they’ve demonstrated a (limited) ability to communicate with humans, but their intelligence certainly goes much further. I wouldn’t be the least bit surprised if animals like dolphins had an oral literature. We penalize them on the intelligence scale because they don’t have hands and can’t pick up a pen. Likewise, some research shows that <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.sciencedaily.com/releases/2012/08/120803103421.htm" target="_blank">elephants communicate</a> with each other using low frequency rumbles that can be heard for miles (if you’re an elephant). Information theory suggests that this communication can’t be fast, but that doesn’t mean that it can’t be rich.</p>



<p>Humans are intelligent. After all, we get to define what “intelligence” means. Controlling the definition of intelligence has always been a source of cultural and political power; just read anything written in America in the 19th century about the intelligence of women, Asians, Africans, or even the Irish and Italians. We have “intelligence tests” to measure intelligence–or do they just measure test-taking ability? We also talk about “emotional” and other kinds of intelligence. And we recognize that mathematical, linguistic, and artistic ability rarely go hand-in-hand. Our own view of our own intelligence is highly fractured, and often has more to do with pseudo-science than anything we could use as a metric in machine learning experiments. (Though GPT-3 and LaMDA are no doubt very good at taking tests.) </p>



<p>Finally, there’s also been a lot of talk recently about the possibility of discovering life on other planets. Life is one thing, and my decidedly amateur opinion is that we will find life fairly common. However, to discover intelligent life, we would need a working definition of intelligence. The only useful definition I can imagine is “able to generate signals that can be received off planet and that are indisputably non-natural.” But by that definition, humans have only been intelligent for roughly 100 years, since the early days of radio. (I’m not convinced that the early electrical experiments from the 19th century and spark-based radio from the first two decades of the 20th century could be detected off planet.) There may be fantastically <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Life_on_Titan" target="_blank">intelligent creatures living under the ice covering Saturn’s moon Titan</a>, but we’ll never be able to detect them without going there. For Titan, a visit may be possible. For planets elsewhere in our galaxy, probably not.</p>



<p>Even more important: these definitions aren’t just different. They’re different in kind. We’re not saying that a parrot or a crow is intelligent if it scores 0.3 (on a scale of 0 to 1) on some test, but an autonomous vehicle has to score .99. The definitions aren’t remotely comparable. I don’t know what it would mean to ask GPT-3 about soaring on air currents. If we asked, we would get an answer, and quite likely a good one with a lot of information about aerodynamics, but would that have anything to do with an eagle’s understanding of flight? I could tell Gato to “sit,” but how would I know if it complied?</p>



<p>So what does this tell us about intelligence that’s artificial? Context is important; an appropriate definition of “intelligence” has to start with what we want the system to do. In some cases, that’s generating publishable papers and good PR. With natural language systems like GPT-3, we tend to ignore the fact that you often have to try several prompts to produce reasonable output. (Would we consider a human intelligent if they had to try 5 times to answer a question?) As has often been noted, systems like GPT-3 often get basic facts wrong. But humans often respond to prompts incoherently, and we frequently get our facts wrong.&nbsp; We get things wrong in different ways, and for different reasons; investigating those differences might reveal something about how our intelligence works, and might lead us to a better understanding of what an “artificial intelligence” might mean.</p>



<p>But without that investigation, our standard for intelligence is fairly loose. An AI system for making product recommendations can be successful even if most of the recommendations are wrong–just look at Amazon. (I’m not being ironic. If there are 10 recommendations and you’re interested in one of them, Amazon has won.) An AI system for an autonomous vehicle has to work to a much higher standard. So do many systems where safety isn’t an issue. We could happily talk about the “intelligence” of an AI chess engine that can beat the average human player, but a chess playing product that can only beat the average human and couldn’t play on a world championship level would be an embarrassment.</p>



<p>Which is just to say that intelligence, especially of the artificial sort, is many things. If you read <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.csee.umbc.edu/courses/471/papers/turing.pdf" target="_blank">Turing’s paper on the Imitation Game</a>, you’ll see quickly that Turing is more interested in the quality of the interaction than the correctness of the result. In his examples, the machine says that it’s not good at writing poetry; hesitates before giving answers; and even gets some results wrong. Turing’s thought experiment is more about whether a machine can behave like a human than about whether it can master many different disciplines. The word “intelligence” only appears once in the body of the paper, and then it refers to a human experimenter.</p>



<p>That leads me to a conclusion: Intelligence doesn’t have any single definition, and shouldn’t. Intelligence is always specific to the application.&nbsp; Intelligence for a search engine isn’t the same as intelligence for an autonomous vehicle, isn’t the same as intelligence for a <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.youtube.com/watch?v=7Vsbb9JW05w" target="_blank">robotic bird</a>, isn’t the same as intelligence for a language model. And it certainly isn’t the same as the intelligence for humans or for our unknown colleagues on other planets.</p>



<p>If that’s true, then why are we talking about “general intelligence” at all?&nbsp; General intelligence assumes a single definition. Discarding the idea of a single unifying definition of “intelligence” doesn’t cost us much, and gains a lot: we are free to create definitions of “intelligence” that are appropriate to specific projects. When embarking on a new project, it’s always helpful to know exactly what you’re trying to achieve. This is great for practical, real-world engineering. And even big, expensive research projects like DALL-E, Gato, LaMDA, and GPT-3 are ultimately engineering projects. If you look beyond the link-bait claims about general intelligence, sentience, and the like, the computer scientists working on these projects are working against well-defined benchmarks. Whether these benchmarks have anything to do with “intelligence” isn’t relevant. They aren’t trying to create an artificial human, or even an artificial dog. (We’ll leave artificial dogs to <a href="https://en.wikipedia.org/wiki/Boston_Dynamics" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">Boston Dynamics</a>.) They are trying–with considerable success–to extend the range of what computers can do. A model that can work successfully in over 600 different contexts is an important achievement. Whether or not that’s “general intelligence” (or intelligence at all) is a side show we don’t need.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/the-problem-with-intelligence/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>On Technique</title>
		<link>https://www.oreilly.com/radar/on-technique/</link>
				<comments>https://www.oreilly.com/radar/on-technique/#respond</comments>
				<pubDate>Tue, 09 Aug 2022 11:12:22 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides]]></dc:creator>
				<category><![CDATA[AI & ML]]></category>
		<category><![CDATA[Commentary]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=14669</guid>
				<description><![CDATA[In a previous article, I wrote about how models like DALL-E and Imagen disassociate ideas from technique. In the past, if you had a good idea in any field, you could only realize that idea if you had the craftsmanship and technique to back it up. With DALL-E, that’s no longer true. You can say, [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>In a <a href="https://www.oreilly.com/radar/artificial-creativity-2/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">previous article</a>, I wrote about how models like DALL-E and Imagen disassociate ideas from technique. In the past, if you had a good idea in any field, you could only realize that idea if you had the craftsmanship and technique to back it up. With DALL-E, that’s no longer true. You can say, “Make me a picture of a lion attacking a horse,” and it will happily generate one. Maybe not as good as the one that <a href="https://collections.britishart.yale.edu/catalog/tms:32" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">hangs in an art museum</a>, but you don’t need to know anything about canvas, paints, and brushes, nor do you need to get your clothes covered with paint. </p>



<p>This raises some important questions, though. What is the connection between expertise and ideation? Does technique help you form ideas? (The Victorian artist William Morris is often <a href="https://books.google.com/books?id=Til0DwAAQBAJ&amp;pg=PT292&amp;lpg=PT292&amp;dq=%E2%80%9CYou+can%E2%80%99t+have+art,%E2%80%9D+said+William+Morris,+the+designer,+poet,+and+master+craftsman+of+the+Victorians,+%E2%80%9Cwithout+resistance+in+the+material.%E2%80%9D&amp;source=bl&amp;ots=WYX6uA9wTq&amp;sig=ACfU3U3M2qyEIEsf1rrjKJ3poWb8CWIj9g&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwj0-JHhlo35AhUVjIkEHYP-AT0Q6AF6BAgCEAM#v=onepage&amp;q=%E2%80%9CYou%20can%E2%80%99t%20have%20art%2C%E2%80%9D%20said%20William%20Morris%2C%20the%20designer%2C%20poet%2C%20and%20master%20craftsman%20of%20the%20Victorians%2C%20%E2%80%9Cwithout%20resistance%20in%20the%20material.%E2%80%9D&amp;f=false" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">quoted</a> as saying “You can’t have art without resistance in the materials,” though he may only have been talking about his hatred of typewriters.) And what kinds of user interfaces will be effective for collaborations between humans and computers, where the computers supply the technique and we supply the ideas? Designing the prompts to get DALL-E to do something extraordinary requires a new kind of technique that’s very different from understanding pigments and brushes. What kinds of creativity does that new technique enable? How are these works different from what came before?</p>



<p>As interesting as it is to talk about art, there’s an area where these questions are more immediate. GitHub Copilot (based on a model named <a href="https://openai.com/blog/openai-codex/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">Codex</a>, which is derived from GPT-3) generates code in a number of programming languages, based on comments that the user writes. Going in the other direction, GPT-3 has proven to be surprisingly good at <a href="https://simonwillison.net/2022/Jul/9/gpt-3-explain-code/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">explaining code</a>. Copilot users still need to be programmers; they need to know whether the code that Copilot supplies is correct, and they need to know how to test it. The prompts themselves are really a sort of pseudo-code; even if the programmers don’t need to remember details of the language’s syntax or the names of library functions, they still need to think like programmers. But it’s obvious where this is trending. We need to ask ourselves how much “technique” we will ask of future programmers: in the 2030s or 2040s, will people just be able to tell some future Copilot what they want a program to be? More to the point, what sort of higher-order knowledge will future programmers need? Will they be able to focus more on the nature of what they want to accomplish, and less on the syntactic details of writing code?</p>



<p>It’s easy to imagine a lot of software professionals saying, “Of course you’ll have to know C. Or Java. Or Python. Or Scala.” But I don’t know if that’s true. We’ve been here before. In the 1950s, computers were programmed in machine language. (And before that, with cables and plugs.) It’s hard to imagine now, but the introduction of the first programming languages–Fortran, COBOL, and the like–was met with resistance from programmers who thought you needed to understand the machine. Now almost no one works in machine language or assembler. Machine language is reserved for a few people who need to work on some specialized areas of operating system internals, or who need to write some kinds of embedded systems code.</p>



<p>What would be necessary for another transformation? Tools like Copilot, useful as they may be, are nowhere near ready to take over. What capabilities will they need? At this point, programmers still have to decide whether or not code generated by Copilot is correct.&nbsp;We don’t (generally) have to decide whether the output of a C or Java compiler is correct, nor do we have to worry about whether, given the same source code, the compiler will generate identical output. Copilot doesn’t make that guarantee–and, even if it did, any change to the model (for example, to incorporate new StackOverflow questions or GitHub repositories) would be very likely to change its output.&nbsp;While we can certainly imagine compiling a program from a series of Copilot prompts, I can’t imagine a program that would be likely to stop working if it was recompiled without changes to the source code. Perhaps the only exception would be a library that could be developed once, then tested, verified, and used without modification–but the development process would have to re-start from ground zero whenever a bug or a security vulnerability was found. That wouldn’t be acceptable; we’ve never written programs that don’t have bugs, or that never need new features. A key principle behind much modern software development is minimizing the amount of code that has to change to fix bugs or add features.</p>



<p>It’s easy to think that programming is all about creating new code. It isn’t; one thing that every professional learns quickly is that most of the work goes into maintaining old code. A new generation of programming tools must take that into account, or we’ll be left in a weird situation where a tool like Copilot can be used to write new code, but programmers will still have to understand that code in detail because it can only be maintained by hand. (It is possible–even likely–that we will have AI-based tools that help programmers research software supply chains, discover vulnerabilities, and possibly even suggest fixes.) Writing about AI-generated art, Raphaël Millière <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.wired.com/story/dalle-art-curation-artificial-intelligence/" target="_blank">says</a>, “No prompt will produce the exact same result twice”; that may be desirable for artwork, but is destructive for programming. Stability and consistency is a requirement for next-generation programming tools; we can’t take a step backwards.</p>



<p>The need for greater stability might drive tools like Copilot from free-form English language prompts to some kind of more formal language. A book about <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://dallery.gallery/the-dalle-2-prompt-book/" target="_blank">prompt engineering for DALL-E</a> already exists; in a way, that’s trying to reverse-engineer a formal language for generating images. A formal language for prompts is a move back in the direction of traditional programming, though possibly with a difference. Current programming languages are all about describing, step by step, what you want the computer to do in great detail. Over the years, we’ve gradually progressed to higher levels of abstraction. Could building a language model into a compiler facilitate the creation of a simpler language, one in which programmers just described what they wanted to do, and let the machine worry about the implementation, while providing guarantees of stability? Remember that it was possible to build applications with graphical interfaces, and for those applications to communicate about the Internet, before the Web. The Web (and, specifically, HTML) added a new formal language that encapsulated tasks that used to require programming.</p>



<p>Now let’s move up a level or two: from lines of code to functions, modules, libraries, and systems. Everyone I know who has worked with Copilot has said that, while you don’t need to remember the details of the programming libraries you’re using, you have to be even more aware of what you’re trying to accomplish. You have to know what you want to do; you have to have a design in mind. Copilot is good at low-level coding; does a programmer need to be in touch with the craft of low-level coding to think about the high-level design? Up until now that’s certainly been true, but largely out of necessity: you wouldn’t let someone design a large system who hasn’t built smaller systems. It is true (as Dave Thomas and Andy Hunt argued in <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://learning.oreilly.com/library/view/the-pragmatic-programmer/9780135956977/" target="_blank">The Pragmatic Programmer</a>) that knowing different programming languages gives you different tools and approaches for solving problems.&nbsp; Is the craft of software architecture different from the craft of programming?</p>



<p>We don’t really have a good language for describing software design. Attempts like UML have been partially successful at best. UML was both over- and under-specified, too precise and not precise enough; tools that generated source code scaffolding from UML diagrams exist, but aren’t commonly used these days. The scaffolding defined interfaces, classes, and methods that could then be implemented by programmers. While automatically generating the structure of a system sounds like a good idea, in practice it may have made things more difficult: if the high-level specification changed, so did the scaffolding, obsoleting any work that had been put into implementing with the scaffold. This is similar to the compiler’s stability problem, modulated into a different key. Is this an area where AI could help?</p>



<p>I suspect we still don’t want source code scaffolding, at least as UML envisioned it; that’s bound to change with any significant change in the system’s description. Stability will continue to be a problem. But it might be valuable to have a AI-based design tool that can take a verbal description of a system’s requirements, then generate some kind of design based on a large library of software systems–like Copilot, but at a higher level. Then the problem would be integrating that design with implementations of the design, some of which could be created (or at least suggested) by a system like Copilot. The problem we’re facing is that software development takes place on two levels: high level design and mid-level programming. Integrating the two is a hard problem that hasn’t been solved convincingly.&nbsp; Can we imagine taking a high-level design, adding our descriptions to it, and going directly from the high-level design with mid-level details to an executable program? That programming environment would need the ability to partition a large project into smaller pieces, so teams of programmers could collaborate. It would need to allow changes to the high-level descriptions, without disrupting work on the objects and methods that implement those descriptions. It would need to be integrated with a version control system that is effective for the English-language descriptions as it is for lines of code. This wouldn’t be thinkable without guarantees of stability.</p>



<p>It was fashionable for a while to talk about programming as “craft.”&nbsp; I think that fashion has waned, probably for the better; “code as craft” has always seemed a bit precious to me. But the idea of “craft” is still useful: it is important for us to think about how the craft may change, and how fundamental those changes can’t be.&nbsp;It’s clear that we are a long way from a world where only a few specialists need to know languages like C or Java or Python. But it’s also possible that developments like Copilot give us a glimpse of what the next step might be. Lamenting the state of programing tools, which haven’t changed much since the 1960s, Alan Kay <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.quora.com/What-was-the-last-breakthrough-in-computer-programming" target="_blank">wrote on Quora</a> that “the next significant threshold that programming must achieve is for programs and programming systems to have a much deeper understanding of both what they are trying to do, and what they are actually doing.” A new craft of programming that is focused less on syntactic details, and more on understanding what the systems we are building are trying to accomplish, is the goal we should be aiming for.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/on-technique/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>Scaling False Peaks</title>
		<link>https://www.oreilly.com/radar/scaling-false-peaks/</link>
				<comments>https://www.oreilly.com/radar/scaling-false-peaks/#respond</comments>
				<pubDate>Thu, 04 Aug 2022 11:12:44 +0000</pubDate>
		<dc:creator><![CDATA[Kevlin Henney]]></dc:creator>
				<category><![CDATA[AI & ML]]></category>
		<category><![CDATA[Commentary]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=14661</guid>
				<description><![CDATA[Humans are notoriously poor at judging distances. There&#8217;s a tendency to underestimate, whether it&#8217;s the distance along a straight road with a clear run to the horizon or the distance across a valley. When ascending toward a summit, estimation is further confounded by false summits. What you thought was your goal and end point turns [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>Humans are notoriously poor at judging distances. There&#8217;s a tendency to underestimate, whether it&#8217;s the distance along a straight road with a clear run to the horizon or the distance across a valley. When ascending toward a summit, estimation is further confounded by false summits. What you thought was your goal and end point turns out to be a lower peak or simply a contour that, from lower down, looked like a peak. You thought you made it–or were at least close–but there&#8217;s still a long way to go.</p>



<p>The story of AI is a story of punctuated progress, but it is also the story of (many) false summits.</p>



<p>In the 1950s, machine translation of Russian into English was considered to be no more complex than dictionary lookups and templated phrases. Natural language processing has come a very long way since then, having burnt through a good few paradigms to get to something we can use on a daily basis. In the 1960s, Marvin Minsky and Seymour Papert proposed the Summer Vision Project for undergraduates: connect a TV camera to a computer and identify objects in the field of view. Computer vision is now something that is commodified for specific tasks, but it continues to be a work in progress and, worldwide, has taken more than a few summers (and AI winters) and many more than a few undergrads.</p>



<p>We can find many more examples across many more decades that reflect naiveté and optimism and–if we are honest–no small amount of ignorance and hubris. The two general lessons to be learned here are not that machine translation involves more than lookups and that computer vision involves more than edge detection, but that when we are confronted by complex problems in unfamiliar domains, we should be cautious of anything that looks simple at first sight, and that when we have successful solutions to a specific sliver of a complex domain, we should not assume those solutions are generalizable. This kind of humility is likely to deliver more meaningful progress and a more measured understanding of such progress. It is also likely to reduce the number of pundits in the future who mock past predictions and ambitions, along with the recurring irony of machine-learning experts who seem unable to learn from the past trends in their own field.</p>



<p>All of which brings us to <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.deepmind.com/publications/a-generalist-agent" target="_blank">DeepMind&#8217;s Gato</a> and the claim that the summit of artificial general intelligence (AGI) is within reach. The hard work has been done and reaching AGI is now a simple matter of scaling. At best, this is a false summit on the right path; at worst, it&#8217;s a local maximum far from AGI, which lies along a very different route in a different range of architectures and thinking.</p>



<p>DeepMind&#8217;s Gato is an AI model that can be taught to carry out many different kinds of tasks based on a single transformer neural network. The 604 tasks Gato was trained on vary from playing Atari video games to chat, from navigating simulated 3D environments to following instructions, from captioning images to real-time, real-world robotics. The achievement of note is that it’s underpinned by a single model trained across all tasks rather than different models for different tasks and modalities. Learning how to ace Space Invaders does not interfere with or displace the ability to carry out a chat conversation.</p>



<p><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://arxiv.org/pdf/2205.06175.pdf" target="_blank">Gato was intended to</a> &#8220;test the hypothesis that training an agent which is generally capable on a large number of tasks is possible; and that this general agent can be adapted with little extra data to succeed at an even larger number of tasks.&#8221; In this, it succeeded. But how far can this success be generalized in terms of loftier ambitions? The <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://twitter.com/NandoDF/status/1525397036325019649" target="_blank">tweet</a> that provoked a wave of responses (this one included) came from DeepMind&#8217;s research director, Nando de Freitas: &#8220;It&#8217;s all about scale now! The game is over!&#8221;</p>



<p>The game in question is the quest for AGI, which is closer to what science fiction and the general public think of as AI than the narrower but applied, task-oriented, statistical approaches that constitute commercial machine learning (ML) in practice.</p>



<p>The claim is that AGI is now simply a matter of improving performance, both in hardware and software, and making models bigger, using more data and more kinds of data across more modes. Sure, there&#8217;s <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://twitter.com/NandoDF/status/1525398087203983360" target="_blank">research work</a> to be done, but now it&#8217;s all about turning the dials up to 11 and beyond and, voilà, we&#8217;ll have scaled the north face of the AGI to plant a flag on the summit.</p>



<p>It&#8217;s easy to get breathless at altitude.</p>



<p>When we look at other systems and scales, it&#8217;s easy to be drawn to superficial similarities in the small and project them into the large. For example, if we look at water swirling down a plughole and then out into the cosmos at spiral galaxies, we see a similar structure. But these spirals are more closely bound in our desire to see connection than they are in physics. In looking at scaling specific AI to AGI, it&#8217;s easy to focus on tasks as the basic unit of intelligence and ability. What we know of intelligence and learning systems in nature, however, suggests the relationships between tasks, intelligence, systems, and adaptation is more complex and more subtle. Simply scaling up one dimension of ability may simply scale up one dimension of ability without triggering emergent generalization.</p>



<p>If we look closely at software, society, physics or life, we see that scaling is usually accompanied by fundamental shifts in organizing principle and process. Each scaling of an existing approach is successful up to a point, beyond which a different approach is needed. You can run a small business using office tools, such as spreadsheets, and a social media page. Reaching Amazon-scale is not a matter of bigger spreadsheets and more pages. Large systems have radically different architectures and properties to either the smaller systems they are built from or the simpler systems that came before them.</p>



<p>It may be that artificial general intelligence is a far more significant challenge than taking task-based models and increasing data, speed, and number of tasks. We typically underappreciate how complex such systems are. We divide and simplify, make progress as a result, only to discover, as we push on, that the simplification was just that; a new model, paradigm, architecture, or schedule is needed to make further progress. Rinse and repeat. Put another way, just because you got to basecamp, what makes you think you can make the summit using the same approach? And what if you can&#8217;t see the summit? If you don&#8217;t know what you&#8217;re aiming for, it&#8217;s difficult to plot a course to it.</p>



<p>Instead of assuming the answer, we need to ask: <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.oreilly.com/radar/closer-to-agi/" target="_blank">How do we define AGI</a>? Is AGI simply task-based AI for N tasks and a sufficiently large value of N? And, even if the answer to that question is <em>yes</em>, is the path to AGI necessarily task-centric? How much of AGI is performance? How much of AGI is big/bigger/biggest data?</p>



<p>When we look at life and existing learning systems, we learn that scale matters, but not in the sense suggested by a simple multiplier. It may well be that the trick to cracking AGI is to be found in scaling–but down rather than up.</p>



<p>Doing more with less looks to be more important than doing more with more. For example, the GPT-3 language model is based on a network of 175 billion parameters. The first version of DALL-E, the prompt-based image generator, used a 12-billion parameter version of GPT-3; the second, improved version used only 3.5 billion parameters. And then there&#8217;s Gato, which achieves its multitask, multimodal abilities with only 1.2 billion.</p>



<p>These reductions hint at the direction, but it&#8217;s not clear that Gato&#8217;s, GPT-3&#8217;s or any other contemporary architecture is necessarily the right vehicle to reach the destination. For example, how many training examples does it take to learn something? For biological systems, the answer is, in general, not many; for machine learning, the answer is, in general, very many. GPT-3, for example, developed its language model based on 45TB of text. Over a lifetime, a human reads and hears of the order of a billion words; a child is exposed to ten million or so before starting to talk. Mosquitoes can learn to avoid a particular pesticide after a <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.nature.com/articles/s41598-022-05754-2" target="_blank">single non-lethal exposure</a>. When you learn a new game–whether video, sport, board or card–you generally only need to be told the rules and then play, perhaps with a game or two for practice and rule clarification, to make a reasonable go of it. Mastery, of course, takes far more practice and dedication, but general intelligence is not about mastery.</p>



<p>And when we look at the hardware and its needs, consider that while the brain is one of the most power-hungry organs of the human body, it still has a modest power consumption of <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.scientificamerican.com/article/thinking-hard-calories/" target="_blank">around 12 watts</a>. Over a life the brain will consume up to 10 MWh; training the GPT-3 language model took an estimated 1 GWh.</p>



<p>When we talk about scaling, the game is only just beginning.</p>



<p>While hardware and data matter, the architectures and processes that support general intelligence may be necessarily quite different to the architectures and processes that underpin current ML systems. Throwing faster hardware and all the world&#8217;s data at the problem is likely to see diminishing returns, although that may well let us scale a false summit from which we can see the real one.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/scaling-false-peaks/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>Artificial Creativity?</title>
		<link>https://www.oreilly.com/radar/artificial-creativity-2/</link>
				<comments>https://www.oreilly.com/radar/artificial-creativity-2/#respond</comments>
				<pubDate>Tue, 12 Jul 2022 13:24:16 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides]]></dc:creator>
				<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Commentary]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=14604</guid>
				<description><![CDATA[There’s a puzzling disconnect in the many articles I read about DALL-E 2, Imagen, and the other increasingly powerful tools I see for generating images from textual descriptions. It’s common to read articles that talk about AI having creativity–but I don’t think that’s the case at all.&#160; As with the discussion of sentience, authors are [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>There’s a puzzling disconnect in the many articles I read about DALL-E 2, Imagen, and the other increasingly powerful tools I see for generating images from textual descriptions. It’s common to read articles that talk about AI having creativity–but I don’t think that’s the case at all.&nbsp; As with the discussion of sentience, authors are being misled by a very human will to believe. And in being misled, they’re missing out on what’s important.</p>



<p>It’s impressive to see AI-generated pictures of an <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://twitter.com/OpenAI/status/1511714545529614338?ref_src=twsrc%5Etfw" target="_blank">astronaut riding a horse</a>, or a <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.thedailybeast.com/googles-new-text-to-image-generator-imagen-is-scary-accurate" target="_blank">dog riding a bike in Times Square</a>. But where’s the creativity?&nbsp; Is it in the prompt or in the product?&nbsp; I couldn’t draw a picture of a dog riding a bike; I’m not that good an artist. Given a few pictures of dogs, Times Square, and whatnot, I could probably photoshop my way into something passable, but not very good. &nbsp;(To be clear: these AI systems are not automating photoshop.) So the AI is doing something that many, perhaps most humans, wouldn’t be able to do. That’s important. Very few humans (if any) can play Go at the level of AlphaGo. We’re getting used to being second-best.</p>



<p>However, a computer replacing a human’s limited photoshop skills isn’t creativity. It took a human to say “create a picture of a dog riding a bike.” An AI couldn’t do that of its own volition. That’s creativity.&nbsp;But before writing off the creation of the picture, let’s think more about what that really means. Works of art really have two sources: the idea itself and the technique required to instantiate that idea. You can have all the ideas you want, but if you can’t paint like Rembrandt, you’ll never generate a Dutch master. Throughout history, painters have learned technique by copying the works of masters. What’s interesting about DALL-E, Imagen, and their relatives is that they supply the technique. Using DALL-E or Imagen, I could create a painting of a tarsier eating an anaconda without knowing how to paint.</p>



<p>That distinction strikes me as very important. In the 20th and 21st centuries we’ve become very impatient with technique. We haven’t become impatient with creating good ideas. (Or at least strange ideas.) The “age of mechanical reproduction” seems to have made technique less relevant; after all, we’re heirs of the poet Ezra Pound, who famously said, “Make it new.”</p>



<p>But does that quote mean what we think? Pound’s “Make it new” has been <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.guernicamag.com/the-making-of-making-it-new/" target="_blank">traced back</a> to 18th century China, and from there to the 12th century, something that’s not at all surprising if you’re familiar with Pound’s fascination with Chinese literature. What’s interesting, though, is that Chinese art has always focused on technique to a level that’s almost inconceivable to the European tradition. And “Make it new” has, within it, the acknowledgment that what’s new first has to be made. Creativity and technique don’t come apart that easily.</p>



<p>We can see that in other art forms. Beethoven broke Classical music and put it back together again, but different-–he’s the most radical composer in the Western tradition (except for, perhaps, Thelonious Monk). And it’s worth asking how we get from what’s old to what’s new.&nbsp; AI has been used to <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.classicfm.com/composers/beethoven/unfinished-tenth-symphony-completed-by-artificial-intelligence/" target="_blank">complete Beethoven’s 10th symphony</a>, for which Beethoven left a number of sketches and notes at the time of his death. The result is pretty good, better than the human attempts I’ve heard at completing the 10th.&nbsp;It sounds Beethoven-like; its flaw is that it goes on and on, repeating Beethoven-like riffs but without the tremendous forward-moving force that you get in Beethoven’s compositions. But completing the 10th isn’t the problem we should be looking at. How did we get Beethoven in the first place?&nbsp; If you trained an AI on the music Beethoven was trained on, would you eventually get the 9th symphony?&nbsp;Or would you get something that sounds a lot like Mozart and Haydn?</p>



<p>I’m betting the latter. The progress of art isn’t unlike the structure of scientific revolutions, and Beethoven indeed took everything that was known, broke it apart, and put it back together differently. Listen to the <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.youtube.com/watch?v=HljSXSm6v9M" target="_blank">opening of Beethoven’s 9th symphony</a>: what is happening? Where’s the theme? It sounds like the orchestra is tuning up. When the first theme finally arrives, it’s not the traditional “melody” that pre-Beethoven listeners would have expected, but something that dissolves back into the sound of instruments tuning, then gets reformed and reshaped. Mozart would never do this. Or listen again to <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.youtube.com/watch?v=a9UApyClFKA" target="_blank">Beethoven’s 5th symphony</a>, probably the most familiar piece of orchestral music in the world. That opening duh-duh-duh-DAH–what kind of theme is that? Beethoven builds this movement by taking that four note fragment, moving it around, changing it, breaking it into even smaller bits and reassembling them. You can’t imagine a witty, urbane, polite composer like Haydn writing music like this. But I don’t want to worship some notion of Beethoven’s “genius” that privileges creativity over technique. Beethoven could never have gotten beyond Mozart and Haydn (with whom Beethoven studied) without extensive knowledge of the technique of composing; he would have had some good ideas, but he would never have known how to realize them. Conversely, the realization of radical ideas as actual works of art inevitably changes the technique. Beethoven did things that weren’t conceivable to Mozart or Haydn, and they changed the way music was written: those changes made the music of Schubert, Schumann, and Brahms possible, along with the rest of the 19th century. </p>



<p>That brings us back to the question of computers, creativity, and craft. Systems like DALL-E and Imagen break apart the idea and the technique, or the execution of the idea. Does that help us be more creative, or less? I could tell Imagen to “paint a picture of a 15th century woman with an enigmatic smile,” and after a few thousand tries I might get something like the Mona Lisa. I don’t think that anyone would care, really.&nbsp; But this isn’t creating something new; it’s reproducing something old. If I magically appeared early in the 20th century, along with a computer capable of running Imagen (though only trained on art through 1900), would I be able to tell it to create a Picasso or a Dali? I have no idea how to do that. Nor do I have any idea what the next step for art is now, in the 21st century, or how I’d ask Imagen to create it. It sure isn’t Bored Apes. And if I could ask Imagen or DALL-E to create a painting from the 22nd century, how would that change the AI’s conception of technique?</p>



<p>At least part of what I lack is the technique, for technique isn’t just mechanical ability; it’s also the ability to think the way great artists do. And that gets us to the big question:</p>



<p>Now that we have abstracted technique away from the artistic process, can we build interfaces between the creators of ideas and the machines of technique in a way that allows the creators to “make it new”?&nbsp; That’s what we really want from creativity: something that didn’t exist, and couldn’t have existed, before.</p>



<p>Can artificial intelligence help us to be creative? That’s the important question, and it’s a question about user interfaces, not about who has the biggest model.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/artificial-creativity-2/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>Closer to AGI?</title>
		<link>https://www.oreilly.com/radar/closer-to-agi/</link>
				<comments>https://www.oreilly.com/radar/closer-to-agi/#respond</comments>
				<pubDate>Tue, 07 Jun 2022 11:09:16 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides]]></dc:creator>
				<category><![CDATA[AI & ML]]></category>
		<category><![CDATA[Commentary]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=14516</guid>
				<description><![CDATA[DeepMind’s new model, Gato, has sparked a debate on whether artificial general intelligence (AGI) is nearer–almost at hand–just a matter of scale.&#160; Gato is a model that can solve multiple unrelated problems: it can play a large number of different games, label images, chat, operate a robot, and more.&#160; Not so many years ago, one [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>DeepMind’s new model, Gato, has sparked a debate on whether artificial general intelligence (AGI) is nearer–almost at hand–just a matter of scale.&nbsp; Gato is a model that can solve multiple unrelated problems: it can <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.deepmind.com/publications/a-generalist-agent" target="_blank">play a large number of different games, label images, chat, operate a robot, and more</a>.&nbsp; Not so many years ago, one problem with AI <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.oreilly.com/radar/what-is-artificial-intelligence/" target="_blank">was that AI systems were only good at one thing</a>. After IBM’s Deep Blue defeated Garry Kasparov in chess,&nbsp; it was easy to say “But the ability to play chess isn’t really what we mean by intelligence.” A model that plays chess can’t also play space wars. That’s obviously no longer true; we can now have models capable of doing many different things. 600 things, in fact, and future models will no doubt do more.</p>



<p>So, are we on the verge of artificial general intelligence, as <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://twitter.com/NandoDF/status/1525397036325019649" target="_blank">Nando de Frietas (research director at DeepMind) claims? That the only problem left is scale?</a> I don’t think so.&nbsp; It seems inappropriate to be talking about AGI when <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.oreilly.com/radar/artificial-intelligence-human-inhuman/" target="_blank">we don’t really have a good definition of “intelligence.”</a> If we had AGI, how would we know it? We have a lot of vague notions about the Turing test, but in the final analysis, Turing wasn’t offering a definition of machine intelligence; he was probing the question <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://aeon.co/essays/why-we-should-remember-alan-turing-as-a-philosopher" target="_blank">of what human intelligence means</a>.</p>



<p>Consciousness and intelligence seem to require <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.oreilly.com/radar/intelligence-and-comprehension/" target="_blank">some sort of agency</a>.&nbsp; An AI can’t choose what it wants to learn, neither can it say “I don’t want to play Go, I’d rather play Chess.” Now that we have computers that can do both, can they “want” to play one game or the other? One reason we know our children (and, for that matter, our pets) are intelligent and not just automatons is that they’re capable of disobeying. A child can refuse to do homework; a dog can refuse to sit. And that refusal is as important to intelligence as the ability to solve differential equations, or to play chess. Indeed, the path towards artificial intelligence is as much about teaching us what intelligence isn’t (as Turing knew) as it is about building an AGI.</p>



<p>Even if we accept that Gato is a huge step on the path towards AGI, and that scaling is the only problem that’s left, it is more than a bit problematic to think that scaling is a problem that’s easily solved. We don’t know how much power it took to train Gato, but GPT-3 required about <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://info.deeplearning.ai/the-batch-recognizing-distracted-drivers-training-fighter-pilots-dominating-the-bridge-table-training-trillions-of-parameters" target="_blank">1.3 Gigawatt-hours</a>: roughly 1/1000th the energy it takes to <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://home.cern/resources/faqs/facts-and-figures-about-lhc#:~:text=What%20is%20the%20LHC%20power,is%20750%20GWh%20per%20year." target="_blank">run the Large Hadron Collider</a> for a year. Granted, Gato is much smaller than GPT-3, though <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.zdnet.com/article/deepminds-gato-is-mediocre-so-why-did-they-build-it/" target="_blank">it doesn’t work as well</a>; Gato’s performance is generally inferior to that of single-function models. And granted, a lot can be done to optimize training (and <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://arxiv.org/pdf/2203.15556.pdf" target="_blank">DeepMind has done a lot of work</a> on models that require less energy). But Gato has just over 600 capabilities, focusing on natural language processing, image classification, and game playing. These are only a few of many tasks an AGI will need to perform. How many tasks would a machine be able to perform to qualify as a “general intelligence”? Thousands?&nbsp; Millions? Can those tasks even be enumerated? At some point, the project of training an artificial general intelligence sounds like something from Douglas Adams’ novel <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/The_Hitchhiker's_Guide_to_the_Galaxy" target="_blank"><em>The Hitchhiker’s Guide to the Galaxy</em></a>, in which the Earth is a computer designed by an AI called Deep Thought to answer the question “What is the question to which 42 is the answer?”</p>



<p>Building bigger and bigger models in hope of somehow achieving general intelligence may be an interesting research project, but AI may already have achieved a level of performance that suggests specialized training on top of existing <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://arxiv.org/abs/2108.07258" target="_blank">foundation models</a> will reap far more short term benefits. A foundation model trained to recognize images can be trained further to be part of a self-driving car, or <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.resetera.com/threads/midjourney-is-lighting-up-the-ai-generated-art-community.586463/" target="_blank">to create generative art</a>. A foundation model like GPT-3 trained to understand and speak human language can be <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://copilot.github.com/" target="_blank">trained more deeply to write computer code</a>.</p>



<p>Yann LeCun posted a <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://m.alpha.facebook.com/story.php?story_fbid=10158256523332143&amp;id=722677142" target="_blank">Twitter thread about general intelligence (consolidated on Facebook)</a> stating some “simple facts.” First, LeCun says that there is no such thing as “general intelligence.” LeCun also says that “human level AI” is a useful goal–acknowledging that human intelligence itself is something less than the type of general intelligence sought for AI. All humans are specialized to some extent. I’m human; I’m arguably intelligent; I can play Chess and Go, but not <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Xiangqi" target="_blank">Xiangqi</a> (often called Chinese Chess) or Golf. I could presumably learn to play other games, but I don’t have to learn them all. I can also play the piano, but not the violin. I can speak a few languages. Some humans can speak dozens, but none of them speak every language.</p>



<p>There’s an important point about expertise hidden in here: we expect our AGIs to be “experts” (to beat top-level Chess and Go players), but as a human, I’m only fair at chess and poor at Go. Does human intelligence require expertise? (Hint: re-read <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://academic.oup.com/mind/article/LIX/236/433/986238" target="_blank">Turing’s original paper</a> about the Imitation Game, and check the computer’s answers.) And if so, what kind of expertise? Humans are capable of broad but limited expertise in many areas, combined with deep expertise in a small number of areas. So this argument is really about terminology: could Gato be a step towards human-level intelligence (limited expertise for a large number of tasks), but not general intelligence?</p>



<p>LeCun agrees that we are missing some “fundamental concepts,” and we don’t yet know what those fundamental concepts are. In short, we can’t adequately define intelligence. More specifically, though, he mentions that “a few others believe that symbol-based manipulation is necessary.” That’s an allusion to the debate (<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://twitter.com/garymarcus/status/1411401507610796032" target="_blank">sometimes on Twitter</a>) between LeCun and Gary Marcus, who has argued many times that <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://nautil.us/deep-learning-is-hitting-a-wall-14467/" target="_blank">combining deep learning with symbolic reasoning</a> is the only way for AI to progress. (In his response to the Gato announcement, Marcus labels this school of thought “<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://garymarcus.substack.com/p/the-new-science-of-alt-intelligence" target="_blank">Alt-intelligence</a>.”) That’s an important point: impressive as models like GPT-3 and <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://arxiv.org/abs/2112.06905" target="_blank">GLaM</a> are, they make a lot of mistakes. Sometimes those are <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.linkedin.com/pulse/gpt-3-does-understand-what-saying-steve-shwartz/" target="_blank">simple mistakes of fact</a>, such as when GPT-3 wrote an article about the United Methodist Church that got a number of basic facts wrong. Sometimes, the mistakes reveal a horrifying (or hilarious, they’re often the same) <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.tidio.com/blog/how-smart-are-gpt-3-chatbots/" target="_blank">lack of what we call “common sense.”</a> Would you sell your children for refusing to do their homework? (To give GPT-3 credit, it points out that selling your children is illegal in most countries, and that there are better forms of discipline.)</p>



<p>It’s not clear, at least to me, that these problems can be solved by “scale.” How much more text would you need to know that humans don’t, normally, sell their children? I can imagine “selling children” showing up in sarcastic or frustrated remarks by parents, along with texts discussing slavery. I suspect there are few texts out there that actually state that selling your children is a bad idea.&nbsp;Likewise, how much more text would you need to know that Methodist general conferences take place every four years, not annually? The general conference in question generated some press coverage, but not a lot; it’s reasonable to assume that GPT-3 had most of the facts that were available. What additional data would a large language model need to avoid making these mistakes? Minutes from prior conferences, documents about Methodist rules and procedures, and a few other things.&nbsp;As modern datasets go, it’s probably not very large; a few gigabytes, at most. But then the question becomes “How many specialized datasets would we need to train a general intelligence so that it’s accurate on any conceivable topic?”&nbsp; Is that answer a million?&nbsp; A billion?&nbsp; What are all the things we might want to know about? Even if any single dataset is relatively small, we’ll soon find ourselves building the successor to Douglas Adams’ Deep Thought.</p>



<p>Scale isn’t going to help. But in that problem is, I think, a solution. If I were to build an artificial therapist bot, would I want a general language model?&nbsp; Or would I want a language model that had some broad knowledge, but has received some special training to give it deep expertise in psychotherapy? Similarly, if I want a system that writes news articles about religious institutions, do I want a fully general intelligence? Or would it be preferable to train a general model with data specific to religious institutions? The latter seems preferable–and it’s certainly more similar to real-world human intelligence, which is broad, but with areas of deep specialization.&nbsp;Building such an intelligence is a problem we’re already on the road to solving, by using large “foundation models” with additional training to customize them for special purposes. GitHub’s <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://copilot.github.com/" target="_blank">Copilot</a> is one such model; <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.oreilly.com/online-learning/article-answers.html" target="_blank">O’Reilly Answers</a> is another.</p>



<p>If a “general AI” is no more than “a model that can do lots of different things,” do we really need it, or is it just an academic curiosity?&nbsp; What’s clear is that we need better models for specific tasks. If the way forward is to build specialized models on top of foundation models, and if this process generalizes from language models like GPT-3 and O’Reilly Answers to other models for different kinds of tasks, then we have a different set of questions to answer. First, rather than trying to build a general intelligence by making an even bigger model, we should ask whether we can build a good foundation model that’s smaller, cheaper, and more easily distributed, perhaps as open source. Google has done <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://info.deeplearning.ai/the-batch-recognizing-distracted-drivers-training-fighter-pilots-dominating-the-bridge-table-training-trillions-of-parameters" target="_blank">some excellent work at reducing power consumption, though it remains huge</a>, and Facebook has released their <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.technologyreview.com/2022/05/03/1051691/meta-ai-large-language-model-gpt3-ethics-huggingface-transparency/" target="_blank">OPT model with an open source license</a>. Does a foundation model actually require anything more than the ability to parse and create sentences that are grammatically correct and stylistically reasonable?&nbsp; Second, we need to know how to specialize these models effectively.&nbsp; We can obviously do that now, but I suspect that training these subsidiary models can be optimized. These specialized models might also incorporate symbolic manipulation, as Marcus suggests; for two of our examples, psychotherapy and religious institutions, symbolic manipulation would probably be essential. If we’re going to build an AI-driven therapy bot, I’d rather have a bot that can do that one thing well than a bot that makes mistakes that are much subtler than <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.artificialintelligence-news.com/2020/10/28/medical-chatbot-openai-gpt3-patient-kill-themselves/" target="_blank">telling patients to commit suicide</a>. I’d rather have a bot that can collaborate intelligently with humans than one that needs to be watched constantly to ensure that it doesn’t make any egregious mistakes.</p>



<p>We need the ability to combine models that perform different tasks, and we need the ability to interrogate those models about the results. For example, I can see the value of a chess model that included (or was integrated with) a language model that would enable it to answer questions like “What is the significance of Black’s 13th move in the 4th game of FischerFisher vs. Spassky?” Or “You’ve suggested Qc5, but what are the alternatives, and why didn’t you choose them?” Answering those questions doesn’t require a model with 600 different abilities. It requires two abilities: chess and language. Moreover, it requires the ability to explain why the AI&nbsp;rejected certain alternatives in its decision-making process. As far as I know, little has been done on this latter question, though the ability to expose other alternatives <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="http://radar.oreilly.com/2011/02/watson-machine-learning.html" target="_blank">could be important in applications like medical diagnosis</a>. “What solutions did you reject, and why did you reject them?” seems like important information we should be able to get from an AI, whether or not it’s “general.”</p>



<p>An AI that can answer those questions seems more relevant than an AI that can simply do a lot of different things.</p>



<p>Optimizing the specialization process is crucial because we’ve turned a technology question into an economic question. How many specialized models, like Copilot or O’Reilly Answers, can the world support? We’re no longer talking about a massive AGI that takes terawatt-hours to train, but about specialized training for a huge number of smaller models. A psychotherapy bot might be able to pay for itself–even though it would need the ability to retrain itself on current events, for example, to deal with patients who are anxious about, say, the invasion of Ukraine. (There is <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://arxiv.org/pdf/2106.06297.pdf" target="_blank">ongoing research</a> on models that can incorporate new information as needed.) It’s not clear that a specialized bot for producing news articles about religious institutions would be economically viable. That’s the third question we need to answer about the future of AI: what kinds of economic models will work? Since AI models are essentially cobbling together answers from other sources that have their own licenses and business models, how will our future agents compensate the sources from which their content is derived? How should these models deal with issues like attribution and license compliance?</p>



<p>Finally, projects like Gato don’t help us understand how AI systems should collaborate with humans. Rather than just building bigger models, researchers and entrepreneurs need to be exploring different kinds of interaction between humans and AI. That question is out of scope for Gato, but it is something we need to address regardless of whether the future of artificial intelligence is general or narrow but deep. Most of our current AI systems are oracles: you give them a prompt, they produce an output.&nbsp; Correct or incorrect, you get what you get, take it or leave it. Oracle interactions don’t take advantage of human expertise, and risk wasting human time on “obvious” answers, where the human says “I already know that; I don’t need an AI to tell me.”</p>



<p>There are some exceptions to the oracle model. Copilot places its suggestion in your code editor, and changes you make can be fed back into the engine to improve future suggestions. <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://boingboing.net/2022/03/24/midjourney-sharpens-style-of-ai-art.html" target="_blank">Midjourney</a>, a platform for AI-generated art that is currently in closed beta, also incorporates a feedback loop.</p>



<p>In the next few years, we will inevitably rely more and more on machine learning and artificial intelligence. If that interaction is going to be productive, we will need a lot from AI. We will need interactions between humans and machines, a better understanding of how to train specialized models, the ability to distinguish between correlations and facts–and that’s only a start. Products like Copilot and O’Reilly Answers give a glimpse of what’s possible, but they’re only the first steps. AI has made dramatic progress in the last decade, but we won’t get the products we want and need merely by scaling.&nbsp;We need to learn to think differently.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/closer-to-agi/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>Quantum Computing without the Hype</title>
		<link>https://www.oreilly.com/radar/quantum-computing-without-the-hype/</link>
				<comments>https://www.oreilly.com/radar/quantum-computing-without-the-hype/#respond</comments>
				<pubDate>Tue, 10 May 2022 11:45:05 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides]]></dc:creator>
				<category><![CDATA[Big Data Tools and Pipelines]]></category>
		<category><![CDATA[Commentary]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=14492</guid>
				<description><![CDATA[Several weeks ago, I had a great conversation with Sebastian Hassinger about the state of quantum computing. It’s exciting–but also, not what a lot of people are expecting. I’ve seen articles in the trade press telling people to invest in quantum computing now or they’ll be hopelessly behind. That’s silly. There are too many people [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>Several weeks ago, I had a great conversation with Sebastian Hassinger about the state of quantum computing. It’s exciting–but also, not what a lot of people are expecting.</p>



<p>I’ve seen articles in the trade press telling people to invest in quantum computing now or they’ll be hopelessly behind. That’s silly. There are too many people in the world who think that a quantum computer is just a fast mainframe. It isn’t; quantum programming is completely different, and right now, the number of algorithms we know that will work on quantum computers is very small. You can count them on your fingers and toes. While it’s probably important to prepare for quantum computers that can decrypt current cryptographic codes, those computers won’t be around for 10-20 years. While there is still debate on how many physical qubits will be needed for error correction, and even on the meaning of a “logical” (error-corrected) qubit, the most common&nbsp; estimates are that it will require on the order of 1,000 error corrected qubits to break current encryption systems, and that it will take 1,000 physical qubits to make one error corrected qubit. So we’ll need an order of 1 million qubits, and current quantum computers are all in the area of 100 qubits. Figuring out how to scale our current quantum computers by 5 orders of magnitude may well be the biggest problem facing researchers, and there’s no solution in sight.</p>



<p>So what can quantum computers do now that’s interesting? First, they are excellent tools for simulating quantum behavior: the behavior of subatomic particles and atoms that make up everything from semiconductors to bridges to proteins. Most, if not all, modeling in these areas is based on numerical methods–and modern digital computers are great at that. But it’s time to think again about non-numerical methods: can a quantum computer simulate directly what happens when two atoms interact? Can it figure out what kind of molecules will be formed, and what their shapes will be? This is the next step forward in quantum computing, and while it’s still research, It’s a significant way forward. We live in a quantum world. We can’t observe quantum behavior directly, but it’s what makes your laptop work and your bridges stay up. If we can model that behavior directly with quantum computers, rather than through numeric analysis, we’ll make a huge step forward towards finding new kinds of materials, new treatments for disease, and more. In a way, it’s like the difference between analog and digital computers. Any engineer knows that digital computers spend a lot of time finding approximate numeric solutions to complicated differential equations. But until digital computers got sufficiently large and fast, the behavior of those systems could be modeled directly on analog computers. Perhaps the earliest known examples of analog computers are <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Stonehenge" target="_blank">Stonehenge</a> and the <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Antikythera_mechanism" target="_blank">Antikythera mechanism</a>, both of which were used to predict astronomical positions. Thousands of years before digital computers existed, these analog computers modeled the behavior of the cosmos, solving equations that their makers couldn’t have understood–and that we now solve numerically on digital computers.</p>



<p>Recently, researchers have developed a <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://arxiv.org/abs/2110.00557" target="_blank">standardized control plane</a> that should be able to work with all kinds of quantum devices. The design of the control plane, including software, is all open source. This should greatly decrease the cost of experimentation, allowing researchers to focus on the quantum devices themselves, instead of designing the circuitry needed to manage the qubits.&nbsp; It’s not unlike the dashboard of a car: relatively early in automotive history, we developed a fairly standard set of tools for displaying data and controlling the machinery.&nbsp; If we hadn’t, the development of automobiles would have been set back by decades: every automaker would need to design its own controls, and you’d need fairly extensive training on your specific car before you could drive it. <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Quantum_programming" target="_blank">Programming languages for quantum devices</a> also need to standardize; fortunately, there has already been a lot of work in that direction.&nbsp; Open source development kits that provide libraries that can be called from Python to perform quantum operations (<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://github.com/Qiskit" target="_blank">Qiskit</a>, <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://searchaws.techtarget.com/definition/Amazon-Braket" target="_blank">Braket</a>, and <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://quantumai.google/cirq" target="_blank">Cirq</a> are some examples), and <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://github.com/Qiskit/openqasm" target="_blank">OpenQASM</a> is an open source “quantum assembly language” that lets programmers write (virtual) machine-level code that can be mapped to instructions on a physical machine.</p>



<p>Another approach to simulating quantum behavior won’t help probe quantum behavior, but might help researchers to develop algorithms for numerical computing. <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://arxiv.org/abs/1809.04028" target="_blank">P-bits</a>, or probabilistic bits, behave probabilistically but don’t depend on quantum physics: they’re traditional electronics that work at room temperature. P-bits have some of the behavior of qubits, but they’re much easier to build; the developers call them “poor man’s qubits.” Will p-bits make it easier to develop a quantum future?&nbsp; Possibly.</p>



<p>It’s important not to get over-excited about quantum computing. The best way to avoid a “trough of disillusionment” is to be realistic about your expectations in the first place. Most of what computers currently do will remain unchanged. There will be some breakthroughs in areas like cryptography, search, and a few other areas where we’ve developed algorithms. Right now, “preparing for quantum computing” means evaluating your cryptographic infrastructure. Given that infrastructure changes are difficult, expensive, and slow, it makes sense to prepare for quantum-safe cryptography now. (Quantum-safe cryptography is cryptography that can’t be broken by quantum computers–it does not require quantum computers.)&nbsp; Quantum computers may still be 20 years in the future, but infrastructure upgrades could easily take that long.</p>



<p>Practical (numeric) quantum computing at significant scale could be 10 to 20 years away, but a few breakthroughs could shorten that time drastically.&nbsp; In the meantime, a lot of work still needs to be done on discovering quantum algorithms. And a lot of important work can already be done by using quantum computers as tools for investigating quantum behavior. It is an exciting time; it’s just important to be excited by the right things, and not misled by the hype.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/quantum-computing-without-the-hype/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>The General Purpose Pendulum</title>
		<link>https://www.oreilly.com/radar/the-general-purpose-pendulum/</link>
				<comments>https://www.oreilly.com/radar/the-general-purpose-pendulum/#respond</comments>
				<pubDate>Tue, 12 Apr 2022 11:59:19 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides]]></dc:creator>
				<category><![CDATA[Hardware]]></category>
		<category><![CDATA[Software Architecture]]></category>
		<category><![CDATA[Software Engineering]]></category>
		<category><![CDATA[Commentary]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=14436</guid>
				<description><![CDATA[Pendulums do what they do: they swing one way, then they swing back the other way.&#160; Some oscillate quickly; some slowly; and some so slowly you can watch the earth rotate underneath them. It’s a cliche to talk about any technical trend as a “pendulum,” though it’s accurate often enough. We may be watching one [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>Pendulums do what they do: they swing one way, then they swing back the other way.&nbsp; Some oscillate quickly; some slowly; and some so slowly you can <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Foucault_pendulum" target="_blank">watch the earth rotate underneath them</a>. It’s a cliche to talk about any technical trend as a “pendulum,” though it’s accurate often enough.</p>



<p>We may be watching one of computing’s longest-term trends turn around, becoming the technological equivalent of Foucault’s very long, slow pendulum: the trend towards generalization. That trend has been swinging in the same direction for some 70 years–since the invention of computers, really.&nbsp; The first computers were just calculating engines designed for specific purposes: breaking codes (in the case of Britain’s <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Bombe" target="_blank">Bombe</a>) or calculating missile trajectories. But those primitive computers soon got the ability to <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Computer" target="_blank">store programs</a>, making them much more flexible; eventually, they became “general purpose” (i.e., business) computers. If you’ve ever seen a manual for the IBM 360’s machine language, you’ll see many instructions that only make sense in a business context–for example, instructions for arithmetic in binary coded decimal.</p>



<p>That was just the beginning. In the 70s, word processors started replacing typewriters. Word processors were essentially early personal computers designed for typing–and they were quickly replaced by personal computers themselves. With the invention of email, computers became communications devices. With file sharing software like Napster and MP3 players like WinAmp, computers started replacing radios–then, when Netflix started streaming, televisions. CD and DVD players are inflexible, task-specific computers, much like word processors or the Bombe, and their functions have been subsumed by general-purpose machines.</p>



<p>The trend towards generalization also took place within software. Sometime around the turn of the millenium, many of us realized the Web browsers (yes, even the early Mosaic, Netscape, and Internet Explorer) could be used as a general user interface for software; all a program had to do was express its user interface in HTML (using forms for user input), and provide a web server so the browser could display the page. It’s not an accident that Java was perhaps the last programming language to have a graphical user interface (GUI) library; other languages that appeared at roughly the same time (Python and Ruby, for example) never needed one.</p>



<p>If we look at hardware, machines have gotten faster and faster–and more flexible in the process. I’ve already mentioned the appearance of instructions specifically for “business” in the IBM 360. GPUs are specialized hardware for high-speed computation and graphics; however, they’re much less specialized than their ancestors, dedicated vector processors.&nbsp; Smartphones and tablets are essentially personal computers in a different form factor, and they have performance specs that beat supercomputers from the 1990s. And they’re also cameras, radios, televisions, game consoles, and even credit cards.</p>



<p>So, why do I think this pendulum might start swinging the other way?&nbsp; A recent article in the <em>Financial Times</em>, <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.ft.com/content/4db69e3c-c901-4776-970e-c57e99f71aba" target="_blank">Big Tech Raises its Bets on Chips</a>, notes that Google and Amazon have both developed custom chips for use in their clouds. It hypothesizes that the next generation of hardware will be one in which chip development is integrated more closely into a wider strategy.&nbsp; More specifically, “the best hope of producing new leaps forward in speed and performance lies in the co-design of hardware, software and neural networks.” Co-design sounds like designing hardware that is highly optimized for running neural networks, designing neural networks that are a good match for that specific hardware, and designing programming languages and tools for that specific combination of hardware and neural network. Rather than taking place sequentially (hardware first, then programming tools, then application software), all of these activities take place concurrently, informing each other. That sounds like a turn away from general-purpose hardware, at least superficially: the resulting chips will be good at doing one thing extremely well. It’s also worth noting that, while there is a lot of interest in quantum computing, quantum computers will inevitably be specialized processors attached to conventional computers. There is no reason to believe that a quantum computer can (or should) run general purpose software such as software that renders video streams, or software that calculates spreadsheets.&nbsp;Quantum computers will be a big part of our future–but not in a general-purpose way. Both co-design and quantum computing step away from general-purpose computing hardware. We’ve come to the end of Moore’s Law, and can’t expect further speedups from hardware itself.&nbsp; We can expect improved performance by optimizing our hardware for a specific task.</p>



<p>Co-design of hardware, software, and neural networks will inevitably bring a new generation of tools to software development. What will those tools be? Our current development environments don’t require programmers to know much (if anything) about the hardware. Assembly language programming is a specialty that’s really only important for embedded systems (and not all of them) and a few applications that require the utmost in performance. In the world of co-design, will programmers need to know more about hardware? Or will a new generation of tools abstract the hardware away, even as they weave the hardware and the software together even more intimately? I can certainly imagine tools with modules for different kinds of neural network architectures; they might know about the kind of data the processor is expected to deal with; they might even allow a kind of “pre-training”–something that could ultimately give you GPT-3 on a chip. (Well, maybe not on a chip. Maybe a few thousand chips designed for some distributed computing architecture.) Will it be possible for a programmer to say “This is the kind of neural network I want, and this is how I want to program it,” and let the tool do the rest? If that sounds like a pipe-dream, realize that tools like GitHub Copilot are already automating programming.</p>



<p>Chip design is the poster child for “the first unit costs 10 billion dollars; the rest are all a penny apiece.”&nbsp; That has limited chip design to well-financed companies that are either in the business of selling chips (like Intel and AMD) or that have specialized needs and can buy in very large quantities themselves (like Amazon and Google). Is that where it will stop–increasing the imbalance of power between a few wealthy companies and everyone else–or will co-design eventually enable smaller companies (and maybe even individuals) to build custom processors? To me, co-design doesn’t make sense if it’s limited to the world’s Amazons and Googles. They can already design custom chips.&nbsp; It’s expensive, but that expense is itself a moat that competitors will find hard to cross.&nbsp;Co-design is about improved performance, yes; but as I’ve said, it’s also inevitably about improved tools.&nbsp; Will those tools result in better access to semiconductor fabrication facilities?</p>



<p>We’ve seen that kind of transition before.&nbsp;Designing and making printed circuit boards used to be hard. I tried it once in high school; it requires acids and chemicals you don’t want to deal with, and a hobbyist definitely can’t do it in volume. But now, it’s easy: you design a circuit with a <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.electronics-lab.com/top-10-free-pcb-design-software-2019/" target="_blank">free tool like Kicad or Fritzing</a>, have the tool generate a board layout, send the layout to a vendor through a web interface, and a few days later, a package arrives with your circuit boards. If you want, you can have the vendor source the board’s components and solder them in place for you. It costs a few tens of dollars, not thousands. Can the same thing happen at the chip level? It hasn’t yet. We’ve thought that field-programmable gate arrays might eventually democratize chip design, and to a limited extent, they have. FPGAs aren’t hard for small- or mid-sized businesses that can afford a few hardware engineers, but they’re far from universal, and they definitely haven’t made it to hobbyists or individuals.&nbsp; Furthermore, FPGAs are still standardized (generalized) components; they don’t democratize the semiconductor fabrication plant.</p>



<p>What would “cloud computing” look like in a co-designed world? Let’s say that a mid-sized company designs a chip that implements a specialized language model, perhaps something like <a href="https://learning.oreilly.com/answers/search/">O’Reilly Answers</a>. Would they have to run this chip on their own hardware, in their own datacenter?&nbsp; Or would they be able to ship these chips to Amazon or Google for installation in their AWS and GCP data centers?&nbsp; That would require a lot of work standardizing the interface to the chip, but it’s not inconceivable.&nbsp; As part of this evolution, the co-design software will probably end up running in someone’s cloud (much as <a href="https://aws.amazon.com/pm/sagemaker/">AWS Sagemaker</a> does today), and it will “know” how to build devices that run on the cloud provider’s infrastructure. The future of cloud computing might be running custom hardware.</p>



<p>We inevitably have to ask what this will mean for users: for those who will use the online services and physical devices that these technologies enable. We may be seeing that pendulum swing back towards specialized devices. A product like Sonos speakers is essentially a re-specialization of the device that was formerly a stereo system, then became a computer. And while I (once) lamented the idea that we’d eventually all wear jackets with innumerable pockets filled with different gadgets (iPods, i-Android-phones, Fitbits, Yubikeys, a collection of dongles and earpods, you name it), some of those products make sense:&nbsp; I lament the loss of the iPod, as distinct from the general purpose phone. A tiny device that could carry a large library of music, and do nothing else, was (and would still be) a wonder.</p>



<p>But those re-specialized devices will also change. A Sonos speaker is more specialized than a laptop plugged into an amp via the headphone jack and playing an MP3; but don’t mistake it for a 1980s stereo, either. If inexpensive, high-performance AI becomes commonplace, we can expect a new generation of exceedingly smart devices. That means voice control that really works (maybe even for <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.scientificamerican.com/article/speech-recognition-tech-is-yet-another-example-of-bias/%5C" target="_blank">those who speak with an accent</a>), locks that can identify people accurately regardless of skin color, and appliances that can diagnose themselves and call a repairman when they need to be fixed. (I’ve always wanted a furnace that could notify my service contractor when it breaks at 2AM.) Putting intelligence on a local device could improve privacy–the device wouldn&#8217;t need to send as much data back to the mothership for processing. (We’re already seeing this on Android phones.) We might get autonomous vehicles that communicate with each other to optimize traffic patterns. We might go beyond voice controlled devices to non-invasive brain control. (Elon Musk’s Neuralink has the right idea, but few people will want sensors surgically embedded in their brains.)</p>



<p>And finally, as I write this, I realize that I’m writing on a laptop–but I don’t want a better laptop. With enough intelligence, would it be possible to build environments that are aware of what I want to do?&nbsp;And offer me the right tools when I want them (possibly something like Bret Victor’s <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://dynamicland.org/" target="_blank">Dynamicland</a>)? After all, we don’t really want computers.&nbsp; We want “<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.youtube.com/watch?v=ob_GX50Za6c" target="_blank">bicycles for the mind</a>”–but in the end, Steve Jobs only gave us computers.</p>



<p>That’s a big vision that will require embedded AI throughout. It will require lots of very specialized AI processors that have been optimized for performance and power consumption. Creating those specialized processors will require re-thinking how we design chips. Will that be co-design, designing the neural network, the processor, and the software together, as a single piece? Possibly. It will require a new way of thinking about tools for programming–but if we can build the right kind of tooling, “possibly” will become a certainty.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/the-general-purpose-pendulum/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>D-Day in Kyiv</title>
		<link>https://www.oreilly.com/radar/d-day-in-kyiv/</link>
				<comments>https://www.oreilly.com/radar/d-day-in-kyiv/#respond</comments>
				<pubDate>Tue, 22 Mar 2022 18:02:51 +0000</pubDate>
		<dc:creator><![CDATA[Jeffrey Carr]]></dc:creator>
				<category><![CDATA[Security]]></category>
		<category><![CDATA[Commentary]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=14392</guid>
				<description><![CDATA[My experience working with Ukraine’s Offensive Cyber Team By Jeffrey CarrMarch 22, 2022 When Russia invaded Ukraine on February 24th,  I had been working with two offensive cyber operators from GURMO—Main Intelligence Directorate of the Ministry of Defense of Ukraine—for several months trying to help them raise funds to expand development on an OSINT (Open [&#8230;]]]></description>
								<content:encoded><![CDATA[
<h2>My experience working with Ukraine’s
Offensive Cyber Team</h2>



<p>By Jeffrey Carr<br>March 22, 2022</p>



<p>When Russia invaded Ukraine on February 24th,  I had been working with two offensive cyber operators from GURMO—Main Intelligence Directorate of the Ministry of Defense of Ukraine—for several months trying to help them raise funds to expand development on an OSINT (Open Source Intelligence) platform they had invented and were using to identify and track Russian terrorists in the region. Since the technology was sensitive, we used Signal for voice and text calls. There was a lot of tension during the first few weeks of February due to Russia’s military buildup on Ukraine’s borders and the uncertainty of what Putin would do. </p>



<p>Then on February 24th at 6am in Kyiv (February 23, 8pm in Seattle where I live), it happened.</p>



<p><strong>SIGNAL log 23 FEB 2022 20:00 (Seattle)&nbsp; / 24 FEB 2022 06:00 (Kyiv)</strong></p>



<pre class="wp-block-preformatted">Missed audio call - 8:00pm</pre>



<pre class="wp-block-preformatted">It started
8:01PM</pre>



<pre class="wp-block-preformatted">                    War?
                    9:36PM</pre>



<pre class="wp-block-preformatted">Incoming audio call - 9:37PM</pre>



<pre class="wp-block-preformatted">                    Call dropped.
                    9:41PM</pre>



<pre class="wp-block-preformatted">                    Are you there?
                    9:42PM</pre>



<p>I didn’t hear from my GURMO friend again for 10 hours. When he pinged me on Signal, it was from a bunker. They were expecting another missile attack at any moment. </p>



<p>“<code>Read this</code>”, he said, and sent me this <a href="https://zn.ua/ukr/POLITICS/ukrajinski-spetssluzhbi-volodijut-informatsijeju-pro-plan-putina-shchodo-kijeva-ta-ukrajini.html">link</a>. “<code>Use Google Translate.</code>”</p>



<p>It linked to an article that described Russia’s operations plan for its attack on Ukraine, obtained by sources of Ukrainian news website <a href="https://zn.ua/ukr/POLITICS/ukrajinski-spetssluzhbi-volodijut-informatsijeju-pro-plan-putina-shchodo-kijeva-ta-ukrajini.html">ZN.UA</a>. It said that the Russian military had sabotage groups already placed in Ukraine whose job was to knock out power and communications in the first 24 hours in order to cause panic. Acts of arson and looting would follow, with the goal of distracting law enforcement from chasing down the saboteurs. Then, massive cyber attacks would take down government websites, including the Office of the President, the General Staff, the Cabinet, and the Parliament (the Verkhovna Rada). The Russian military expected little resistance when it moved against Kyiv and believed that it could capture the capital in a matter of days.</p>



<blockquote class="wp-block-quote"><p><em>The desired result is to seize the leadership of the state (it is not specified who exactly) and force a peace agreement to be signed on Russian terms under blackmail and the possibility of the death of a large number of civilians.</em></p><p><em>Even if part of the country&#8217;s leadership is evacuated, some pro-Russian politicians will be able to &#8220;take responsibility&#8221; and sign documents, citing the &#8220;escape&#8221; of the political leadership from Kyiv.</em></p><p><em>As a result, Ukraine can be divided into two parts—on the principle of West and East Germany, or North and South Korea.</em></p><p><em>At the same time, the Russian Federation recognizes the legitimate part of Ukraine that will sign these agreements and will be loyal to the Russian Federation. Guided by the principle: &#8220;he who controls the capital—he controls the state.&#8221;</em></p></blockquote>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/netblocks.png" alt="" class="wp-image-14396" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/netblocks.png 504w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/netblocks-260x300.png 260w" sizes="(max-width: 504px) 100vw, 504px" /></figure>



<p>The first significant Russian cyber attack of
the war is suspected to be the one that took down satellite provider ViaSat at
precisely 06:00 Kyiv time (04:00 UTC), the exact time that Russia started its
invasion. </p>



<p>The cause is believed to be a malicious
firmware update sent to ViaSat customers that “bricked” the satellite modems.
Since ViaSat is a defense contractor, the NSA, France’s ANSSI, and Ukrainian
Intelligence are investigating. ViaSat hired Mandiant to handle digital
forensics and incident response (DFIR). </p>



<p>“<code>Is Ukraine planning to retaliate?</code>”, I asked.</p>



<p>“<code>We’re engaging in six hours. I’ll keep you informed.</code>”</p>



<p>That last exchange happened about 22 hours
after the start of the war. </p>



<p><strong>FRIDAY,
FEB 25, 2022 07:51</strong></p>



<p>I received a Signal alert. </p>



<p>“<code>Download ready</code>” and a link.</p>



<p>The GURMO cyber team had gained access to the accounting and document management system at Russian Military Unit 6762, part of the Ministry of Internal Affairs that deals with riot control, terrorists, and the territorial defense of Russia. They downloaded all of their personnel data, including passports, military IDs, credit cards, and payment records. I was sent a sampling of documents to do further research and post via my channels.</p>



<p>The credit cards were all issued by Sberbank. “<code>What are you going to do with these</code>”, I asked. He sent me a wink and a grin icon on Signal and said:</p>



<pre class="wp-block-preformatted">Buy weapons and ammo for our troops! 
We start again at 6:30am tomorrow. 
When you wake up, join us.
                    
                    Will do!</pre>



<p>Over the next few days, GURMO’s offensive
cyber team hacked a dizzying array of Russian targets and stole thousands of
files from:</p>



<ul><li>Black Sea Fleet’s communications
servers</li><li>ROSATOM</li><li>FSB Special Operations unit 607</li><li>Sergey G. Buev, the Chief Missile
Officer of the Ministry of Defense</li><li>Federal Air Transport Agency</li></ul>



<p>Everything was in Russian, so the translation process was very time-consuming. There were literally hundreds of documents in all different file types, and to make the translation process even harder, many of the documents were images of a document. You can’t just upload those into Google Translate. You have to download the Google Translate app onto your mobile phone, then point it at the document on your screen and read it that way.</p>



<p>Once I had read enough, I could write a post at my Inside Cyber Warfare <a href="https://jeffreycarr.substack.com/">Substack</a> that provided information and context to the breach. Between the translation, research, writing, and communication with GURMO ,who were 11 hours ahead (10 hours after the time change), I was getting about 4 ½ hours of sleep each night. </p>



<h2>We Need Media Support</h2>



<p><strong>TUESDAY,
MARCH 1, 2022 09:46 (Seattle)</strong></p>



<p>On Signal</p>



<pre class="wp-block-preformatted">We need media support from USA.
All the attacks you mentioned during these 6 days.
We have to make headlines to demoralize Russians.

                   I know the team at a young British PR firm.
                   I’ll check with them now.</pre>



<p>Nara Communications immediately stepped up to the challenge. They agreed to waive their fee and help place news stories about the GURMO cyber team’s successes. The Ukrainians did their part and gave them some amazing breaches, starting with the Beloyarsk Nuclear Power Plant—the world’s only commercial fast breeder reactors. Other countries were spending billions of dollars trying to achieve what Russia had already mastered, so a breach of their design documents and processes was a big deal. </p>



<p>The problem was that journalists wanted to
speak to GURMO and that was off the table for three important reasons:</p>



<ol><li>They were too busy fighting a war to give interviews.</li><li>The Russian government knew who they were, and their names and faces were on the playing cards given to Kadryov’s Chechen Guerillas for assassination.</li><li>They didn’t want to expose themselves to facial recognition or voice capture technologies because&#8230;see #2. </li></ol>



<p>Journalists had only a few options if they didn’t want to run with a single-source story. </p>



<p>They could speak with me because I was the only person who the GURMO team would directly speak to. Plus, I had possession of the documents and understood what they were.</p>



<p>They could contact the CIA Legat in Warsaw, Poland where the U.S. embassy had evacuated to prior to the start of the war. GURMO worked closely with and gave frequent briefings to its allied partners, and they would know about these breaches. Of course, the CIA most likely wouldn’t speak with a journalist. </p>



<p>They could speak with other experts to vet the documents, which would effectively be their second source after speaking with me. Most reporters at major outlets didn’t bother reporting these breaches under those conditions. To make matters worse, there were no obvious victims. The GURMO hackers weren’t breaking things, they were stealing things, and they liked to keep a persistent presence in the network so they could keep coming back for more. Plus, Russia often implemented a communications strategy known as Ихтамнет (Ihtamnet), which roughly translated means “nothing happened” or to put it into context “What hacks? There were no hacks.”</p>



<p>In spite of all those obstacles, Nara Communications was successful in getting an article placed with SC magazine, a radio interview with Britain’s <em>The Times</em>, and a podcast with the <em>Evening Standard</em>. </p>



<p>By mid-March, Putin showed no signs of wanting
peace, even after President Zelensky had conceded that NATO membership was
probably off the table for Ukraine, and GURMO was popping bigger targets than
ever.</p>



<p>The Russians&#8217; plan to establish a fully automated lunar base called Luna-Glob was breached. Russia’s EXOMars project was breached. The new launch complex being built at Vostochny for the Angara rocket was breached. In every instance, a trove of files was downloaded for study by Ukraine’s government and shared with its allies. A small amount was always carved out for me to review, post at the <a href="https://jeffreycarr.substack.com/">Inside Cyber Warfare</a> Substack, and share with journalists. Journalist <a href="https://www.scmagazine.com/analysis/breach/in-a-first-ukraine-leaks-russian-intellectual-property-as-act-of-war">Joe Uchill</a> referred to this strategy as Hack and Leak. </p>



<h2>Hack and Leak</h2>



<p>By hacking some of Russia’s proudest
accomplishments (its space program) and most successful technologies (its
nuclear research program), the Ukrainian government is sending Putin a message
that your cybersecurity systems cannot keep us out, that even your most
valuable technological secrets aren’t safe from us, and that if you push us too
far, we can do whatever we want to your networks. </p>



<p>Apart from the attack on ViaSat, there hasn’t been evidence of any destructive cyber attacks against Ukrainian infrastructure. Part of that was strategic planning on the part of Ukraine (that’s all that I can say about that), part was Ukraine’s cyber defense at work, and part of that may be that GURMO’s strategy is working. However, there’s no sign that these leaks are having any effect on impeding Russia’s military escalation, probably because that’s driven out of desperation in the face of its enormous military losses so far. Should that escalation continue, GURMO has contingency plans that will bring the war home to Russia. </p>



<hr class="wp-block-separator" />



<p>Jeffrey Carr has been an internationally-known cybersecurity adviser, author, and researcher since 2006. He has worked as a Russia SME for the CIA&#8217;s Open Source Center Eurasia Desk. He invented REDACT, the world’s first global R&amp;D database and search engine to assist companies in identifying which intellectual property is of value to foreign governments. He is the founder and organizer of Suits &amp; Spooks, a “collision” event to discuss hard challenges in the national security space, and is the author of <em>Inside Cyber Warfare: Mapping the Cyber Underworld</em> (O&#8217;Reilly Media, 2009, 2011).&nbsp;</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/d-day-in-kyiv/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>Epstein Barr and the Cause of Cause</title>
		<link>https://www.oreilly.com/radar/epstein-barr-and-the-cause-of-cause/</link>
				<comments>https://www.oreilly.com/radar/epstein-barr-and-the-cause-of-cause/#respond</comments>
				<pubDate>Tue, 08 Mar 2022 12:17:00 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides]]></dc:creator>
				<category><![CDATA[Radar Column]]></category>
		<category><![CDATA[Commentary]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=14326</guid>
				<description><![CDATA[One of the most intriguing news stories of the new year claimed that the Epstein-Barr virus (EBV) is the &#8220;cause&#8221; of Multiple Sclerosis (MS), and suggested that antiviral medications or vaccinations for Epstein-Barr could eliminate MS. I am not an MD or an epidemiologist. But I do think this article forces us to think about [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>One of the most intriguing news stories of the new year claimed that the <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.hsph.harvard.edu/news/press-releases/epstein-barr-virus-may-be-leading-cause-of-multiple-sclerosis/" target="_blank">Epstein-Barr virus (EBV) is the &#8220;cause&#8221; of Multiple Sclerosis</a> (MS), and suggested that antiviral medications or vaccinations for Epstein-Barr could eliminate MS.</p>



<p>I am not an MD or an epidemiologist. But I do think this article forces us to think about the meaning of &#8220;cause.&#8221; Although Epstein-Barr isn&#8217;t a familiar name, it&#8217;s extremely common; a good estimate is that 95% of the population is infected with it. It&#8217;s a variant of Herpes; if you&#8217;ve ever had mononucleosis, you&#8217;ve had it; most infections are asymptomatic. We hear much more about MS; I&#8217;ve had friends who have died from it. But MS is much less common: about 0.036% of the population has it (<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7720355/" target="_blank">35.9 per 100,000</a>).</p>



<p>We know that causation isn&#8217;t a one-size-fits-all thing: if X happens, then Y always happens. Lots of people smoke; we know that smoking causes lung cancer; but many people who smoke don&#8217;t get lung cancer. We&#8217;re fine with that; the causal connection has been painstakingly documented in great detail, in part because the tobacco industry went to such great lengths to spread misinformation.</p>



<p>But what does it mean to say that a virus that infects almost everyone causes a disease that affects very few people? The researchers appear to have done their job well. They studied 10 million people in the US military. 5 percent of those were negative for Epstein-Barr at the start of their service. 955 of that group were eventually diagnosed with MS, and had been infected with EBV prior to their MS diagnosis, indicating a risk factor 32 times higher than for those without EBV.</p>



<p>It is certainly fair to say that Epstein-Barr is implicated in MS, or that it contributes to MS, or some other phrase (that could not unreasonably be called &#8220;weasel words&#8221;). Is there another trigger that only has an effect when EBV is already present? Or is EBV the sole cause of MS, a cause that just doesn&#8217;t take effect in the vast majority of people?</p>



<p>This is where we have to think very carefully about causality, because as important as this research is, it seems like something is missing. An omitted variable, perhaps a genetic predisposition? Some other triggering condition, perhaps environmental? Cigarettes were clearly a &#8220;smoking gun&#8221;:&nbsp; <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.medicinenet.com/what_percentage_of_smokers_get_lung_cancer/article.htm" target="_blank">10 to 20 percent of smokers develop lung cancer</a> (to say nothing of other diseases). EBV may also be a smoking gun, but one that only goes off rarely.</p>



<p>If there are no other factors, we&#8217;re justified in using the word &#8220;causes.&#8221; But it&#8217;s hardly satisfying—and that&#8217;s where the more precise language of causal inference runs afoul of human language. Mathematical language is more useful: Perhaps EBV is &#8220;necessary&#8221; for MS (i.e., EBV is required; you can&#8217;t get MS without it), but clearly not &#8220;sufficient&#8221; (EBV doesn&#8217;t necessarily lead to MS). Although once again, the precision of mathematics may be too much.</p>



<p>Biological systems aren&#8217;t necessarily mathematical, and it is possible that there is no &#8220;sufficient&#8221; condition; EBV just leads to MS in an extraordinarily small number of instances. In turn, we have to take this into account in decision-making. Does it make sense to develop a vaccine against a rare (albeit tragic, disabling, and inevitably fatal) disease? If EBV is implicated in other diseases, possibly. However, vaccines aren&#8217;t without risk (or expense), and even though the risk is very small (as it is for all the vaccines we use today), it&#8217;s not clear that it makes sense to take that risk for a disease that very few people get. How do you trade off a small risk against a very small reward? Given the anti-vax hysteria around COVID, requiring children to be vaccinated for a rare disease might not be poor public health policy; it might be the end of public health policy.</p>



<p>More generally: how do you build software systems that predict rare events? This is another version of the same problem—and unfortunately, the policy decision we are least likely to make is not to create such software. The abuse of such systems is a clear and present danger: for example, AI systems that pretend to predict &#8220;criminal behavior&#8221; on the basis of everything from crime data to <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.bbc.com/news/technology-53165286" target="_blank">facial images</a>, are already being developed. Many <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm" target="_blank">are already in use</a>, and in <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.brennancenter.org/our-work/research-reports/predictive-policing-explained" target="_blank">high demand</a> from law enforcement agencies. They will certainly generate far more false positives than true positives, stigmatizing thousands (if not millions) of people in the process. Even with carefully collected, unbiased data (which doesn&#8217;t exist), and assuming some kind of causal connection between past history, physical appearance, and future criminal behavior (as in the discredited 19th century pseudoscience of <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Physiognomy" target="_blank">physiognomy</a>), it is very difficult, if not impossible, to reason from a relatively common cause to a very rare effect. Most people don&#8217;t become criminals, regardless of their physical appearance. Deciding a priori who will can only become an exercise in applied racism and bias.</p>



<p>Virology aside, the Epstein-Barr virus has one thing to teach us. How do we think about a cause that rarely causes anything? That is a question we need to answer.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/epstein-barr-and-the-cause-of-cause/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>The Human Web</title>
		<link>https://www.oreilly.com/radar/the-human-web/</link>
				<comments>https://www.oreilly.com/radar/the-human-web/#respond</comments>
				<pubDate>Tue, 08 Feb 2022 12:36:48 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides]]></dc:creator>
				<category><![CDATA[Web]]></category>
		<category><![CDATA[Commentary]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=14301</guid>
				<description><![CDATA[A few days ago, I recommended that Tim O&#8217;Reilly invite someone to our next FOO Camp. I thought she had been to a prior FOO event, though I didn&#8217;t meet her there; I&#8217;d had a prior conversation with her about data governance (I think), and gotten on her mailing list, which reminded me that she [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>A few days ago, I recommended that Tim O&#8217;Reilly invite someone to our next FOO Camp. I thought she had been to a prior FOO event, though I didn&#8217;t meet her there; I&#8217;d had a prior conversation with her about data governance (I think), and gotten on her mailing list, which reminded me that she was doing very interesting work. I don&#8217;t remember who introduced us, except that it was someone who had met her at the earlier FOO event.</p>



<p>That may sound convoluted. That&#8217;s the point. This is a very human web. It&#8217;s a very small window onto a web of introductions. At the start of almost every FOO camp, Tim says that FOO is about &#8220;creating synapses in the global brain.&#8221; He&#8217;s said many times that he sees his function as introducing people who should know each other. That web of connections—what we used to call the &#8220;social graph&#8221;—is very broad. It eventually includes all 7+ billion of us. And again, it is intensely human. It&#8217;s Web0.</p>



<p>It&#8217;s necessary to remind ourselves of that when we talk about Web3. Web3 will succeed, or fail, to the extent that it solves human problems, to the extent that it makes navigating Web0 more tractable—not to the extent that it monetizes everything conceivable, or enables a small number of people to make a financial killing. Making it possible for artists to earn a living is solving a human problem (though we won&#8217;t know whether NFTs actually do that until we&#8217;re past the initial bubble). Using links that incorporate history to build communities of people who care about the same things, as Chris Anderson <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://thenewstack.io/nfts-in-the-uncanny-valley/" target="_blank">suggests</a>, is solving a human problem.</p>



<p>Once we realize that, Web3 isn&#8217;t all that different from the earlier generations of the web. Facebook succeeded because it solved a human problem: People want to associate, to congregate. Facebook may have been a poor solution (it certainly became a poor solution after it decided to prioritize &#8220;engagement&#8221;), but it was a solution. Google succeeded because it solved a different human problem: finding information. The world&#8217;s information was radically decentralized, stored in millions of books and websites. At O&#8217;Reilly, we made one of the first attempts to manage that rapidly growing mess, but our solution, publishing <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.amazon.com/Whole-Internet-Users-Guide-Catalog/dp/1565920635" target="_blank">The Whole Internet</a> and creating <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Global_Network_Navigator" target="_blank">a web portal</a> (the industry’s first) based on it, couldn&#8217;t scale the way Google did five years later. As Larry Page and Sergey Brin discovered, organizing the world&#8217;s information was about computing the tree of relationships dynamically. Like Facebook, Google has become less useful over time, as <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://qz.com/1666863/why-big-tech-keeps-outsmarting-antitrust-regulators/" target="_blank">it seems to have compromised its results to &#8220;maximize shareholder value.&#8221;</a> I would certainly prefer burying monopolies to praising them. But it&#8217;s important to think carefully about what they do well. Google and Facebook, like AT&amp;T before them, succeeded because they solved problems that people cared about solving. Their solutions had real lasting value.</p>



<p>Cryptocurrency provides a cautionary tale. Blockchains may be a brilliant solution to the problem of double-spending. But double spending is a problem very few people have, while theft and other financial crimes on the blockchain are growing every day. (Given the rate at which cryptocurrency crime is growing, perhaps we should be glad that double-spend isn’t just another problem on the very long list.) The catalog of failed startups is full of businesses with ideas that were very cool, but didn&#8217;t actually solve problems that people care about, or didn’t think through the new problems that they would create. As technologists, we&#8217;re unfortunately addicted to the cool and the clever.</p>



<p>Can Web3 make Web0, the web of human interconnections and interests, more manageable? Can it solve human problems, not just abstract computational problems, and do so without creating more problems of its own? Can it help us build new synapses in the human brain, or will it just connect us to people who infuriate us?&nbsp; That&#8217;s the challenge Web3 faces. I think it can meet that challenge; but doing so will require developers to understand that blockchains, NFTs, Dapps, and so on are the means, not the ends. They&#8217;re the components, not the finished product.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/the-human-web/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>Andy Warhol, Clay Christensen, and Vitalik Buterin walk into a bar</title>
		<link>https://www.oreilly.com/radar/andy-warhol-clay-christensen-and-vitalik-buterin-walk-into-a-bar/</link>
				<comments>https://www.oreilly.com/radar/andy-warhol-clay-christensen-and-vitalik-buterin-walk-into-a-bar/#respond</comments>
				<pubDate>Wed, 26 Jan 2022 20:47:52 +0000</pubDate>
		<dc:creator><![CDATA[Tim O’Reilly]]></dc:creator>
				<category><![CDATA[Web]]></category>
		<category><![CDATA[Commentary]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=14292</guid>
				<description><![CDATA[In 1962, Daniel Boorstin crystallized a notion that had been around since at least the 1890s, writing of the new kind of celebrities: “Their chief claim to fame is their fame itself. They are notorious for their notoriety.” The same might be said of cryptocurrencies, NFTs, and meme stocks: They are valuable for being valuable. [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>In 1962, Daniel Boorstin crystallized a notion that had been around <a rel="noreferrer noopener" href="https://quoteinvestigator.com/2019/12/21/famous/" target="_blank">since at least the 1890s</a>, writing of the new kind of celebrities: “Their chief claim to fame is their fame itself. They are notorious for their notoriety.” The same might be said of cryptocurrencies, <a rel="noreferrer noopener" href="https://www.nytimes.com/2021/03/24/opinion/what-are-nfts.html" target="_blank">NFTs</a>, and <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Meme_stock" target="_blank">meme stocks</a>: They are valuable for being valuable.</p>



<p>So were the rare tulip bulbs whose prices rose to such heights in 17th-century Holland that the “<a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Tulip_mania" target="_blank">tulip bubble</a>” has been the standard to which other financial manias have been compared since. Exactly what drove the bubble is unclear: Futures markets had just been introduced, and tulips were one of the first speculative commodities to be explored. Imports of plants from distant regions, new technologies of plant breeding, and financial innovation made for a heady mix. The prosperity of the rising Dutch colonial empire may have, like today, produced abundant capital eager to be invested and looking for outsized returns in a market that offered tantalizing prospects. People bought tulip bulbs at outrageous prices with the seemingly reasonable expectation that they could sell them for even higher prices in future.</p>



<p>But the idea that crypto is simply a bubble may miss something important that this suite of technologies has to teach us about the economy. In <a rel="noreferrer noopener" href="https://books.google.com/books?id=gViwLbCJ7X0C" target="_blank"><em>Tulipmania</em></a>, written in 2007, Anne Goldgar made the case that the tulip mania was far less widespread and damaging than outlined in Charles Mackay’s 1841 book <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Extraordinary_Popular_Delusions_and_the_Madness_of_Crowds" target="_blank"><em>Extraordinary Popular Delusions and the Madness of Crowds</em></a>, which had made it so notorious. But even in minimizing its impact, she agreed that the tulip bubble called into question the very nature of what constitutes value:</p>



<blockquote class="wp-block-quote"><p>In the 17th century, it was unimaginable to most people that something as common as a flower could be worth so much more money than most people earned in a year. The idea that the prices of flowers that grow only in the summer could fluctuate so wildly in the winter, threw into chaos the very understanding of &#8220;value&#8221;.</p></blockquote>



<p>The question of what makes things “valuable” in the first place is a wonderful lens through which to think about cryptocurrencies, NFTs, and meme stocks. As economist Mariana Mazzucato outlines in her book <a rel="noreferrer noopener" href="https://marianamazzucato.com/books/the-value-of-everything/" target="_blank"><em>The Value of Everything</em></a>, the notion of value is not fixed. For early economists, only land and agricultural production created value. Trade, finance, and the power of princes were just moving that value around. By the time of Adam Smith, manufacturing was also understood to create value, but trade and finance—well, they were still just moving that value around. Over time, trade, finance, and entertainment have been brought inside what Mazzucato calls “the value boundary.” Meanwhile, household labor—child rearing, caring for aging parents, cooking, cleaning, and the like done by members of a household rather than purchased as a service—is clearly intrinsically valuable, even essential, but still remains so far outside the value boundary that it remains unpaid and isn&#8217;t even counted as part of GDP. So too, government is widely derided as an extractor rather than a creator of value, despite the efforts of Mazzucato and others to point out <a rel="noreferrer noopener" href="https://marianamazzucato.com/books/the-entrepreneurial-state/" target="_blank">its contributions to innovation and economic growth</a>.</p>



<p>Entertainment is a particularly relevant case in point for how sectors cross the value boundary. Adam Smith thought that opera singers, actors, dancers, and the like were frivolous and created no value for society. Today, many of our most highly paid professionals are entertainers: actors, musicians, athletes, TikTok stars and other social media influencers. Creativity has moved to the heart of today’s internet-fueled “<a rel="noreferrer noopener" href="https://www.nytimes.com/2021/02/04/opinion/michael-goldhaber-internet.html" target="_blank">attention economy</a>.” (OK, maybe politics competes with it for that position, but modern politics shares with creative expression the bestowing of status through attention.) At the same time, much of what people do to entertain each other—both in person and on social media—remains unpaid and treated as outside the value boundary.</p>



<p>The question of how much value is being created by a new sector is not settled quickly when the boundary shifts. Finance is a good example. After the financial crisis of 2009, Lloyd Blankfein said with a straight face that <a rel="noreferrer noopener" href="https://palladiummag.com/2019/11/21/mariana-mazzucato-has-reinvigorated-the-most-important-battle-in-economics/" target="_blank">Goldman Sachs financiers were the most productive workers in the world</a>, even as their machinations brought the global economy to the brink of collapse.</p>



<p>The financial industry is in theory a key enabler of the rest of the economy, managing the flows of capital that allow businesses to invest, to hire, and to build and deliver new products and services. But a large part of finance operates in what we might call the “<a rel="noreferrer noopener" href="https://www.oreilly.com/radar/why-elon-musk-is-so-rich/" target="_blank">betting economy</a>.” Hedge funds and other investors place bets on the direction of interest rates and the price of commodities or company stocks, and build sophisticated financial instruments to harvest profits from changes in those prices, regardless of their direction. Are these people creating value when they place these bets, or are they merely extracting it from someone else in a zero-sum game? That question remains up for debate. Nonetheless, those bets eventually are settled based on some measurable impact in the operating economy. What did the Fed do to interest rates? What were people willing to pay for corn or soybeans or scrap iron? What were Apple’s or Amazon’s or Tesla’s profits, and were they growing or shrinking?</p>



<p>With crypto and Web3 more generally, there is a similar kind of real-world bet that blockchain technology will reshape the plumbing of the financial industry. If it succeeds, the winners will eventually be rewarded with enormous profits, justifying the price that has been paid. Crypto might be a bubble, a flash in the pan that will enrich some speculators while impoverishing others. But it might also be a fundamental innovation that will lead to greater prosperity for all of society. And to many, that’s a bet worth placing.</p>



<p>However, much of the betting is not on the intrinsic value that crypto technologies might deliver in the future. Economist John Maynard Keynes <a rel="noreferrer noopener" href="https://thedecisionlab.com/reference-guide/psychology/the-keynesian-beauty-contest/" target="_blank">compared financial markets to a beauty contest</a> in which the point isn’t to pick the most beautiful contestant but to choose the one that everyone else will think is the most beautiful. And since everyone is playing the game, you’re trying to outguess other people who are constantly changing their votes based on what they think you and others are going to choose. What Keynes didn’t emphasize: <em>it’s a contest!</em> Rich people who have already met their every economic need continue to bet just for the sheer pleasure and addictiveness of playing.</p>



<p>NFTs and meme stocks are out at the bleeding edge of this betting economy, because they are largely untethered from traditional notions of value derived from profits in the operating economy. They might best be described as the tokens in a futures market for attention. Like tulips in 17th-century Holland, they represent a challenge to the very notion of “intrinsic value.”</p>



<p><a rel="noreferrer noopener" href="https://warzel.substack.com/p/the-absurdity-is-the-point" target="_blank">Charlie Warzel captured</a> perfectly the puzzlement that many people are feeling:</p>



<blockquote class="wp-block-quote"><p>When I say I’m thinking a lot about cryptocurrency, what I really mean is that I’m thinking a lot about absurdity. I’m thinking about the way that groups of people who are good at harnessing attention are giddily, proudly using that power to drag absurdist memes/currencies/fortunes into mainstream discourse and force the rest of us to care about/debate/or at least know about it all.</p></blockquote>



<p>And that’s the point where artist and impresario Andy Warhol, innovation expert Clayton Christensen, and Etherum creator Vitalik Buterin walk into the bar. They don’t start out talking about crypto, but like everyone else, they end up there.</p>



<blockquote class="wp-block-quote"><p><a rel="noreferrer noopener" href="https://www.amazon.com/Philosophy-Andy-Warhol-Back-Again/dp/0156717204" target="_blank">Andy Warhol says</a>: “What&#8217;s great about this country is that America started the tradition where the richest consumers buy essentially the same things as the poorest. You can be watching TV and see Coca-Cola, and you know that the President drinks Coca-Cola, Liz Taylor drinks Coca-Cola, and just think, you can drink Coca-Cola, too. A Coke is a Coke and no amount of money can get you a better Coke than the one the bum on the corner is drinking. All the Cokes are the same and all the Cokes are good. Liz Taylor knows it, the President knows it, the bum knows it, and you know it.”</p><p><a rel="noreferrer noopener" href="https://claytonchristensen.com/" target="_blank">Clay Christensen</a> replies: It’s worth noticing that a soft drink like Coke is basically a commodity—carbonated and flavored sugar water—mixed with a whole lot of marketing and branding. That’s actually the secret of the modern economy. I call it the <a rel="noreferrer noopener" href="https://store.hbr.org/product/breakthrough-ideas-for-2004-the-hbr-list/R0402A" target="_blank">law of conservation of attractive profits</a>. “When attractive profits disappear at one stage in the value chain because a product becomes commoditized, the opportunity to earn attractive profits with proprietary products usually emerges at an adjacent stage.”</p><p>Tim O’Reilly and I had a real mind meld about that at the Open Source Business Conference in 2004, Clay continues. Tim gave a talk about how <a rel="noreferrer noopener" href="https://www.oreilly.com/pub/a/tim/articles/paradigmshift_0504.html" target="_blank">the internet and open source were commoditizing proprietary software</a>. He’d noticed that after the IBM personal computer design had commoditized computer hardware, Microsoft had figured out how to make software the next source of proprietary value. Tim was seeing the pattern and was starting to think that what we now call “big data” was going to be the new source of proprietary lock-in and value. I was giving <a rel="noreferrer noopener" href="http://www.asymco.com/2014/06/23/clayton-christensen-on-capturing-the-upside/" target="_blank">my talk about the conservation of attractive profits</a> the same day, and so we had a real laugh about it. He’d uncovered a new example of just what I was talking about.</p><p>But as Tim and I continued to talk about this idea over the years, we realized that the law of conservation of attractive profits applies to way more than the alternating cycle of modularity and open standards versus tight proprietary integration that we’d both originally observed. <a rel="noreferrer noopener" href="https://www.oreilly.com/tim/wtf-book.html" target="_blank">Tim likes to point out</a> that in a world where more and more has become a commodity, things become valuable again because we mix in ideas that <em>persuade</em> people to value them differently. Advertising makes a branded product bring a higher price than a generic equivalent. Cycles of fashion make the latest offerings worth more than last year’s perfectly good clothes. But that’s just the tip of the iceberg. Now everything is infused with imaginative value. People say, “This isn’t just coffee; it’s organic <a rel="noreferrer noopener" href="https://counterculturecoffee.com/blog/coffee-basics-what-is-single-origin-coffee" target="_blank">single-origin coffee</a>.” We’re increasingly paying a premium for intangibles. In 2015, <a rel="noreferrer noopener" href="https://sca.coffee/research/specialty-coffee-facts-figures" target="_blank">55% of the $48 billion US coffee market was for “specialty coffee</a>” of various kinds.</p><p><a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Dave_Hickey" target="_blank">Dave Hickey</a>, who’s been listening, <a rel="noreferrer noopener" href="https://www.amazon.com/Air-Guitar-Essays-Art-Democracy/dp/0963726455" target="_blank">pipes in</a>: That’s been going on for a long time. After World War II, “American businesses stopped advertising products for what they were, or for what they could do, and began advertising them for what they <em>meant—</em>as sign systems within the broader culture.…Rather than producing and marketing infinitely replicable objects that adequately served unchanging needs, American commerce began creating finite sets of objects that embodied ideology for a finite audience at a particular moment—objects that created desire rather than fulfilling needs. This is nothing more or less than an art market.”</p><p>He really gets on a roll then, continuing with enthusiasm: “The Leonardo of this new art market was an ex-custom-car designer from Hollywood named Harley Earl, who headed the design division at General Motors during the postwar period. Earl’s most visible and legendary contributions to American culture were the Cadillac tailfin and the pastel paint job.” It’s not just about creating objects of desire, he continues, but about creating new mechanisms for signaling status. “Most importantly,&#8230;Earl invented the four-year style-change cycle linked to the Platonic hierarchy of General Motors cars, and this revolutionary dynamic created the post-industrial world. Basically, what Earl invented was a market situation in which the consumer moved up the status-ladder within the cosmology of General Motors products—from Chevrolet to Pontiac to Buick to Oldsmobile to Cadillac—as the tailfin or some other contagious motif moved <em>down</em> the price ladder, from Cadillac to Chevrolet, year by year, as styles changed incrementally.”</p><p>Giving a nod to the guy who’d kicked off the conversation, Hickey continues: “As Warhol [is] fond of telling us, the strange thing about the sixties was not that Western art was becoming commercialized but that Western commerce was becoming so much more artistic.”</p><p>Vitalik Buterin jumps in: I wish I’d heard about your work before, Dave. I wasn’t thinking enough about art. “<a rel="noreferrer noopener" href="https://twitter.com/VitalikButerin/status/1477402803433840642?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1477402803433840642%7Ctwgr%5E%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2Findianexpress.com%2Farticle%2Ftechnology%2Fcrypto%2Fethereum-white-paper-predicted-defi-but-missed-nfts-vitalik-buterin-7704098%2F" target="_blank">I completely missed NFTs</a>.” I was focused on practical applications like DeFi, incentivized file storage, and compute, and I didn’t think a lot about how much of the economy has become an art market.</p><p>Hickey replies that he wishes everyone would think more deeply about what art teaches us about how economies and people tick. I didn’t subtitle my book <a rel="noreferrer noopener" href="https://www.amazon.com/Air-Guitar-Essays-Art-Democracy/dp/0963726455" target="_blank"><em>Air Guitar</em></a> “Essays on Art and Democracy” for shits and giggles, he says.</p><p>Hickey then starts rhapsodizing about his fascination with cars growing up “in the American boondocks” during the 1950s and ’60s. “My first glimmerings of higher [art] theory grew out of that culture: the rhetoric of image and icon, the dynamics of embodied desire, the algorithms of style change, and the ideological force of disposable income. All of these came to me in the lingua franca of cars, arose out of our perpetual exegesis of its nuanced context and iconography. And it was worth the trouble, because all of us who partook of this discourse, as artists, critics, collectors, mechanics, and citizens, understood its politico-aesthetic implications, understood that we were voting with cars….We also understood that we were <em>dissenting</em> when we customized them and hopped them up—demonstrating against the standards of the republic and advocating our own refined vision of power and loveliness.”</p><p>In the computer industry, you can see how Steve Jobs did for Apple the exact thing that Earl had done for GM. From the 1984 Macintosh ad to the “Think Different” campaign, Apple wasn’t selling hardware and software. It was selling identity and a sense of meaning. The new $40 billion market for NFTs—essentially digital collectibles whose chief value is in the bragging rights of how much you paid for them or how cool and unusual they are—takes this idea to the next level.</p><p>Buterin replies: Your point about “demonstrating against the standards of the republic and advocating our own refined vision of power and loveliness” really resonates with me, and I suspect it will with a lot of the crypto community. We aren’t just thinking about how to advance blockchain technology. We’re also thinking a lot about upending the current financial system and about deep questions like <a rel="noreferrer noopener" href="https://vitalik.ca/general/2021/03/23/legitimacy.html" target="_blank">legitimacy</a>. “An outcome in some social context is legitimate if the people in that social context broadly accept and play their part in enacting that outcome, and each individual person does so because they expect everyone else to do the same.”</p><p>“Why is it that Elon Musk can sell an NFT of Elon Musk&#8217;s tweet, but Jeff Bezos would have a much harder time doing the same? Elon and Jeff have the same level of ability to screenshot Elon&#8217;s tweet and stick it into an NFT dapp, so what&#8217;s the difference? To anyone who has even a basic intuitive understanding of human social psychology (or the <a rel="noreferrer noopener" href="https://www.austinartistsmarket.com/famous-fakes-art-history/" target="_blank">fake art scene</a>), the answer is obvious: Elon selling Elon&#8217;s tweet is the real thing, and Jeff doing the same is not. Once again, millions of dollars of value are being controlled and allocated, not by individuals or cryptographic keys, but by social conceptions of legitimacy.”</p><p>But there’s more to it than that. “Which NFTs people find attractive to buy, and which ones they do not, is [also] a question of legitimacy: if everyone agrees that one NFT is interesting and another NFT is lame, then people will strongly prefer buying the first, because it would have both higher value for bragging rights and personal pride in holding it, and because it could be resold for more because everyone else is thinking in the same way.”</p><p>“If you&#8217;re not in a <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Coordination_game" target="_blank">coordination game</a>, there&#8217;s no reason to act according to your expectation of how other people will act, and so legitimacy is not important. But as we have seen, coordination games are everywhere in society, and so legitimacy turns out to be quite important indeed. In almost any environment with coordination games that exists for long enough, there inevitably emerge some mechanisms that can choose which decision to take. These mechanisms are powered by an established culture that everyone pays attention to these mechanisms and (usually) does what they say. Each person reasons that because everyone else follows these mechanisms, if they do something different they will only create conflict and suffer, or at least be left in a lonely forked ecosystem all by themselves.”</p><p>So one way to understand what we’re working on in the crypto world is that we’re building new mechanisms for solving the problems of consensus and coordination and legitimacy. And that’s also exactly what “the market” is doing when it tries to settle the messy question of value. So when we talk about building a new financial system with crypto, we’re not talking about just rebuilding the plumbing of the existing system with fancy new pipes, we’re questioning how value is created and who gets it.</p><p>We can change the way we distribute wealth. Crypto made a lot of people rich through the betting economy, but we don’t have to spend our gains just on new bets that make the rich richer, looking for the next breakout cryptocurrency or company. We can take those gains and give them away, as I did when I <a rel="noreferrer noopener" href="https://www.vox.com/recode/2021/5/12/22433113/vitalik-buterlin-cryptocurrency-india-shiba-inu-coin-philanthropy" target="_blank">donated over a billion dollars of Ether and Shiba Inu coins to India</a> for COVID relief. But more importantly, we can build new <em>mechanisms</em> for people to coordinate around socially valuable goals.</p><p>“The concept of supporting public goods through value generated ‘out of the ether’ by publicly supported conceptions of legitimacy has value going far beyond the Ethereum ecosystem. An important and immediate challenge and opportunity is NFTs. NFTs stand a great chance of significantly helping many kinds of public goods, especially of the creative variety, at least partially solve their <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Tragedy_of_the_commons" target="_blank">chronic and systemic funding deficiencies</a>.…If the conception of legitimacy for NFTs can be pulled in a good direction, there is an opportunity to establish a solid channel of funding to artists, charities and others.”</p><p><a rel="noreferrer noopener" href="https://forkast.news/vitalik-buterin-defi-nfts-daos/" target="_blank">Buterin adds</a>: Ethereum, NFTs, and <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Decentralized_autonomous_organization" target="_blank">DAOs</a> are building blocks. “There’s a lot of different ways to connect every one of these components and most of the interesting applications end up connecting different pieces together.…I don’t see one kind of dominating use case. I just see it opening up the floodgates for a thousand different experiments.” NFTs are one experiment. DAOs are another. Who would have thought a few years ago that someone would organize a DAO to compete with billionaires to <a rel="noreferrer noopener" href="https://www.wsj.com/articles/crypto-investors-want-to-buy-rare-copy-of-u-s-constitution-at-sothebys-auction-11637071590" target="_blank">buy a rare copy of the US constitution</a> or to <a rel="noreferrer noopener" href="https://www.vice.com/en/article/93b5ve/crypto-investors-buy-40-acres-of-land-in-wyoming-to-build-blockchain-city" target="_blank">buy land in Wyoming</a>?</p><p>At this point, Blaise Aguera y Arcas, who’s been sitting over at the next table sketching out for his buddies the latest <a rel="noreferrer noopener" href="https://medium.com/@blaisea?p=6f881d6d8e75" target="_blank">progress on Google’s LaMDA large language model and its implications for our notion of personhood</a>, can’t resist leaning over and jumping into the conversation.</p><p>“We&#8217;ve been having these conversations for a long time about robots taking people&#8217;s jobs, and we&#8217;ve been thinking about it entirely in the domain of actual robots with arms and things. But the real impact is going to be that most middle class people nowadays are doing what David Graeber called <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Bullshit_Jobs" target="_blank">bullshit jobs</a>. And it&#8217;s clear that large language models can already do many of those jobs. We&#8217;re approaching the point where it feels like capitalism is maybe about to rupture, or <em>something</em> is about to rupture.”</p><p>He continues, “Graeber was questioning the legitimacy of labor in its modern form, and also the ideas of efficiency that supposedly underlie capitalism, which is actually tremendously inefficient in a variety of ways. And in particular, the thesis is that Keynes was right, in the ’20s and ’30s, in saying that, by now, due to automation, we&#8217;d all be working 15-hour workweeks. But rather than turning this into a utopia, in which we all have all these free services and don&#8217;t have to work a lot and so on, instead we&#8217;ve made a socialism for the middle class, socialism for the bourgeois, in the form of inventing all kinds of bullshit jobs.” And all the people who still have essential jobs—they still have to work, and we don’t pay a lot of them very well.</p></blockquote>



<p class="has-text-align-center"><strong>* * *</strong></p>



<p>So what will people do if they no longer have to do bullshit jobs? Maybe they’ll make up cool shit and share it with each other, eventually building a world like the one Cory Doctorow imagined in <a rel="noreferrer noopener" href="https://www.amazon.com/Down-Magic-Kingdom-Cory-Doctorow/dp/076530953X" target="_blank"><em>Down and Out in the Magic Kingdom</em></a> and <a href="https://en.wikipedia.org/wiki/Walkaway_(Doctorow_novel)"><em>Walkaway</em></a>, where measures of status are the actual currency. In the meantime, some of them might show their creativity on YouTube or TikTok and convert status to value by directing attention to products and other people. Some might create and sell NFTs. Others might peddle bullshit startups or fancy new get-rich-quick schemes. But <a rel="noreferrer noopener" href="https://mattstoller.substack.com/p/cryptocurrencies-a-necessary-scam" target="_blank">is that really new</a>? The future always has its share of hucksters along with its inventors. Sometimes the same people are both.</p>



<p>Bill Gates once said, “<a rel="noreferrer noopener" href="https://www.inc.com/damon-brown/this-perfect-bill-gates-quote-will-frame-your-next-decade-of-success.html" target="_blank">We always overestimate the change</a> that will occur in the next two years and underestimate the change that will occur in the next ten. Don&#8217;t let yourself be lulled into inaction.” That doesn’t mean to rush out and buy the latest meme stock, meme coin, or overpriced NFT. But it does mean that it’s important to engage with the social, legal, and economic implications of crypto. <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Technological_Revolutions_and_Financial_Capital" target="_blank">The world advances one bubble at a time</a>. What matters is that <a rel="noreferrer noopener" href="https://www.oreilly.com/radar/why-its-too-early-to-get-excited-about-web3/" target="_blank">what’s left behind when the bubble pops</a> makes the world richer in possibilities for the next generation to build on.</p>



<p>Looking at the arc of the modern economy, we are on a path for the market for status to become a central part of how value is measured.</p>



<p>Let’s give John Maynard Keynes the last word, even though he left the bar long before we arrived. In “<a rel="noreferrer noopener" href="http://www.econ.yale.edu/smith/econ116a/keynes1.pdf" target="_blank">Economic Possibilities for Our Grandchildren</a>,” the 1929 piece that Blaise referred to earlier, he wrote:</p>



<blockquote class="wp-block-quote"><p>For the first time since [our] creation [we] will be faced with [our] real, [our] permanent problem—how to use [our] freedom from pressing economic cares, how to occupy the leisure, which science and compound interest will have won for [us], to live wisely and agreeably and well.…</p><p>To judge from the behaviour and the achievements of the wealthy classes to-day in any quarter of the world, the outlook is very depressing! For these are, so to speak, our advance guard—those who are spying out the promised land for the rest of us and pitching their camp there. For they have most of them failed disastrously, so it seems to me—those who have an independent income but no associations or duties or ties—to solve the problem which has been set them.</p><p>I feel sure that with a little more experience we shall use the new-found bounty of nature quite differently from the way in which the rich use it to-day, and will map out for ourselves a plan of life quite otherwise than theirs.</p></blockquote>



<p>We’re now coming on to nearly 100 years since Keynes dreamed that optimistic, egalitarian dream and made his critique of the idle rich who were already living it. Abundance seems as far away as ever, or even further, and the rich haven’t changed as much as Keynes hoped.</p>



<p>It may seem deeply out of touch to talk about an economy of abundance when so many people face such great economic hardship. But that was also true for those alive in 1929. They had a worldwide depression and a great war ahead of them, and turned all their energies to dealing with both. Their success ushered in decades of widely shared prosperity. We face climate change, new pandemics, and persistent economic inequality and consequent political instability. Wars are not out of the question. Can we also rise to the challenge?</p>



<p>Through it all, the Next Economy beckons. We see its signs all around us. Keynes was right that humanity’s job in an economy of abundance is to learn to live together wisely and agreeably and well, but he was wrong to think that abundance will mean the end of competition and striving. If we do reach Keynes’s predicted future, in which more and more of what people depend on for survival has become cheap—a commodity—and our labor is not needed, how will the circulatory system of the economy sustain itself? Might the seeming froth and craziness of the crypto markets be an early implementation—not Web3 but NextEconomy1—of the next stage by which humanity engages in the ongoing imaginative competition to make things valuable again?</p>



<hr class="wp-block-separator" />



<p><em>John Maynard Keynes died in 1946, Andy Warhol in 1987, </em><a rel="noreferrer noopener" href="https://www.nytimes.com/2020/01/25/business/clayton-christensen-dead.html" target="_blank"><em>Clay Christensen in 2020</em></a><em>, and </em><a rel="noreferrer noopener" href="https://www.nytimes.com/2021/11/30/arts/dave-hickey-dead.html" target="_blank"><em>Dave Hickey just at the end of last year</em></a><em>. I wish that they could have had this conversation with Vitalik Buterin, who joins them in thinking deeply about the intersection of art, economics, business, politics, and culture. I have put my own words into their mouths; those that are in quotation marks are their own, from their books, published articles, and interviews, though the order in which paragraphs appear may be different from the original. The quotes from Blaise Aguera y Arcas are from a recording of a Zoom conversation that we had while I was writing this piece. I told him what I was working on, and his thoughts were so relevant that I couldn’t help but include them.</em></p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/andy-warhol-clay-christensen-and-vitalik-buterin-walk-into-a-bar/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>Remote Teams in ML/AI</title>
		<link>https://www.oreilly.com/radar/remote-teams-in-ml-ai/</link>
				<comments>https://www.oreilly.com/radar/remote-teams-in-ml-ai/#respond</comments>
				<pubDate>Tue, 09 Nov 2021 14:05:48 +0000</pubDate>
		<dc:creator><![CDATA[Q McCallum]]></dc:creator>
				<category><![CDATA[Building a data culture]]></category>
		<category><![CDATA[Business]]></category>
		<category><![CDATA[Commentary]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=14075</guid>
				<description><![CDATA[I&#8217;m well-versed in the ups and downs of remote work. I&#8217;ve been doing some form thereof for most of my career, and I&#8217;ve met plenty of people who have a similar story. When companies ask for my help in building their ML/AI teams, I often recommend that they consider remote hires. Sometimes I&#8217;ll even suggest [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>I&#8217;m well-versed in the ups and downs of remote work. I&#8217;ve been doing some form thereof for most of my career, and I&#8217;ve met plenty of people who have a similar story. When companies ask for my help in building their ML/AI teams, I often recommend that they consider remote hires. Sometimes I&#8217;ll even suggest that they build their data function as a fully-remote, distributed group. (I&#8217;ll oversimplify for brevity, using &#8220;remote team&#8221; and &#8220;distributed team&#8221; interchangeably. And I&#8217;ll treat both as umbrella terms that cover &#8220;remote-friendly&#8221; and &#8220;fully-distributed.&#8221;)</p>



<p>Remote hiring has plenty of benefits. As an employer, your talent pool spans the globe and you save a ton of money on office rent and insurance. For the people you hire, they get a near-zero commute and a Covid-free workplace.</p>



<p>Then again, even though you really <em>should</em> build a remote team, <em>you also shouldn&#8217;t.</em> Not just yet. You first want to think through one very important question:</p>



<p><em>Do I, as a leader, really want a remote team?</em></p>



<h3><strong>The Litmus Test</strong></h3>



<p>The key ingredient to successful remote work is, quite simply, whether company leadership wants it to work. Yes, it also requires policies, tooling, and re-thinking a lot of interactions. Not to mention, your HR team will need to double-check local laws wherever team members choose to live.&nbsp; But before any of that, the people in charge have to actually <em>want</em> a remote team.</p>



<p>Here&#8217;s a quick test for the executives and hiring managers among you:</p>



<ul><li>As the Covid-19 pandemic forced your team to work from home, did you insist on hiring only local candidates (so they could eventually work in the office)?</li><li>With wider vaccine rollouts and lower case counts, do you now require your team to spend some time in the office every week?</li><li>Do you see someone as &#8220;not really part of the team&#8221; or &#8220;less suitable for promotion&#8221; because they don&#8217;t come into the office?</li></ul>



<p>If you&#8217;ve said yes to any of these, then you simply do not want a distributed team. You want an in-office team that you begrudgingly permit to work from home now and then. And as long as you don&#8217;t truly want one, any attempts to build and support one will not succeed.</p>



<p>If you&#8217;ve said yes to any of these, then you simply do not want a distributed team. You want an in-office team that you begrudgingly permit to work from home now and then. And as long as you don&#8217;t truly want one, any attempts to build and support one will not succeed.</p>



<p>And if you <em>don&#8217;t</em> want that, that&#8217;s fine. I&#8217;m not here to change your mind.</p>



<p>But if you <em>do</em> want to build a successful remote team, and you want some ideas on how to make it work, read on.</p>



<h3><strong>How You Say What You Have to Say</strong></h3>



<p>As a leader, most of your job involves communicating with people. This will require some adjustment in a distributed team environment.</p>



<p>A lot of you have developed a leadership style that&#8217;s optimized for everyone being in the same office space during working hours. That has cultivated poor, interruption-driven communication habits. It&#8217;s too easy to stop by someone&#8217;s office, pop over a cubicle wall, or bump into someone in the hallway and share some information with them.</p>



<p>With a remote team you&#8217;ll need to write these thoughts down instead. That also means deciding what you want to do <em>before you even start writing,</em> and then sticking with it after you&#8217;ve filed the request.</p>



<p>By communicating your thoughts in clear, unambiguous language, you&#8217;ve demonstrated your commitment to what you&#8217;re asking someone to do. You&#8217;re also leaving them a document they can refer to as they perform the task you&#8217;ve requested. This is key because, depending on work schedules, a person can&#8217;t just tap you on the shoulder to ask you to clarify a point.</p>



<p>(Side note: I&#8217;ve spent my career working with extremely busy people, and being one myself. That&#8217;s taught me a lot about how to communicate in written form. Short sentences, bullet points, and starting the message with the call-to-action—sometimes referred to as <em>BLUF:</em> Bottom Line Up-Front—will go a long way in making your e-mails clearer.)</p>



<p>The same holds true for meetings: the person who called the meeting should send an agenda ahead of time and follow up with recap notes. Attendees will be able to confirm their shared understanding of what is to be done and who is doing what.</p>



<p>Does this feel like a lot of documentation? That&#8217;s great. In my experience, what feels like over-communication for an in-office scenario is usually the right amount for a distributed team.</p>



<h3><strong>Embracing Remote for What It Is</strong></h3>



<p>Grammar rules differ by language. You won&#8217;t get very far speaking the words of a new language while using grammatical constructs from your native tongue. It takes time, practice, and patience to learn the new language so that you can truly <em>express</em> yourself.&nbsp; The path takes you from &#8220;this is an unnatural and uncomfortable word order&#8221; to &#8220;German requires that I put the verb&#8217;s infinitive at the end of the clause.&nbsp; That&#8217;s just how it works.&#8221;</p>



<p>There are parallels here to leading a distributed team. It&#8217;s too easy to assume that &#8220;remote work&#8221; is just &#8220;people re-creating the in-office experience, from their kitchen tables.&#8221; It will most certainly feel unnatural and uncomfortable if you hold that perspective.&nbsp; And it <em>should</em> feel weird, since optimizing for remote work will require re-thinking a lot of the whats and hows of team interactions and success metrics.&nbsp; You start winning when you determine where a distributed team works out <em>better</em> than the in-office alternative.</p>



<p>Remote work is people getting things done from a space that is not your central office, on time schedules that aren&#8217;t strict 9-to-5, and maybe even communicating in text-based chat systems.&nbsp; Remote work is checking your messages in the morning, and seeing a stream of updates from your night-owl teammates.&nbsp; Remote work is its own thing, and trying to shoe-horn it into the shape of an in-office setup means losing out on all of the benefits.</p>



<p>Embracing remote teams will require letting go of outdated in-office tropes to accept some uncomfortable truths. People will keep working when you&#8217;re not looking over their shoulder.&nbsp; Some of them will work even better when they can do so in the peace and quiet of an environment they control.&nbsp; They can be fully present in a meeting, even if they&#8217;ve turned off their video. They can most certainly be productive on a work schedule that doesn&#8217;t match yours, while wearing casual attire.</p>



<p>The old tropes were hardly valid to begin with. And now, 18 months after diving head-first into remote work, those tropes are officially dead. It&#8217;s up to you to learn new ways to evaluate team (and team member) productivity. More importantly, in true remote work fashion, you&#8217;ll have to step back and trust the team you&#8217;ve hired.</p>



<h3><strong>Exploring New Terrain</strong></h3>



<p>If distributed teamwork is new territory for your company, expect to stumble now and then. You&#8217;re walking through a new area and instead of following your trusty old map, you&#8217;re now <em>creating</em> the map. One step at a time, one stubbed toe at a time.</p>



<p>You&#8217;ll spend time defining new best practices that are specific to this environment. This will mean thinking through a lot more decisions than before—decisions that you used to be able to handle on autopilot—and as such you will find yourself saying &#8220;I don&#8217;t know&#8221; a lot more than you used to.</p>



<p>You&#8217;ll feel some of this friction when sorting out workplace norms.&nbsp; What are &#8220;working hours,&#8221; if your team even has any?&nbsp; Maybe all you need is a weekly group check-in, after which everyone heads in separate directions to focus on their work?&nbsp; In that case, how will individuals specify <em>their</em> working hours and their off-time?&nbsp; With so much asynchronous communication, there&#8217;s bound to be confusion around when a person is expected to pick up on an ongoing conversation in a chat channel, versus their name being @-mentioned, or contacting them by DM.&nbsp; Setting those expectations will help the team shift into (the right kind of) autopilot, because they&#8217;ll know to not get frustrated when a person takes a few hours to catch up on a chat thread.&nbsp; As a bonus, going through this exercise will sort out when you really <em>need</em> to hold a group meeting versus when you have to just make an announcement (e-mail) or pose a quick question (chat).</p>



<p>Security will be another source of friction.&nbsp; When everyone is in the same physical office space, there&#8217;s little question of the &#8220;inside&#8221; versus the &#8220;outside&#8221; network.&nbsp; But when your teammates are connecting to shared resources from home or a random cafe, how do you properly wall off the office from everything else? Mandating VPN usage is a start, but it&#8217;s hardly the entire picture.&nbsp; There are also questions around company-issued devices having visibility into home-network traffic, and what they&#8217;re allowed to do with that information.&nbsp; Or even a company laptop, hacked through the company network, infecting personal devices on the home LAN. Is your company&#8217;s work so sensitive that employees will require a separate, work-only internet service for their home office?&nbsp; That would be fairly extreme—in my experience, I haven&#8217;t even seen banks go that far—but it&#8217;s not out of the realm of possibility.&nbsp; At some point a CISO may rightfully determine that this is the best path.</p>



<p>Saying &#8220;I don&#8217;t know&#8221; is OK in all of these cases, so long as you follow that with &#8220;so let&#8217;s figure it out.&#8221; Be honest with your team to explain that you, as a group, may have to try a few rounds of something before it all settles. The only two sins here are to refuse to change course when it&#8217;s not working, and to revert to the old, familiar, in-office ways just to ease your cognitive burden. So long as you are thoughtful and intentional in your approach, you&#8217;ll succeed over the long run.</p>



<h3><strong>It&#8217;s Here to Stay</strong></h3>



<p>Your data scientists (and developers, and IT ops team) have long known that remote work is possible. They communicate through Slack and collaborate using shared documents. They see that their &#8220;datacenter&#8221; is a cloud infrastructure. They already know that a lot of their day-to-day interactions don&#8217;t require everyone being in the same office. Company leadership is usually the last to pick up on this, which is why they tend to show the most resistance.</p>



<p>If adaptive leadership is the key to success with distributed teams, then <em>discipline</em> is the key to that adaptation. You&#8217;ll need the discipline to plan your communication, to disable your office autopilot, and to trust your team more.</p>



<p>You must focus on what matters—defining what needs to get done, and letting people do it—and learn to let go of what doesn&#8217;t. That will be uncomfortable, yes. But your job as a leader is to clear the path for people who are doing the implementation work. What makes <em>them</em> comfortable trumps what makes <em>you</em> comfortable.</p>



<p>Not every company will accept this. Some are willing to trade the benefits of a distributed team for what they perceive to be a superior in-office experience. And that&#8217;s fine. But for those who want it, remote is here to stay.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/remote-teams-in-ml-ai/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>The Quality of Auto-Generated Code</title>
		<link>https://www.oreilly.com/radar/the-quality-of-auto-generated-code/</link>
				<comments>https://www.oreilly.com/radar/the-quality-of-auto-generated-code/#respond</comments>
				<pubDate>Tue, 12 Oct 2021 13:45:10 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides and Kevlin Henney]]></dc:creator>
				<category><![CDATA[AI & ML]]></category>
		<category><![CDATA[Commentary]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=14007</guid>
				<description><![CDATA[Kevlin Henney and I were riffing on some ideas about GitHub Copilot, the tool for automatically generating code base on GPT-3&#8217;s language model, trained on the body of code that&#8217;s in GitHub. This article poses some questions and (perhaps) some answers, without trying to present any conclusions. First, we wondered about code quality. There are [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>Kevlin Henney and I were riffing on some ideas about <a href="https://copilot.github.com/">GitHub Copilot</a>, the tool for automatically generating code base on GPT-3&#8217;s language model, trained on the body of code that&#8217;s in GitHub. This article poses some questions and (perhaps) some answers, without trying to present any conclusions.</p>



<p>First, we wondered about code quality. There are lots of ways to solve a given programming problem; but most of us have some ideas about what makes code &#8220;good&#8221; or &#8220;bad.&#8221; Is it readable, is it well-organized? Things like that.&nbsp; In a professional setting, where software needs to be maintained and modified over long periods, readability and organization count for a lot.</p>



<p>We know how to test whether or not code is correct (at least up to a certain limit). Given enough unit tests and acceptance tests, we can imagine a system for automatically generating code that is correct. <a rel="noreferrer noopener" aria-label="Property (opens in a new tab)" href="https://increment.com/testing/in-praise-of-property-based-testing/" target="_blank">Property</a><a href="https://increment.com/testing/in-praise-of-property-based-testing/">-based testing</a> might give us some additional ideas about building test suites robust enough to verify that code works properly. But we don&#8217;t have methods to test for code that&#8217;s &#8220;good.&#8221; Imagine asking Copilot to write a function that sorts a list. There are lots of ways to sort. Some are pretty good—for example, <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Quicksort" target="_blank">quicksort</a>. Some of them are awful. But a unit test has no way of telling whether a function is implemented using quicksort, <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://kevlinhenney.medium.com/a-sort-of-permutation-768c1a7e029b" target="_blank">permutation sort</a>, (which completes in factorial time), <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://kevlinhenney.medium.com/need-something-sorted-sleep-on-it-11fdf8453914" target="_blank">sleep sort</a>, or one of the other strange sorting algorithms that Kevlin has been writing about.</p>



<p>Do we care? Well, we care about O(N log N) behavior versus O(N!). But assuming that we have some way to resolve that issue, if we can specify a program&#8217;s behavior precisely enough so that we are highly confident that Copilot will write code that&#8217;s correct and tolerably performant, do we care about its aesthetics? Do we care whether it&#8217;s readable? 40 years ago, we might have cared about the assembly language code generated by a compiler. But today, we don&#8217;t, except for a few increasingly rare corner cases that usually involve device drivers or embedded systems. If I write something in C and compile it with gcc, realistically I&#8217;m never going to look at the compiler&#8217;s output. I don&#8217;t need to understand it.</p>



<p>To get to this point, we may need a meta-language for describing what we want the program to do that&#8217;s almost as detailed as a modern high-level language. That could be what the future holds: an understanding of &#8220;prompt engineering&#8221; that lets us tell an AI system precisely what we want a program to do, rather than how to do it. Testing would become much more important, as would understanding precisely the business problem that needs to be solved. “Slinging code” in whatever the language would become less common.</p>



<p>But what if we don&#8217;t get to the point where we trust automatically generated code as much as we now trust the output of a compiler? Readability will be at a premium as long as humans need to read code. If we have to read the output from one of Copilot&#8217;s descendants to judge whether or not it will work, or if we have to debug that output because it mostly works, but fails in some cases, then we will need it to generate code that&#8217;s readable. Not that humans currently do a good job of writing readable code; but we all know how painful it is to debug code that isn’t readable, and we all have some concept of what “readability” means.</p>



<p>Second: Copilot was trained on the body of code in GitHub. At this point, it is all (or almost all) written by humans. Some of it is good, high quality, readable code; a lot of it isn&#8217;t. What if Copilot became so successful that Copilot-generated code came to constitute a significant percentage of the code on GitHub? The model will certainly need to be re-trained from time to time. So now, we have a feedback loop: Copilot trained on code that has been (at least partially) generated by Copilot. Does code quality improve? Or does it degrade? And again, do we care, and why?</p>



<p>This question can be argued either way. People working on automated tagging for AI seem to be taking the position that iterative tagging leads to better results: i.e., after a tagging pass, use a human-in-the-loop to check some of the tags, correct them where wrong, and then use this additional input in another training pass. Repeat as needed. That&#8217;s not all that different from current (non-automated) programming: write, compile, run, debug, as often as needed to get something that works. The feedback loop enables you to write good code.</p>



<p>A human-in-the-loop approach to training an AI code generator is one possible way of getting &#8220;good code&#8221; (for whatever &#8220;good&#8221; means)—though it&#8217;s only a partial solution. Issues like indentation style, meaningful variable names, and the like are only a start. Evaluating whether a body of code is structured into coherent modules, has well-designed APIs, and could easily be understood by maintainers is a more difficult problem. Humans can evaluate code with these qualities in mind, but it takes time. A human-in-the-loop might help to train AI systems to design good APIs, but at some point, the &#8220;human&#8221; part of the loop will start to dominate the rest.</p>



<p>If you look at this problem from the standpoint of evolution, you see something different. If you breed plants or animals (a highly selected form of evolution) for one desired quality, you will almost certainly see all the other qualities degrade: you&#8217;ll get <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.akc.org/expert-advice/health/hip-dysplasia-in-dogs/" target="_blank">large dogs with hips that don&#8217;t work</a>, or <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.puppyleaks.com/done-bulldogs/" target="_blank">dogs with flat faces that can&#8217;t breathe properly</a>.</p>



<p>What direction will automatically generated code take? We don&#8217;t know. Our guess is that, without ways to measure &#8220;code quality&#8221; rigorously, code quality will probably degrade. Ever since Peter Drucker, management consultants have liked to say, &#8220;If you can&#8217;t measure it, you can&#8217;t improve it.&#8221; And we suspect that applies to code generation, too: aspects of the code that can be measured will improve, aspects that can&#8217;t won&#8217;t.&nbsp; Or, as the accounting historian <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/H._Thomas_Johnson" target="_blank">H. Thomas Johnson</a> said, “Perhaps what you measure is what you get. More likely, what you measure is all you’ll get. What you don’t (or can’t) measure is lost.&#8221;</p>



<p>We can write tools to measure some superficial aspects of code quality, like obeying stylistic conventions. We already have tools that can &#8220;fix&#8221; fairly superficial quality problems like indentation. But again, that superficial approach doesn&#8217;t touch the more difficult parts of the problem. If we had an algorithm that could score readability, and restrict Copilot&#8217;s training set to code that scores in the 90th percentile, we would certainly see output that looks better than most human code. Even with such an algorithm, though, it&#8217;s still unclear whether that algorithm could determine whether variables and functions had appropriate names, let alone whether a large project was well-structured.</p>



<p>And a third time: do we care? If we have a rigorous way to express what we want a program to do, we may never need to look at the underlying C or C++. At some point, one of Copilot&#8217;s descendants may not need to generate code in a &#8220;high level language&#8221; at all: perhaps it will generate machine code for your target machine directly. And perhaps that target machine will be <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://webassembly.org/" target="_blank">Web Assembly</a>, the JVM, or something else that&#8217;s very highly portable.</p>



<p>Do we care whether tools like Copilot write good code? We will, until we don&#8217;t. Readability will be important as long as humans have a part to play in the debugging loop. The important question probably isn’t “do we care”; it’s “when will we stop caring?” When we can trust the output of a code model, we’ll see a rapid phase change.&nbsp; We’ll care less about the code, and more about describing the task (and appropriate tests for that task) correctly.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/the-quality-of-auto-generated-code/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>Two economies. Two sets of rules.</title>
		<link>https://www.oreilly.com/radar/two-economies-two-sets-of-rules/</link>
				<comments>https://www.oreilly.com/radar/two-economies-two-sets-of-rules/#respond</comments>
				<pubDate>Tue, 22 Jun 2021 13:07:19 +0000</pubDate>
		<dc:creator><![CDATA[Tim O’Reilly]]></dc:creator>
				<category><![CDATA[Bubbles]]></category>
		<category><![CDATA[Economy]]></category>
		<category><![CDATA[Stock Prices]]></category>
		<category><![CDATA[Tax and Monetary Policy]]></category>
		<category><![CDATA[Commentary]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=13820</guid>
				<description><![CDATA[At one point early this year, Elon Musk briefly became the richest person in the world. After a 750% increase in Tesla’s stock market value added over $180 billion to his fortune, he briefly had a net worth of over $200 billion. It’s now back down to “only” $155 billion. Understanding how our economy produced [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>At one point early this year, Elon Musk briefly became the richest person in the world. After a 750% increase in Tesla’s stock market value added over $180 billion to his fortune, he briefly <a href="https://www.bloomberg.com/news/articles/2021-01-06/musk-close-to-surpassing-bezos-as-world-s-richest-person">had a net worth of over $200 billion</a>. It’s now <a href="https://www.forbes.com/profile/elon-musk/?sh=c31d8f97999b">back down to “only” $155 billion</a>.</p>



<p>Understanding how our economy produced a result like this—what is good about it and what is dangerous—is crucial to any effort to address the wild inequality that threatens to tear our society apart.</p>



<h3>The betting economy versus the operating economy</h3>



<p>In response to the news of Musk’s surging fortune, Bernie Sanders <a rel="noreferrer noopener" href="https://twitter.com/SenSanders/status/1348253256330276867" target="_blank">tweeted</a>:</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/06/Picture1.png" alt="Wealth of Elon Musk on March 18, 2020: $24.5 billion
Wealth of Elon Musk on January 9, 2021: $209 billion

U.S. minimum wage in 2009: $7.25 an hour
U.S. minimum wage in 2021: $7.25 an hour

Our job: Raise the minimum wage to at least $15, tax the rich &amp; create an economy for all." class="wp-image-13821" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/06/Picture1.png 941w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/06/Picture1-300x206.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/06/Picture1-768x526.png 768w" sizes="(max-width: 941px) 100vw, 941px" /></figure>



<p>Bernie was right that a $7.25 minimum wage is an outrage to human decency. If the minimum wage had kept up with increases in productivity since 1979, <a rel="noreferrer noopener" href="https://www.commondreams.org/views/2020/01/21/if-worker-pay-had-kept-pace-productivity-gains-1968-todays-minimum-wage-would-be-24#:~:text=In%20such%20a%20world%2C%20a,year%20in%20the%20United%20States.&amp;text=If%20the%20minimum%20wage%20had,wage%20of%20%247.25%20an%20hour." target="_blank">it would be over $24</a> by now, putting a two-worker family into the middle class. But Bernie was wrong to imply that Musk’s wealth increase was at the expense of Tesla’s workers. <a rel="noreferrer noopener" href="https://www.businessinsider.com/teslas-median-employee-made-81-more-than-the-median-american-in-2018-2019-5" target="_blank">The median Tesla worker makes considerably more than the median American worker</a>.</p>



<p>Elon Musk’s wealth doesn’t come from him hoarding Tesla’s extractive profits, like a robber baron of old. For most of its existence, Tesla had no profits at all. <a rel="noreferrer noopener" href="https://www.nytimes.com/2021/01/27/business/tesla-earnings.html" target="_blank">It became profitable only last year</a>. But even in 2020, Tesla’s profits of $721 million on $31.5 billion in revenue were small—only slightly more than 2% of sales, a bit less than those of the average grocery chain, <a rel="noreferrer noopener" href="https://thegrocerystoreguy.com/what-is-the-profit-margin-for-grocery-stores/" target="_blank">the least profitable major industry segment in America</a>.</p>



<p>No, Musk won the lottery, or more precisely, the stock market beauty contest. In theory, the price of a stock reflects a company’s value as an ongoing source of profit and cash flow. In practice, it is subject to wild booms and busts that are unrelated to the underlying economics of the businesses that shares of stock are meant to represent. </p>



<p>Why is Musk so rich? The answer tells us something profound about our economy: he is wealthy because people are betting on him. But unlike a bet in a lottery or at a racetrack, in the vast betting economy of the stock market, people can cash out their winnings before the race has ended. </p>



<p>This is one of the biggest unacknowledged drivers of inequality in America, the reason why <a href="https://www.vice.com/en/article/g5pdab/the-rich-just-keep-on-getting-richer">one segment of our society prospered so much during the pandemic</a> while the other languished.</p>



<h3>What are the odds?</h3>



<p>If the stock market is like a horse race where people can cash out their bets while the race is still being run, what does it mean for the race to finish? For an entrepreneur or an early-stage investor, an IPO is a kind of finish, the point where they can sell previously illiquid shares on to others. An acquisition or a shutdown, either of which puts an end to a company’s independent existence, is another kind of ending. But it is also useful to think of the end of the race as the point in time at which the stream of company profits will have repaid the investment.</p>



<p>Since ownership of public companies is spread across tens of thousands of people and institutions, it’s easier to understand this point by imagining a small private company with one owner, say, a home construction business or a storage facility or a car wash. If it cost $1 million to buy the business, and it delivered $100,000 of profit a year, the investment would be repaid in 10 years. If it delivered $50,000 in profit, it would take 20. And of course, those future earnings would need to be discounted at some rate, since a dollar received 20 years from now is not worth as much as a dollar received today. This same approach works, in theory, for large public companies. Each share is a claim on a fractional share of the company’s future profits and the present value that people put on that profit stream.</p>



<p>This is, of course, a radical oversimplification. There are many more sophisticated ways to value companies, their assets, and their prospects for future streams of profits. But what I’ve described above is one of the oldest, the easiest to understand, and the most clarifying. It is called the price/earnings ratio, or simply the P/E ratio. It’s the ratio between the price of a single share of stock and the company’s earnings per share (its profits divided by the number of shares outstanding.) What the P/E ratio gives, in effect, is a measure of how many years of current profits it would take to pay back the investment. </p>



<p>The rate of growth also plays a role in a company’s valuation. For example, imagine a business with $100 million in revenue with a 10% profit margin, earning $10 million a year. How much it is worth to own that asset depends how fast it is growing and what stage of its lifecycle it is in when you bought it. If you were lucky enough to own that business when it had only $1 million in revenue and, say, $50,000 in profits, you would now be earning 200x as much as you were when you made your original investment. If a company grows to hundreds of billions in revenue and tens of billions in profits, as Apple, Microsoft, Facebook, and Google have done, even a small investment early on that is held for the long haul can make its lucky owner into a billionaire. Tesla <em>might</em> be one of these companies, but if so, the opportunity to buy its future is long past because it is already so highly valued. The P/E ratio helps you to understand the magnitude of the bet you are making at today’s prices.</p>



<p>The average <a href="https://www.multpl.com/s-p-500-pe-ratio/table/by-year">P/E ratio of the S&amp;P 500 has varied over time</a> as “the market” (the aggregate opinion of all investors) goes from bullish about the future to bearish, either about specific stocks or about the market as a whole. Over the past 70 years, the ratio has ranged from a low of 7.22 in 1950 to almost 45 today. (A note of warning: it was only 17 on the eve of the Great Depression.)</p>



<p>What today’s P/E ratio of 44.8 means that, on average, the 500 companies that make up the S&amp;P 500 are valued at about 45 years’ worth of present earnings. Most companies in the index are worth less, and some far more. In today’s overheated market, it is often the case that the more certain the outcome the less valuable a company is considered to be. For example, despite their enormous profits and huge cash hoards, Apple, Google, and Facebook have ratios much lower than you might expect: about 30 for Apple, 34 for Google, and 28 for Facebook. Tesla at the moment of Elon Musk’s peak wealth? <a href="https://ycharts.com/companies/TSLA/pe_ratio">1,396</a>.</p>



<p>Let that sink in. You’d have had to wait almost 1,400 years to get your money back if you’d bought Tesla stock this past January and simply relied on taking home a share of its profits. Tesla’s more recent quarterly earnings are a bit higher, and its stock price quite a bit lower, so now you’d only have to wait about 600 years.</p>



<p>Of course, it’s certainly possible that Tesla will so dominate the auto industry and related energy opportunities that its revenues could grow from its current $28 billion to hundreds of billions with a proportional increase in profits. But as Rob Arnott, Lillian Wu, and Bradford Cornell point out in their analysis “<a href="https://www.researchaffiliates.com/en_us/publications/articles/826-big-market-delusion-electric-vehicles.html">Big Market Delusion: Electric Vehicles</a>,” electric vehicle companies are already valued at roughly the same amount as the entire rest of the auto industry despite their small revenues and profits and despite the likelihood of more, rather than less, competition in future. Barring some revolution in the fundamental economics of the business, current investors are likely paying now for the equivalent of hundreds of years of future profits.</p>



<p>So why do investors do this? Simply put: because they believe that they will be able to sell their shares to someone else at an even higher price. In times where betting predominates in financial markets, what a company is actually worth by any intrinsic measure seems to have no more meaning than the actual value of tulips during the <a href="https://en.wikipedia.org/wiki/Tulip_mania">17th century Dutch “tulip mania</a>.” As the history of such moments teaches, eventually the bubble does pop.</p>



<p>This betting economy, within reason, is a good thing. Speculative investment in the future gives us new products and services, new drugs, new foods, more efficiency and productivity, and a rising standard of living. Tesla has kickstarted a new gold rush in renewable energy, and given the climate crisis, that is vitally important. A betting fever can be a useful collective fiction, like money itself (the value ascribed to pieces of paper issued by governments) or the wild enthusiasm that led to the buildout of railroads, steel mills, or the internet. As economist Carlota Perez has noted, <a href="https://en.wikipedia.org/wiki/Technological_Revolutions_and_Financial_Capital">bubbles are a natural part of the cycle by which revolutionary new technologies are adopted</a>.</p>



<p>Sometimes, though, the betting system goes off the rails. Tesla’s payback may take centuries, but it is the forerunner of a necessary industrial transformation. But what about the payback on companies such as WeWork? How about Clubhouse? Silicon Valley is awash in companies that have persuaded investors to value them at billions despite no profits, no working business model, and no pathway to profitability. Their destiny, like <a href="https://www.businessinsider.com/weworks-nightmare-ipo">WeWork’s</a> or <a href="https://www.theinformation.com/articles/how-katerras-facade-crumbled">Katerra’s</a>, is to go bankrupt.</p>



<p>John Maynard Keynes, the economist whose idea that it was essential to invest in the demand side of the economy and not just the supply side helped bring the world out of the Great Depression, wrote in his <a href="https://en.wikipedia.org/wiki/The_General_Theory_of_Employment,_Interest_and_Money"><em>General Theory of Employment</em>, <em>Interest and Money</em></a>, “Speculators may do no harm as bubbles on a steady stream of enterprise. But the position is serious when enterprise becomes the bubble on a whirlpool of speculation. When the capital development of a country becomes a by-product of the activities of a casino, the job is likely to be ill-done.”</p>



<p>In recent decades, we have seen the entire economy lurch from one whirlpool of speculation to another. And as at the gambling table, each lurch represents a tremendous transfer of wealth from the losers to the winners. The dot-com bust. The subprime mortgage meltdown. Today’s Silicon Valley “unicorn” bubble. The failures to deliver on their promises by WeWork, Katerra, and their like are just the start of yet another bubble popping.</p>



<h3>Why this matters</h3>



<p>Those at the gaming table can, for the most part, afford to lose. They are disproportionately wealthy. Nearly <a href="https://www.cnbc.com/2020/08/27/wealth-gap-grows-as-rising-corporate-profits-boost-stock-holdings-controlled-by-richest-households.html">52% of stock market value is held by the top 1% of Americans</a>, with another 35% of total market value held by the next 9%. The bottom 50% hold only 0.7% of stock market wealth. </p>



<p>Bubbles, though, are only an extreme example of a set of dynamics that shape our economy far more widely than we commonly understand. The leverage provided by the betting economy drives us inevitably toward a monoculture of big companies. The local bookstore trying to compete with Amazon, the local cab company competing with Uber, the neighborhood dry cleaner, shopkeeper, accountant, fitness studio owner, or any other local, privately held business gets exactly $1 for every dollar of profit it earns. Meanwhile, a dollar of Tesla profit turns into $600 of stock market value; a dollar of Amazon profit turns into $67 of stock market value; a dollar of Google profit turns into $34, and so on. A company and its owners <a href="https://www.businessinsider.com/how-wework-ceo-adam-neumann-spends-his-money-real-estate-2019-7">can extract massive amounts of value despite having no profits</a>—value that can be withdrawn by those who own shares—essentially getting something for nothing.</p>



<p>And that, it turns out, is also one underappreciated reason why in the modern economy, the rich get richer and the poor get poorer. <em>Rich and poor are actually living in two different economies, which operate by different rules.</em> Most ordinary people live in a world where a dollar is a dollar. Most rich people live in a world of what financial pundit Jerry Goodman, writing under the pseudonym Adam Smith, called “<a href="https://www.wiley.com/en-us/Supermoney-p-9781118040775">supermoney</a>,” where assets have been “financialized” (that is, able to participate in the betting economy) and are valued today as if they were already delivering the decades worth of future earnings that are reflected in their stock price. </p>



<p>Whether you are an hourly worker or a small business owner, you live in the dollar economy. If you’re a Wall Street investor, an executive at a public company compensated with stock grants or options, a venture capitalist, or an entrepreneur lucky enough to win, place, or show in the financial market horse race, you live in the supermoney economy. You get a huge interest-free loan from the future.</p>



<p>Elon Musk has built not one but two world-changing companies (Tesla and SpaceX.) He clearly deserves to be wealthy. As does Jeff Bezos, who quickly regained his title as the world’s wealthiest person. Bill Gates, Steve Jobs, Larry Page and Sergey Brin, Mark Zuckerberg, and many other billionaires changed our world and have been paid handsomely for it.</p>



<p>But how much is too much? When Bernie Sanders said that billionaires shouldn’t exist, <a href="https://www.businessinsider.com/facebook-ceo-mark-zuckerberg-responds-bernie-sanders-billionaires-shouldnt-exist-2019-10">Mark Zuckerberg agreed</a>, saying, &#8220;On some level, no one deserves to have that much money.&#8221; He added, &#8220;I think if you do something that&#8217;s good, you get rewarded. But I do think some of the wealth that can be accumulated is unreasonable.&#8221; Silicon Valley was founded by individuals for whom hundreds of millions provided plenty of incentive! The notion that entrepreneurs will stop innovating if they aren’t rewarded with billions is a pernicious fantasy.</p>



<h3>What to do about it</h3>



<p>Taxing the rich and redistributing the proceeds might seem like it would solve the problem. After all, during the 1950s, ’60s, and ’70s, progressive income tax rates as high as 90% did a good job of redistributing wealth and creating a broad-based middle class. But we also need to put a brake on the betting economy that is creating so much phantom wealth by essentially letting one segment of society borrow from the future while another is stuck in an increasingly impoverished present.</p>



<p>Until we recognize the systemic role that supermoney plays in our economy, we will never make much of a dent in inequality. Simply raising taxes is a bit like sending out firefighters with hoses spraying water while another team is spraying gasoline. </p>



<p>The problem is that government policy is biased in favor of supermoney. The mandate for central bankers around the world is to keep growth rates up without triggering inflation. Since the 2009 financial crisis, they have tried to do this by “<a href="https://www.investopedia.com/terms/q/quantitative-easing.asp">quantitative easing</a>,” that is, flooding the world with money created out of nothing. This has kept interest rates low, which in theory should have sparked investment in the operating economy, funding jobs, factories, and infrastructure. But <a href="https://www.ft.com/content/a2083406-ee83-11e8-89c8-d36339d835c0">far too much of it went instead to the betting economy</a>.</p>



<p>Stock markets have become so central to our imagined view of how the economy is doing that keeping stock prices going up even when companies are overvalued has become a central political talking point. Any government official whose policies cause the stock market to go down is considered to have failed. This leads to poor public policy as well as poor investment decisions by companies and individuals.</p>



<p>As Steven Pearlstein, <em>Washington Post</em> columnist and author of the book <a href="https://www.amazon.com/Moral-Capitalism-Fairness-Wont-Make/dp/1250251451"><em>Moral Capitalism</em></a>, put it in <a href="https://www.washingtonpost.com/business/2020/06/17/fed-is-addicted-propping-up-market-whether-it-needs-help-or-not/">a 2020 column</a>:</p>



<blockquote class="wp-block-quote"><p>When the markets are buoyant, Fed officials claim that central bankers should never second-guess markets by declaring that there are financial bubbles that might need to be deflated. Markets on their own, they assure, will correct whatever excesses may develop.</p><p>But when bubbles burst or markets spiral downward, the Fed suddenly comes around to the idea that markets aren’t so rational and self-correcting and that it is the Fed’s job to second-guess them by lending copiously when nobody else will.</p><p>In essence, the Fed has adopted a strategy that works like a one-way ratchet, providing a floor for stock and bond prices but never a ceiling.</p></blockquote>



<p>That’s the fire hose spraying gasoline. To turn it off, central banks should:</p>



<ul><li>Raise interest rates, modestly at first, and more aggressively over time. Yes, this would quite possibly puncture the stock market bubble, but that could well be a good thing. If people can no longer make fortunes simply by betting that stocks will go up and instead have to make more reasonable assessments of the underlying value of their investments, the market will become better at allocating capital.<br></li><li>Alternatively, accept much larger increases in inflation. As Thomas Piketty explained in <a href="https://www.amazon.com/Capital-Twenty-First-Century-Thomas-Piketty/dp/0674979850"><em>Capital in the Twenty-First Century</em></a>, inflation is one of the prime forces that decreases inequality, reducing the value of existing assets and more importantly for the poor, reducing the value of debt and the payments paid to service it.<br></li><li>Target small business creation, hiring, and profitability in the operating economy rather than phantom valuation increases for stocks.</li></ul>



<p>Tax policy also fans the fire. Taxes shape the economy in much the same way as Facebook’s algorithms shape its news feed. The debate about whether taxes as a whole should be higher or lower completely lacks nuance and so misses the point, especially in the US, where elites <a href="https://www.americanprogress.org/issues/economy/reports/2020/09/28/490816/capital-gains-tax-preference-ended-not-expanded/">use their financial and political power to get favored treatment</a>. Here are some ideas:</p>



<ul><li>Tax winnings from the betting economy at a <em>higher</em> rate than investment in the operating economy. Today’s system of capital gains taxes treats innovator Steve Jobs and hedge fund magnate Carl Icahn the same way. One of these people created enormous value. The other simply extracted it. When Jobs died in 2011 after decades of creating world-changing products and putting millions of people to work around the world, <a href="https://www.forbes.com/profile/steve-jobs/?sh=6fd781102808">his stake in Apple was worth about $2 billion</a>. In 2013, <a href="https://www.marketwatch.com/story/carl-icahns-2-billion-apple-stake-was-a-prime-example-of-investment-inequality-2016-06-07">Icahn “invested” $3.6 billion in Apple stock</a> and earned about $2 billion when he sold it in 2016. Apple didn’t need Icahn’s money—or that of any other investor. It was awash in cash. Nor did Icahn’s supposed investment help Apple to create anything of value. Icahn simply used his stake to pressure the company to do share buybacks, a technique that is used to drive up the share price and so “return cash to shareholders.” This kind of financial gamesmanship could be subject to a <a href="https://en.wikipedia.org/wiki/Pigovian_tax">Pigovian tax</a>—that is, a tax explicitly designed to discourage it. <br></li><li>As President Biden has recently proposed, <a href="https://www.whitehouse.gov/briefing-room/speeches-remarks/2021/04/28/remarks-as-prepared-for-delivery-by-president-biden-address-to-a-joint-session-of-congress/">we could tax capital gains at the same rate as we tax so-called “ordinary income”</a>—that is, income from labor. Labor income not only has a much higher graduated rate, it also bears payroll taxes for social security and unemployment insurance. This is why, as multibillionaire investor Warren Buffett pointed out, <a href="https://money.cnn.com/2013/03/04/news/economy/buffett-secretary-taxes/index.html">he pays a lower tax rate than the people working in his office</a>.<br></li><li>Provide full charitable deductions only to those who, <a href="https://www.cbsnews.com/news/mackenzie-scott-charity-donation-billions/">like MacKenzie Scott, actually give their money away</a>. Provide a much lower (or even nonexistent) deduction for putting money into an institution controlled by the donor, such as a private foundation or <a href="https://acceleratecharitablegiving.org/about/">donor-advised fund</a> that then doles out a tiny fraction each year so as to preserve another form of generational wealth.<br></li><li><a href="https://www.brookings.edu/wp-content/uploads/2020/01/SarinSummers_LO_FINAL.pdf">Fund the IRS properly</a>, and target enforcement not against the poorest but against those most likely to be using aggressive tax avoidance techniques. According to the IRS commissioner, the <a href="https://www.nytimes.com/2021/04/13/business/irs-tax-gap.html">US loses $1 trillion a year to “tax cheats</a>,” most of them the ultrawealthy. Fortunately, <a href="https://www.nytimes.com/2021/06/18/us/politics/taxes-wealthy-natasha-sarin-treasury.html">there is some movement in this direction</a>.<br></li><li>Stop the practice outlined in <a href="https://www.propublica.org/article/the-secret-irs-files-trove-of-never-before-seen-records-reveal-how-the-wealthiest-avoid-income-tax">a recent <em>ProPublica</em> report</a> by which <a href="https://www.nytimes.com/2021/06/15/podcasts/the-daily/jeff-bezos-elon-musk-billionaires-taxes.html">the ultrarich fund their lifestyles tax free</a> by borrowing against their appreciating supermoney assets rather than paying themselves any taxable income. We have a progressive tax rate for a reason, and when the ultrarich <a href="https://www.propublica.org/article/you-may-be-paying-a-higher-tax-rate-than-a-billionaire">pay a fraction of the stated rate</a> due to loopholes like this, we make a mockery of the system.</li></ul>



<p>In general, we should treat not just illegal evasion but tax loopholes the way software companies treat zero-day exploits, as something to be fixed as soon as they are recognized, not years or decades later. Even better, stop building them into the system in the first place! Most loopholes are backdoors installed knowingly by our representatives on behalf of their benefactors.</p>



<p>This last idea is perhaps the most radical. The tax system could and should become more dynamic rather than more predictable. Imagine if Facebook or Google were to tell us that they couldn’t change their algorithms to address misinformation or spam without upsetting their market and so had to leave abuses in place for decades in the interest of maintaining stability—we’d think they were shirking their duty. So too our policy makers. It’s high time we all recognize the market-shaping role of tax and monetary policy. If we can hold Facebook’s algorithms to account, why can’t we do the same for our government?</p>



<p>Our society and markets are getting the results the algorithm was designed for. Are they the results we actually want?</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/two-economies-two-sets-of-rules/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>Code as Infrastructure</title>
		<link>https://www.oreilly.com/radar/code-as-infrastructure/</link>
				<comments>https://www.oreilly.com/radar/code-as-infrastructure/#respond</comments>
				<pubDate>Tue, 08 Jun 2021 13:22:32 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides]]></dc:creator>
				<category><![CDATA[Infrastructure]]></category>
		<category><![CDATA[Commentary]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=13808</guid>
				<description><![CDATA[A few months ago, I was asked if there were any older technologies other than COBOL where we were in serious danger of running out of talent. They wanted me to talk about Fortran, but I didn&#8217;t take the bait. I don&#8217;t think there will be a critical shortage of Fortran programmers now or at [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>A few months ago, I was asked if there were any older technologies other than COBOL where we were in serious danger of running out of talent. They wanted me to talk about Fortran, but I didn&#8217;t take the bait. I don&#8217;t think there will be a critical shortage of Fortran programmers now or at any time in the future. But there&#8217;s a bigger question lurking behind Fortran and COBOL: what are the ingredients of a technology shortage? Why is running out of COBOL programmers a problem?</p>



<p>The answer, I think, is fairly simple. We always hear about the millions (if not billions) of lines of COBOL code running financial and government institutions, in many cases code that was written in the 1960s or 70s and hasn&#8217;t been touched since. That means that COBOL code is infrastructure we rely on, like roads and bridges. If a bridge collapses, or an interstate highway falls into disrepair, that&#8217;s a big problem. The same is true of the software running banks.</p>



<p>Fortran isn&#8217;t the same. Yes, the language was invented in 1957, two years earlier than COBOL. Yes, millions of lines of code have been written in it. (Probably billions, maybe even trillions.) However, Fortran and COBOL are used in fundamentally different ways. While Fortran was used to create infrastructure, software written in Fortran isn&#8217;t itself infrastructure. (There are some exceptions, but not at the scale of COBOL.) Fortran is used to solve specific problems in engineering and science. Nobody cares anymore about the Fortran code written in the 60s, 70s, and 80s to design new bridges and cars. Fortran is still heavily used in engineering—but that old code has retired. Those older tools have been reworked and replaced.  Libraries for linear algebra are still important (<a href="https://en.wikipedia.org/wiki/LAPACK">LAPACK</a>), some modeling applications are still in use (<a href="https://en.wikipedia.org/wiki/Numerical_Electromagnetics_Code">NEC4</a>, used to design antennas), and even some important libraries used primarily by other languages (the Python machine learning library scikit-learn calls both NumPy and SciPy, which in turn call LAPACK and other low level mathematical libraries written in Fortran and C). But if all the world&#8217;s Fortran programmers were to magically disappear, these libraries and applications could be rebuilt fairly quickly in modern languages—many of which already have excellent libraries for linear algebra and machine learning. The continued maintenance of Fortran libraries that are used primarily by Fortran programmers is, almost by definition, not a problem.</p>



<p>If shortages of COBOL programmers are a problem because COBOL code is infrastructure, and if we don&#8217;t expect shortages of Fortran talent to be a problem because Fortran code isn&#8217;t infrastructure, where should we expect to find future crises? What other shortages might occur?</p>



<p>When you look at the problem this way, it&#8217;s a no-brainer. For the past 15 years or so, we&#8217;ve been using the slogan &#8220;infrastructure as code.&#8221; So what&#8217;s the code that creates the infrastructure? Some of it is written in languages like Python and Perl. I don&#8217;t think that&#8217;s where shortages will appear. But what about the configuration files for the systems that manage our complex distributed applications? Those configuration files are code, too, and should be managed as such.</p>



<p>Right now, companies are moving applications to the cloud <em>en masse</em>. In addition to simple lift and shift, they&#8217;re refactoring monolithic applications into systems of microservices, frequently orchestrated by Kubernetes. Microservices in some form will probably be the dominant architectural style for the foreseeable future (where &#8220;foreseeable&#8221; means at least 3 years, but probably not 20). The microservices themselves will be written in Java, Python, C++, Rust, whatever; these languages all have a lot of life left in them.</p>



<p>But it&#8217;s a safe bet that many of these systems will still be running 20 or 30 years from now; they&#8217;re the next generation&#8217;s &#8220;legacy apps.&#8221; The infrastructure they run on will be managed by <a href="https://en.wikipedia.org/wiki/Kubernetes">Kubernetes</a>—which may well be replaced by something simpler (or just more stylish). And that&#8217;s where I see the potential for a shortage—not now, but 10 or 20 years from now. Kubernetes configuration is complex, a distinct specialty in its own right. If Kubernetes is replaced by something simpler (which I think is inevitable), who will maintain the infrastructure that already relies on it? What happens when learning Kubernetes isn&#8217;t the ticket to the next job or promotion? The YAML files that configure Kubernetes aren’t a Turing-complete programming language like Python; but they are code. The number of people who understand how to work with that code will inevitably dwindle, and may eventually become a &#8220;dying breed.&#8221; When that happens, who will maintain the infrastructure? Programming languages have lifetimes measured in decades; popular infrastructure tools don’t stick around that long.</p>



<p>It&#8217;s not my intent to prophesy disaster or gloom. Nor is it my intention to critique Kubernetes; it’s just one example of a tool that has become critical infrastructure, and if we want to understand where talent shortages might arise, I’d look at critical infrastructure. Who’s maintaining the software we can&#8217;t afford not to run? If it&#8217;s not Kubernetes, it&#8217;s likely to be something else. Who maintains the CI/CD pipelines? What happens when Jenkins, CircleCI, and their relatives have been superseded? Who maintains the source archives?&nbsp; What happens when git is a legacy technology?</p>



<p>Infrastructure as code: that&#8217;s a great way to build systems. It reflects a lot of hard lessons from the 1980s and 90s about how to build, deploy, and operate mission-critical software. But it&#8217;s also a warning: know where your infrastructure is, and ensure that you have the talent to maintain it.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/code-as-infrastructure/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
	</channel>
</rss>
