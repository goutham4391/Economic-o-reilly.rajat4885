<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"

	>

<channel>
	<title>Research &#8211; Radar</title>
	<atom:link href="https://www.oreilly.com/radar/tag/research/feed/" rel="self" type="application/rss+xml" />
	<link>https://www.oreilly.com/radar</link>
	<description>Now, next, and beyond: Tracking need-to-know trends at the intersection of business and technology</description>
	<lastBuildDate>Thu, 04 Aug 2022 01:48:35 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.3.13</generator>
	<item>
		<title>The Metaverse Is Not a Place</title>
		<link>https://www.oreilly.com/radar/the-metaverse-is-not-a-place/</link>
				<comments>https://www.oreilly.com/radar/the-metaverse-is-not-a-place/#respond</comments>
				<pubDate>Tue, 02 Aug 2022 18:38:46 +0000</pubDate>
		<dc:creator><![CDATA[Tim O’Reilly]]></dc:creator>
				<category><![CDATA[Metaverse]]></category>
		<category><![CDATA[Research]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=14641</guid>
				<description><![CDATA[The metaphors we use to describe new technology constrain how we think about it, and, like an out-of-date map, often lead us astray. So it is with the metaverse. Some people seem to think of it as a kind of real estate, complete with land grabs and the attempt to bring traffic to whatever bit [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>The metaphors we use to describe new technology constrain how we think about it, and, <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.linkedin.com/pulse/20121029141916-16553-language-is-a-map" target="_blank">like an out-of-date map</a>, often lead us astray. So it is with the metaverse. Some people seem to think of it as <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://news.bitcoin.com/metaverse-real-estate-sales-to-grow-by-5-billion-by-2026/" target="_blank">a kind of real estate</a>, complete with land grabs and the attempt to bring traffic to whatever bit of virtual property they’ve created.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/08/the_metaverse_is_not_a_place_01.png" alt="" class="wp-image-14642" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/08/the_metaverse_is_not_a_place_01.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/08/the_metaverse_is_not_a_place_01-300x186.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/08/the_metaverse_is_not_a_place_01-768x476.png 768w" sizes="(max-width: 977px) 100vw, 977px" /></figure>



<p>Seen through the lens of the real estate metaphor, the metaverse becomes a natural successor not just to <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Second_Life" target="_blank">Second Life</a> but to the World Wide Web and to social media feeds, which can be thought of as a set of places (sites) to visit. Virtual Reality headsets will make these places more immersive, we imagine.</p>



<p>But what if, instead of thinking of the metaverse as a set of interconnected virtual places, we think of it as a communications medium? Using this metaphor, we see the metaverse as a continuation of a line that passes through messaging and email to “rendezvous”-type social apps like Zoom, Google Meet, Microsoft Teams, and, for wide broadcast, Twitch + Discord. This is a progression from text to images to video, and from store-and-forward networks to real time (and, for broadcast, “stored time,” which is a useful way of thinking about recorded video), but in each case, the interactions are not place based but happening in the ether between two or more connected people. The occasion is more the point than the place.</p>



<p>In an interview with Lex Fridman, Mark Zuckerberg disclaimed the notion of the metaverse as a place, but in the same sentence <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://youtu.be/5zOHSysMmH0?t=1019" target="_blank">described its future in a very place-based way</a>:</p>



<blockquote class="wp-block-quote"><p><em>A lot of people think that the Metaverse is about a place, but one definition of this is it&#8217;s about a time when basically immersive digital worlds become the primary way that we live our lives and spend our time.</em></p></blockquote>



<p>Think how much more plausible this statement might be if it read:</p>



<blockquote class="wp-block-quote"><p><em>A lot of people think that the Metaverse is about a place, but one definition of this is it&#8217;s about a time when immersive digital worlds become the primary way that we communicate and share digital experiences.</em></p></blockquote>



<p>My personal metaverse prototype moment does not involve VR at all, but Zoom. My wife Jen and I join our friend Sabrina over Zoom each weekday morning to exercise together. Sabrina leads the sessions by sharing her Peloton app, which includes live and recorded exercise videos. Our favorites are the strength training videos with <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.onepeloton.com/instructors/peloton_r" target="_blank">Rad Lopez</a> and the 15-minute abs videos with <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.onepeloton.com/instructors/bike/robin" target="_blank">Robin Arzón</a>. We usually start with Rad and end with Robin, for a vigorous 45-minute workout.</p>



<p>Think about this for a moment: Jen and I are in our home. Sabrina is in hers. Rad and Robin recorded their video tracks from their studios on the other side of the county. Jen and Sabrina and I are there in real time. Rad and Robin are there in stored time. We have joined five people in four different places and three different times into one connected moment and one connected place, “the place between” the participants.</p>



<p>Sabrina also works out on her own on her Peloton bike, and that too has this shared quality, with multiple participants at various “thicknesses” of connection. While Jen and Sabrina and I are “enhancing” the sharing using real-time Zoom video, Sabrina’s “solo” bike workouts use the intrinsic sharing in the Peloton app, which lets participants see real-time stats from others doing the same ride.</p>



<p>This is the true internet—the network of networks, with dynamic interconnections. If the metaverse is to inherit that mantle, it has to have that same quality. <em>Connection</em>.</p>



<p>Hacker News user <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://news.ycombinator.com/item?id=29083271" target="_blank">kibwen put it beautifully</a> when they wrote:</p>



<blockquote class="wp-block-quote"><p><em>A metaverse involves some kind of shared space and shared experience across a networked medium. Not only is it more than just doing things in VR, a metaverse doesn&#8217;t even require VR.</em></p></blockquote>



<h3>The metaverse as a vector</h3>



<p>It’s useful to look at technology trends (lines of technology progression toward the future, and inheritance from the past) as vectors—quantities that can only be fully described by both a magnitude and a direction and that can be summed or multiplied to get a sense of how they might cancel, amplify, or redirect possible pathways to the future.</p>



<p>I wrote about this idea back in 2020, in a piece called “<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.oreilly.com/tim/21stcentury/" target="_blank">Welcome to the 21st Century</a>,” in the context of using <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.wired.com/1995/11/how-to-build-scenarios/" target="_blank">scenario planning</a> to imagine the post-COVID future. It’s worth recapping here:</p>



<blockquote class="wp-block-quote"><p><em>Once you’ve let loose your imagination, observe the world around you and watch for what scenario planners sometimes call “news from the future”—data points that tell you that the world is trending in the direction of one or another of your imagined scenarios. As with any scatter plot, data points are all over the map, but when you gather enough of them, you can start to see the trend line emerge.…</em><br><br><em>If you think of trends as vectors, new data points can be seen as extending and thickening the trend lines and showing whether they are accelerating or decelerating. And as you see how trend lines affect each other, or that new ones need to be added, you can continually update your scenarios (or as those familiar with Bayesian statistics might put it, you can </em><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Prior_probability" target="_blank"><em>revise your priors</em></a><em>). This can be a relatively unconscious process. Once you’ve built mental models of the world as it might be, the news that you read will slot into place and either reinforce or dismantle your imagined future.</em></p></blockquote>



<p>Here’s how my thinking about the metaverse was formed by “news from the future” accreting around a technology-development vector:</p>



<ol><li>I had a prior belief, <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://web.archive.org/web/20000711041545/http://www.oreillynet.com/pub/a/network/2000/06/09/java_keynote.html" target="_blank">going back decades</a>, that the internet is a tool for connection and communication, and that advances along that vector will be important. I’m always looking with soft focus for evidence that the tools for connection and communication are getting richer, trying to understand <em>how</em> they are getting richer and how they are changing society.&nbsp;</li><li>I’ve been looking at VR for years, trying various headsets and experiences, but they are mostly solo and feel more like stand-alone games or if shared, awkward and cartoonish. Then I read a thoughtful piece by my friend Craig Mod in which he noted that while he lives his physical life in a small town in Japan or <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://craigmod.com/essays/walk_japan/" target="_blank">walking its ancient footpaths</a>, he also has a work life in which he spends time daily with people all over the world. I believe he made the explicit connection to the metaverse, but neither he nor I can find the piece that planted this thought to confirm that. In any case, I think of Craig’s newsletter as where the notion that the metaverse is a continuation of the communications technologies of the internet took hold for me.</li><li>I began to see the connection to Zoom when friends started using interesting backgrounds, some of which make them appear other than where they are and others that make clear just where they are. (For example, my friend Hermann uses as a background the beach behind his home in New Zealand, which is more vividly place based than his home office, which could be anywhere.) That then brought my exercise sessions with Sabrina and Jen into focus as part of this evolving story.</li><li>I talked to <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.sequoiacap.com/article/phil-libin-mmhmm-spotlight/" target="_blank">Phil Libin</a> about his brilliant service <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.mmhmm.app/" target="_blank">mmhmm</a>, which makes it easy to create and deliver richer, more interactive presentations over Zoom and similar apps. The speaker literally gets to occupy the space of the presentation. Phil’s presentation on “<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.youtube.com/watch?v=mBKIMhGO8WA" target="_blank">The Out of Office World</a>” was where it all clicked. He talks about the hierarchy of communication and the tools for modulating it. (IMO this is a must-watch piece for anyone thinking about the future of internet apps. I’m surprised how few people seem to have watched it.)<br></li></ol>



<figure class="wp-block-embed-youtube wp-block-embed is-type-video is-provider-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe title="The Out-of-office World" width="500" height="281" src="https://www.youtube.com/embed/mBKIMhGO8WA?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div></figure>



<ol start="5"><li>Trying <a href="https://www.getsupernatural.com/">Supernatural</a> using the Meta Quest 2 headset completed the connection between my experience using Zoom and Peloton for fitness with friends and the VR-dominant framing of the metaverse. Here I was, standing on the edge of one of the lava lakes at <a href="https://www.atlasobscura.com/places/erta-ale">Erta Ale</a> in Ethiopia, an astonishing volcano right out of central casting for Mount Doom in <em>The Lord of the Rings</em>, working through warm-up exercises with a video of a fitness instructor green-screened into the scene, before launching into a boxing training game. Coach Susie was present in stored time, just like Robin and Rad. All that was missing was Jen and Sabrina. I’m sure that such shared experiences in remarkable places are very much part of the VR future.</li></ol>



<p></p>



<figure class="wp-block-image size-large"><a href="https://www.youtube.com/watch?v=jufCUdibP4c" target="_blank" rel="noreferrer noopener"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/08/the_metaverse_is_not_a_place_02.png" alt="" class="wp-image-14643" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/08/the_metaverse_is_not_a_place_02.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/08/the_metaverse_is_not_a_place_02-300x169.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/08/the_metaverse_is_not_a_place_02-768x432.png 768w" sizes="(max-width: 977px) 100vw, 977px" /></a></figure>



<p>That kind of shared experience is central to Mark Zuckerberg’s vision of <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.youtube.com/watch?v=b9vWShsmE20" target="_blank">socializing in the metaverse</a>.</p>



<figure class="wp-block-embed-youtube wp-block-embed is-type-video is-provider-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe title="Watch Mark Zuckerberg&#039;s vision for socializing in the Metaverse" width="500" height="281" src="https://www.youtube.com/embed/b9vWShsmE20?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div></figure>



<p>In that video, Zuck shows off lavishly decorated personal spaces, photorealistic and cartoon avatars, and an online meeting interrupted by a live video call. He says:</p>



<blockquote class="wp-block-quote"><p><em>It&#8217;s a ways off but you can start to see some of the fundamental building blocks take shape. First the feeling of presence. This is the defining quality of the metaverse. You&#8217;re going to really feel like you&#8217;re there with other people. You&#8217;ll see their facial expressions, you&#8217;ll see their body language, maybe figure out if they&#8217;re actually holding a winning hand—all the subtle ways that we communicate that today&#8217;s technology can&#8217;t quite deliver.</em></p></blockquote>



<p>I totally buy the idea that presence is central. But Meta’s vision seems to miss the mark in its focus on avatars. Embedded video delivers more of that feeling of presence with far less effort on the part of the user than learning to create avatars that mimic our gestures and expressions.</p>



<p>Chris Milk, the CEO of Within, the company that created Supernatural, both agreed and disagreed about avatars when explaining the company’s origin story to me in a phone conversation a few months ago:</p>



<blockquote class="wp-block-quote"><p><em>What we learned early on was that photorealism matters a lot in terms of establishing presence and human connection. Humans, captured using photorealistic methods like immersive video, allow for a deeper connection between the audience and the people recorded in the immersive VR experience. The audience feels present in the story with them. But it&#8217;s super hard to do from a technical standpoint and you give up a bunch of other things. The trade-off is that you can have photorealism but sacrifice interactivity, as the photorealistic humans need to be prerecorded. Alternatively, you can have lots of interactivity and human-to-human communication, but you give up on anyone looking real. In the latter, the humans need to be real-time-rendered avatars, and those, for the moment, don’t look remotely like real humans.</em></p></blockquote>



<p>At the same time, Milk pointed out that humans are able to read a lot into even crude avatars, especially when they’re accompanied by real-time communication using voice.</p>



<blockquote class="wp-block-quote"><p><em>Especially if it’s someone you already know, then the human connection can overcome a lot of missing visual realism. We did an experiment back in 2014 or 2015, probably. Aaron [Koblin, the cofounder of Within] was living in San Francisco, and I was in Los Angeles. We had built a VR prototype where we each had a block for the head and two blocks for our hands. I got into my headset in LA, and Aaron&#8217;s blocks were sitting over on the floor across from me as his headset and hand controllers were sitting on his floor in San Francisco. All of a sudden the three blocks jumped up off the ground into the air as he picked up his headset and put it on. The levitating cubes “walked” up to me, waved, and said, “Hey.” Immediately, before I even heard the voice, I recognized the person in those blocks as Aaron. I recognized through the posture and gait the spirit of Aaron in these three cubes moving through space. The resolution, or any shred of photorealism, was completely absent, but the humanity still showed through. And when his voice came out of them, my brain just totally accepted that the soul of Aaron now resides in these three floating cubes. Nothing was awkward about communicating back and forth. My brain just accepted it instantly. </em></p></blockquote>



<p>And that’s where we get back to vectors. Understanding the future of photorealism in the metaverse depends on the speed and direction of progress in AI. In many ways, a photorealistic avatar is a kind of <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Deepfake" target="_blank">deepfake</a>, and we know how computationally expensive their creation is today. How long will it be before the creation of deepfakes is cheap enough and fast enough that hundreds of millions of people can be creating and using them in real time? I suspect it will be a while.</p>



<p>Mmhmm’s blending of video and virtual works really well, using today’s technology. It’s ironic that in Meta’s video about the future, video is only shown on a screen in the virtual space rather than as an integral part of it. Meta could learn a lot from mmhmm.</p>



<p>On the other hand, creating a vast library of immersive 3D still images of amazing places into which either avatars or green-screened video images can be inserted seems much closer to realization. It’s still hard, but the problem is orders of magnitude smaller. The virtual spaces offered by Supernatural and other VR developers give an amazing taste of what’s possible here.</p>



<p>In this regard, an interesting sidenote came from a virtual session that we held earlier this year at the Social Science Foo Camp (an event put together annually by O’Reilly, Meta, and Sage) using the <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://engagevr.io/" target="_blank">ENGAGE</a> virtual media conferencing app. The group began their discussion in one of the default meeting spaces, but one of the attendees, Adam Flaherty, proposed that they have it in a more appropriate place. They moved to a beautifully rendered version of <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Bodleian_Library" target="_blank">Oxford’s Bodleian Library</a>, and attendees reported that the entire tenor of the conversation changed.</p>



<p>Two other areas worth thinking about:</p>



<ol><li>Social media evolved from a platform for real-time interaction (real-time status updates, forums, conversations, and groups) to one that’s often dominated by stored-time interaction (posts, stories, reels, et al). Innovation in formats for stored-time communications is at the heart of future social media competition, as TikTok has so forcefully reminded Facebook. There’s a real opportunity for developers and influencers to pioneer new formats as the metaverse unfolds.</li><li>Bots are likely to play a big role in the metaverse, just as they do in today’s gaming environments. Will we be able to distinguish bots from humans? Chris Hecker’s indie game <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="http://www.spyparty.com/" target="_blank"><em>SpyParty</em></a>, prototyped in 2009, made this a central feature of <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/SpyParty" target="_blank">its game play</a>, requiring two human players (one spy and one sniper) to find or evade each other among a party crowded with bots (what game developers call non-player characters or NPCs). Bots and deepfakes are already transforming our social experiences on the internet; expect this to happen on steroids in the metaverse. Some bots will be helpful, but others will be malevolent and disruptive. We will need to tell the difference.</li></ol>



<h3>The need for interoperability</h3>



<p>There’s one thing that a focus on communications as the heart of the metaverse story reminds us: communication, above all, depends on interoperability. A balkanized metaverse in which a few big providers engage in a winner-takes-all competition to create the Meta- or Apple- or whatever-owned metaverse will take far longer to develop than one that allows developers to create great environments and experiences and connect them bit by bit with the innovations of others. It would be far better if the metaverse were an extension of the internet (“the network of networks”) rather than an attempt to replace it with a walled garden.</p>



<p>Some things that it would be great to have be interoperable:</p>



<ul><li><strong>Identity.</strong> We should be able to use the digital assets that represent who we are across platforms, apps, and places offered by different companies.</li><li><strong>Sensors.</strong> Smartwatches, rings, and so forth are increasingly being used to collect physiological signals. This technology can be built into VR-specific headsets, but we would do better if it were easily shared between devices from different providers.</li><li><strong>Places.</strong> (Yes, places are part of this after all.) Rather than having a single provider (say Meta) become the ur-repository of photorealistic 360-degree immersive spaces, it would be great to have an interoperability layer that allows their reuse.</li><li><strong>Bot identification.</strong> Might NFTs end up becoming the basis for a nonrepudiable form of identity that must be produced by both humans and bots? (I suspect we can only force bots to identify themselves as such if we also require humans to do so.)</li></ul>



<h3>Foundations of the metaverse</h3>



<p>You can continue this exercise by thinking about the metaverse as the combination of multiple technology trend vectors progressing at different speeds and coming from different directions, and pushing the overall vector forward (or backward) accordingly. No new technology is the product of a single vector.</p>



<p>So rather than settling on just “the metaverse is a communications medium,” think about the various technology vectors besides real-time communications that are coming together in the current moment. What news from the future might we be looking for?</p>



<ul><li><strong>Virtual Reality/Augmented Reality. </strong>Lighter and less obtrusive headsets. Advances in 3D video recording. Advances in sensors, including eye-tracking, expression recognition, physiological monitoring, even <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.cnbc.com/2019/09/23/facebook-announces-acquisition-of-brain-computing-start-up-ctrl-labs.html" target="_blank">brain-control interfaces</a>. Entrepreneurial innovations in the balance between AR and VR. (Why do we think of them as mutually exclusive rather than on a continuum?)</li><li><strong>Social media. </strong>Innovations in connections between influencers and fans. How does stored time become more real time?</li><li><strong>Gaming. </strong>Richer integration between games and communications. What’s the next Twitch + Discord?</li><li><strong>AI. </strong>Not just deepfakes but the proliferation of AIs and bots as participants in social media and other communications. NPCs becoming a routine part of our online experience outside of gaming. Standards for identification of bots versus humans in online communities.</li><li><strong>Cryptocurrencies and “Web3.”</strong>&nbsp;Does crypto/Web3 provide new business models for the metaverse? (BTW, I enjoyed the way that Neal Stephenson, in <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Reamde" target="_blank"><em>Reamde</em></a>, had his character design the business model and money flows for his online game before he designed anything else. Many startups just try to get users and assume the business model will follow, but that has led us down the dead end of advertising and <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Surveillance_capitalism" target="_blank">surveillance capitalism</a>.) </li><li><strong>Identity. </strong>Most of today’s identity systems are centralized in one way or another, with identity supplied by a trusted provider or verifier. Web3 proponents, however, are exploring a variety of systems for decentralized “<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Self-sovereign_identity" target="_blank">self-sovereign identity</a>,” including Vitalik Buterin’s “<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://vitalik.eth.limo/general/2022/01/26/soulbound.html" target="_blank">soulbound tokens</a>.” The vulnerability of crypto systems to <a href="https://en.wikipedia.org/wiki/Sybil_attack">Sybil attacks</a> in the absence of verifiable identity is driving a lot of innovation in the identity space. <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://blog.mollywhite.net/is-acceptably-non-dystopian-self-sovereign-identity-even-possible/" target="_blank">Molly White’s skeptical survey of these various initiatives</a> is a great overview of the problem and the difficulties in overcoming it. Gordon Brander’s “<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://subconscious.substack.com/p/soulbinding-like-a-state" target="_blank">Soulbinding Like A State</a>,” a riff on Molly White’s post and James C. Scott’s <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Seeing_Like_a_State" target="_blank"><em>Seeing Like A State</em></a>, provides a further warning: “Scott’s framework reveals…that the dangers of legibility are not related to the sovereignty of an ID. There are many reasons self-sovereignty is valuable, but the function of a self-sovereign identity is still to make the bearer legible. What’s measured gets managed. What’s legible gets controlled.” As is often the case, no perfect solution will be found, but society will adopt an imperfect solution by making trade-offs that are odious to some, very profitable to others, and that the great mass of users will passively accept.</li></ul>



<p>There’s a lot more we ought to be watching. I’d love your thoughts in the comments.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/the-metaverse-is-not-a-place/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>2022 Cloud Salary Survey</title>
		<link>https://www.oreilly.com/radar/2022-cloud-salary-survey/</link>
				<comments>https://www.oreilly.com/radar/2022-cloud-salary-survey/#respond</comments>
				<pubDate>Wed, 22 Jun 2022 11:21:41 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides]]></dc:creator>
				<category><![CDATA[Cloud]]></category>
		<category><![CDATA[Radar Column]]></category>
		<category><![CDATA[Research]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=14547</guid>
				<description><![CDATA[Last year, our&#160;report on cloud adoption&#160;concluded that adoption was proceeding rapidly; almost all organizations are using cloud services. Those findings confirmed the results we got in&#160;2020: everything was “up and to the right.” That’s probably still true—but saying “everything is still up and to the right” would be neither interesting nor informative. So rather than [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>Last year, our&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://learning.oreilly.com/library/view/the-cloud-in/9781492096733" target="_blank">report on cloud adoption</a>&nbsp;concluded that adoption was proceeding rapidly; almost all organizations are using cloud services. Those findings confirmed the results we got in&nbsp;<a href="https://learning.oreilly.com/library/view/cloud-adoption-in/9781492088042">2020</a>: everything was “up and to the right.” That’s probably still true—but saying “everything is still up and to the right” would be neither interesting nor informative. So rather than confirming the same results for a third year, we decided to do something different.</p>



<p>This year’s survey asked questions about compensation for “cloud professionals”: the software developers, operations staff, and others who build cloud-based applications, manage a cloud platform, and use cloud services. We limited the survey to residents of the United States because salaries from different countries aren’t directly comparable; in addition to fluctuating exchange rates, there are different norms for appropriate compensation. This survey ran from April 4 through April 15, 2022, and was publicized via email to recipients of our&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.oreilly.com/emails/newsletters" target="_blank"><em>Infrastructure &amp; Ops Newsletter</em></a>&nbsp;whom we could identify as residing in the United States or whose location was unknown.</p>



<div class="wp-block-group has-very-light-gray-background-color has-background"><div class="wp-block-group__inner-container">
<h4>Executive Summary</h4>



<ul><li>Survey respondents earn an average salary of $182,000.</li><li>The average salary increase over the past year was 4.3%.</li><li>20% of respondents reported changing employers in the past year.</li><li>25% of respondents are planning to change employers because of compensation.</li><li>The average salary for women is 7% lower than the average salary for men.</li><li>63% of respondents work remotely all the time; 94% work remotely at least one day a week.</li><li>Respondents who participated in 40 or more hours of training in the past year received higher salary increases.</li></ul>
</div></div>



<p>Of the 1,408 responses we initially received, 468 were disqualified. Respondents were disqualified (and the survey terminated) if the respondent said they weren’t a US resident or if they were under 18 years old; respondents were also disqualified if they said they weren’t involved with their organization’s use of cloud services. Another 162 respondents filled out part of the survey but didn’t complete it; we chose to include only complete responses. That left us with 778 responses. Participants came from 43 states plus Washington, DC. As with our other surveys, the respondents were a relatively senior group: the average age was 47 years old, and while the largest number identified themselves as programmers (43%), 14% identified as executives and 33% as architects.</p>



<h2>The Big Picture</h2>



<p>Cloud professionals are well paid. That’s not a surprise in itself. We expected salaries (including bonuses) to be high, and they were. The cloud professionals who responded to our survey earn an average salary of $182,000; the most common salary range among respondents was $150,000 to $175,000 per year (16% of the total), as shown in&nbsp;Figure 1. The peak was fairly broad: 68% of the respondents earn between $100,000 and $225,000 per year. And there was a significant “long tail” in the compensation stratosphere: 7% of the respondents earn over $300,000 per year, and 2.4% over $400,000 per year.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0101-2-1048x762.png" alt="" class="wp-image-14550" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0101-2-1048x762.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0101-2-300x218.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0101-2-768x558.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0101-2.png 1252w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 1. <em>Annual salary by percentage of respondents</em></figcaption></figure>



<p>We believe that job changes are part of what’s driving high salaries. After all, we’ve heard about talent shortages in almost every field, with many employers offering very high salaries to attract the staff they need. By staying with their current employer, an employee may get an annual salary increase of 4%. But if they change jobs, they might get a significantly higher offer—20% or more—plus a signing bonus.</p>



<p>20% of the respondents reported that they changed employers in the past year. That number isn’t high in and of itself, but it looks a lot higher when you add it to the 25% who are planning to leave jobs over compensation. (Another 20% of the respondents declined to answer this question.) It’s also indicative that 19% of the respondents received promotions. There was some overlap between those who received promotions and those who changed jobs (5% of the total said “yes” to both questions, or roughly one quarter of those who changed jobs). When you look at the number of respondents who left their employer, are planning to leave their employer, or got a promotion and a salary increase, it’s easy to see why salary budgets are under pressure. Right now, qualified candidates have the power in the job market, though with the stock market correction that began in March 2022 and significant layoffs from some large technology-sector companies, that may be changing.</p>



<p>These conclusions are borne out when you look at the salaries of those who were promoted, changed jobs, or intend to change jobs. A promotion roughly doubled respondents’ year-over-year salary increase. On the average, those who were promoted received a 7% raise; those who weren’t promoted received a 3.7% increase. The result was almost exactly the same for those who changed jobs: those who changed averaged a 6.8% salary increase, while those who remained averaged 3.7%. We also see a difference in the salaries of those who intend to leave because of compensation: their average salary is $171,000, as opposed to $188,000 for those who didn’t plan to leave. That’s a $17,000 difference, or roughly 10%.</p>



<h2>Salaries by Gender</h2>



<p>One goal of this survey was to determine whether women are being paid fairly. Last year’s&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://learning.oreilly.com/library/view/2021-data-ai-salary/9781098118662" target="_blank">salary survey for data and AI</a>&nbsp;found a substantial difference between men’s and women’s salaries: women were paid 16% less than men. Would we see the same here?</p>



<p>The quick answer is “yes,” but the difference was smaller. Average salaries for women are 7% lower than for men ($172,000 as opposed to $185,000). But let’s take a step back before looking at salaries in more detail. We asked our respondents what pronouns they use. Only 8.5% said “she,” while 79% chose “he.” That’s still only 87% of the total. Where are the rest? 12% preferred not to say; this is a larger group than those who used “she.” 0.5% chose &#8220;other,&#8221; and 0.7% chose &#8220;they.&#8221; (That&#8217;s only four and six respondents, respectively.) Compared to results from our survey on the data/AI industry, the percentage of cloud professionals who self-identified as women appears to be much smaller (8.5%, as opposed to 14%). But there’s an important difference between the surveys: “I prefer not to answer” wasn’t an option for the Data/AI Salary Survey. We can’t do much with those responses. When we eyeballed the data for the “prefer not to say” group, we saw somewhat higher salaries than for women, but still significantly less (5% lower) than for men.</p>



<p>The difference between men’s and women’s salaries is smaller than we expected, given the results of last year’s Data/AI Salary Survey. But it’s still a real difference, and it begs the question: Is compensation improving for women? Talent shortages are driving compensation up in many segments of the software industry. Furthermore, the average reported salaries for both men and women in our survey are high. Again, is that a consequence of the talent shortage? Or is it an artifact of our sample, which appears to be somewhat older, and rich in executives? We can’t tell from a single year’s data, and the year-over-year comparison we made above is based on a different industry segment. But the evidence suggests that the salary gap is closing, and progress is being made. And that is indeed a good thing.</p>



<p>Salaries for respondents who answered “other” to the question about the pronouns they use are 31% lower than salaries for respondents who chose “he.” Likewise, salaries for respondents who chose “they” are 28% lower than men’s average salaries. However, both of these groups are extremely small, and in both groups, one or two individuals pulled the averages down. We could make the average salaries higher by calling these individuals “outliers” and removing their data; after all, outliers can have outsized effects on small groups. That’s a step we won’t take. Whatever the reason, the outliers are there; they’re part of the data. Professionals all across the spectrum have low-paying jobs—sometimes by choice, sometimes out of necessity. Why does there appear to be a concentration of them among people who don’t use “he” or “she” as their pronouns? The effect probably isn’t quite as strong as our data indicates, but we won’t try to explain our data away. It’s certainly indicative that the groups that use “they” or another pronoun than &#8220;he&#8221; or &#8220;she&#8221; showed a salary penalty. We have to conclude that respondents who use nonbinary pronouns earn lower salaries, but without more data, we don’t know why, nor do we know how much lower their salaries are or whether this difference would disappear with a larger sample.</p>



<p>To see more about the differences between men&#8217;s and women&#8217;s salaries, we looked at the men and women in each salary range. The overall shapes of the salary distributions are clear: a larger percentage of women earn salaries between $0 and $175,000, and (with two exceptions) a larger percentage of men earn salaries over $175,000. However, a slightly larger percentage of women earn supersize salaries ($400,000 or more), and a significantly larger percentage earn salaries between $225,000 and $250,000 (Figure 2).</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0102-1-807x1048.png" alt="" class="wp-image-14551" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0102-1-807x1048.png 807w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0102-1-231x300.png 231w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0102-1-768x998.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0102-1-1182x1536.png 1182w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0102-1.png 1247w" sizes="(max-width: 807px) 100vw, 807px" /><figcaption>Figure 2. <em>Men’s and women’s salaries by percentage of respondents</em></figcaption></figure>



<p>We can get some additional information by looking at salary increases (Figure 3). On average, women’s salary increases were higher than men’s: $9,100 versus $8,100. That doesn’t look like a big difference, but it’s over 10%. We can read that as a sign that women’s salaries are certainly catching up. But the signals are mixed. Men’s salaries increased more than women’s in almost every segment, with two big exceptions: 12% of women received salary increases over $30,000, while only 8% of men did the same. Likewise, 17% of women received increases between $10,000 and $15,000, but only 9% of men did. These differences might well disappear with more data.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0103-1-1048x874.png" alt="" class="wp-image-14553" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0103-1-1048x874.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0103-1-300x250.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0103-1-768x640.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0103-1.png 1258w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 3. <em>Salary increases for women and men by percentage of respondents</em></figcaption></figure>



<p>When we look at salary increases as a percentage of salary, we again see mixed results (Figure 4). Women’s salary increases were much larger than men’s in three bands: over $325,000 (with the exception of $375,000–$400,000, where there were no women respondents), $275,000–$300,000, and $150,000–$175,000. For those with very large salaries, women’s salary increases were much higher than men’s. Furthermore, the $150,000–$175,000 band had the largest number of women. While there was a lot of variability, salary increases are clearly an important factor driving women&#8217;s salaries toward parity with men’s.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0104-1-754x1048.png" alt="" class="wp-image-14554" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0104-1-754x1048.png 754w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0104-1-216x300.png 216w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0104-1-768x1067.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0104-1-1105x1536.png 1105w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0104-1.png 1236w" sizes="(max-width: 754px) 100vw, 754px" /><figcaption>Figure 4. <em>Salary increases as a percentage of salary</em></figcaption></figure>



<h3>The Effect of Education</h3>



<p>The difference between men’s and women’s salaries is significant at almost every educational level (Figure 5). The difference is particularly high for respondents who are self-taught, where women earned 39% less ($112,000 versus $184,000), and for students (45% less, $87,000 versus $158,000). However, those were relatively small groups, with only two women in each group. It’s more important that for respondents with bachelor’s degrees, women’s salaries were 4% higher than men’s ($184,000 versus $176,000)—and this was the largest group in our survey. For respondents with advanced degrees, women with doctorates averaged a 15% lower salary than men with equivalent education; women with master’s degrees averaged 10% lower. The difference between women’s and men’s salaries appears to be greatest at the extremes of the educational spectrum.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0105-1-1048x756.png" alt="" class="wp-image-14555" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0105-1-1048x756.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0105-1-300x216.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0105-1-768x554.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0105-1.png 1260w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 5. <em>Men’s and women’s salaries by degree</em></figcaption></figure>



<h2>Salaries by State</h2>



<p>Participants in the survey come from 43 states plus Washington, DC. Looking at salaries by state creates some interesting puzzles. The highest salaries are found in Oklahoma; South Dakota is third, following California. And the top of the list is an interesting mix of states where we expected high salaries (like New York) and states where we expected salaries to be lower. So what’s happening?</p>



<p>The average salary from Oklahoma is $225,000—but that only reflects two respondents, both of whom work remotely 100% of the time. (We’ll discuss remote work later in this report.) Do they work for a Silicon Valley company and get a Silicon Valley salary? We don’t know, but that’s certainly a possibility. The average salary for South Dakota is $212,000, but we shouldn’t call it an “average,” because we only had one response, and this respondent reported working remotely 1–4 days per week. Likewise, Vermont had a single respondent, who works remotely and who also had an above-average salary. Many other states have high average salaries but a very small number of respondents.</p>



<p>So the first conclusion that we can draw is that remote work might be making it possible for people in states without big technology industries to get high salaries. Or it could be the opposite: there’s no state without some businesses using the cloud, and the possibility of remote work puts employers in those states in direct competition with Silicon Valley salaries: they need to pay much higher salaries to get the expertise they need. And those job offers may include the opportunity to work remotely full or part time—even if the employer is local. Both of those possibilities no doubt hold true for individuals, if not for geographical regions as a whole.</p>



<p>Outliers aside, salaries are highest in California ($214,000), New York ($212,000), Washington ($203,000), Virginia ($195,000), and Illinois ($191,000). Massachusetts comes next at $189,000. At $183,000, average salaries in Texas are lower than we’d expect, but they’re still slightly above the national average ($182,000). States with high average salaries tended to have the largest numbers of respondents—with the important exceptions that we’ve already noted. The lowest salaries are found in West Virginia ($87,000) and New Mexico ($84,000), but these reflected a small number of respondents (one and four, respectively). These two states aside, the average salary in every state was over $120,000 (Figure 6).</p>



<p>So, is remote work equalizing salaries between different geographical regions? It’s still too early to say. We don’t think there will be a mass exodus from high-salary states to more rural states, but it’s clear that professionals who want to make that transition can, and that companies that aren’t in high-salary regions will need to offer salaries that compete in the nationwide market. Future surveys will tell us whether this pattern holds true.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0106-1-558x1048.png" alt="" class="wp-image-14556" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0106-1-558x1048.png 558w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0106-1-160x300.png 160w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0106-1-768x1443.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0106-1-817x1536.png 817w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0106-1-1090x2048.png 1090w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0106-1.png 1256w" sizes="(max-width: 558px) 100vw, 558px" /><figcaption>Figure 6. <em>Average salary by state</em></figcaption></figure>



<h2>Salaries by Age</h2>



<p>The largest group of respondents to our survey were between 45 and 54 years old (Figure 7). This group also had the highest average salary ($196,000). Salaries for respondents between 55 and 65 years old were lower (averaging $173,000), and salaries dropped even more for respondents over 65 ($139,000). Salaries for the 18- to 24-year-old age range were low, averaging $87,000. These lower salaries are no surprise because this group includes both students and those starting their first jobs after college.</p>



<p>It’s worth noting that our respondents were older than we expected; 29% were between 35 and 44 years old, 36% were between 45 and 54, and 22% were between 55 and 64. Data from our learning platform shows that this distribution isn’t indicative of the field as a whole, or of our audience. It may be an artifact of the survey itself. Are our newsletter readers older, or are older people more likely to respond to surveys? We don’t know.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0107-1-1048x420.png" alt="" class="wp-image-14557" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0107-1-1048x420.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0107-1-300x120.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0107-1-768x308.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0107-1.png 1256w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 7. <em>Average salary by age</em></figcaption></figure>



<p>The drop in salaries after age 55 is surprising. Does seniority count for little? It’s easy to make hypotheses: Senior employees are less likely to change jobs, and we’ve seen that changing jobs drives higher salaries. But it’s also worth noting that AWS launched in 2002, roughly 20 years ago. People who are now 45 to 54 years old started their careers in the first years of Amazon’s rollout. They “grew up” with the cloud; they’re the real cloud natives, and that appears to be worth something in today’s market.</p>



<h2>Job Titles and Roles</h2>



<p>Job titles are problematic. There’s no standardized naming system, so a programming lead at one company might be an architect or even a CTO at another. So we ask about job titles at a fairly high level of abstraction. We offered respondents a choice of four “general” roles: executive, director, manager, or associate. We also allowed respondents to write in their own job titles; roughly half chose this option. The write-in titles were more descriptive and, as expected, inconsistent. We were able to group them into some significant clusters by looking for people whose write-in title used the words “engineer,” “programmer,” “developer,” “architect,” “consultant,” or “DevOps.” We also looked at two modifiers: “senior” and “lead.” There’s certainly room for overlap: someone could be a “senior DevOps engineer.” But in practice, overlap was small. (For example, no respondents used both “developer” and “architect” in a write-in job title.) There was no overlap between the titles submitted by respondents and the general titles we offered on the survey: our respondents had to choose one or the other.</p>



<p>So what did we see? As shown in&nbsp;Figure 8, the highest salaries go to those who classified themselves as directors ($235,000) or executives ($231,000). Salaries for architects, “leads,” and managers are on the next tier ($196,000, $190,000, and $188,000, respectively). People who identified as engineers earn slightly lower salaries ($175,000). Associates, a relatively junior category, earn an average of $140,000 per year. Those who used “programmer” in their job title are a puzzle. There were only three of them, which is a surprise in itself, and all have salaries in the $50,000 to $100,000 range (average $86,000). Consultants also did somewhat poorly, with an average salary of $129,000.</p>



<p>Those who identified as engineers (19%) made up the largest group of respondents, followed by associates (18%). Directors and managers each comprised 15% of the respondents. That might be a bias in our survey, since it’s difficult to believe that 30% of cloud professionals have directorial or managerial roles. (That fits the observation that our survey results may skew toward older participants.) Architects were less common (7%). And relatively few respondents identified themselves with the terms “DevOps” (2%), “consultant” (2%), or “developer” (2%). The small number of people who identify with DevOps is another puzzle. It’s often been claimed that the cloud makes operations teams unnecessary; “NoOps” shows up in discussions from time to time. But we’ve never believed that. Cloud deployments still have a significant operational component. While the cloud may allow a smaller group to oversee a huge number of virtual machines, managing those machines has become more complex—particularly with cloud orchestration tools like Kubernetes.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0108-1-1048x807.png" alt="" class="wp-image-14558" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0108-1-1048x807.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0108-1-300x231.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0108-1-768x592.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0108-1.png 1237w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 8. <em>Average salary by job title</em></figcaption></figure>



<p>We also tried to understand what respondents are doing at work by asking about job roles, decoupling responsibilities from titles (Figure 9). So in another question, we asked respondents to choose between marketing, sales, product, executive, programmer, and architect roles, with no write-in option. Executives earn the highest salaries ($237,000) but were a relatively small group (14%). Architects are paid $188,000 per year on average; they were 33% of respondents. And for this question, respondents didn’t hesitate to identify as programmers: this group was the largest (43%), with salaries somewhat lower than architects ($163,000). This is roughly in agreement with the data we got from job titles. (And we should have asked about operations staff. Next year, perhaps.)</p>



<p>The remaining three groups—marketing, sales, and product—are relatively small. Only five respondents identified their role as marketing (0.6%), but they were paid well ($187,000). 1.5% of the respondents identified as sales, with an average salary of $186,000. And 8% of the respondents identified themselves with product, with a somewhat lower average salary of $162,000.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0109-1-1048x387.png" alt="" class="wp-image-14559" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0109-1-1048x387.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0109-1-300x111.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0109-1-768x284.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0109-1.png 1247w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 9. <em>Average salary by role</em></figcaption></figure>



<h2>Working from Home</h2>



<p>When we were planning this survey, we were very curious about&nbsp;<em>where</em>&nbsp;people worked. Many companies have moved to a fully remote work model (as O’Reilly has), and many more are taking a hybrid approach. But just how common is remote work? And what consequences does it have for the employees who work from home rather than in an office?</p>



<p>It turns out that remote work is surprisingly widespread (Figure 10). We found that only 6% of respondents answered no to the question “Do you work remotely?” More than half (63%) said that they work remotely all the time, and the remainder (31%) work remotely 1–4 days per week.</p>



<p>Working remotely is also associated with higher salaries: the average salary for people who work remotely 1–4 days a week is $188,000. It’s only slightly less ($184,000) for people who work remotely all the time. Salaries are sharply lower for people who never work remotely (average $131,000).</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0110-1-1048x223.png" alt="" class="wp-image-14560" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0110-1-1048x223.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0110-1-300x64.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0110-1-768x163.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0110-1.png 1259w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 10. <em>Salaries and remote work</em></figcaption></figure>



<p>Salary increases show roughly the same pattern (Figure 11). While salaries are slightly higher for respondents who occasionally work in the office, salary increases were higher for those who are completely remote: the average increase was $8,400 for those who are remote 100% of the time, while those who work from home 1–4 days per week only averaged a $7,800 salary increase. We suspect that given time, these two groups would balance out. Salary changes for those who never work remotely were sharply lower ($4,500).</p>



<p>Of all jobs in the computing industry, cloud computing is probably the most amenable to remote work. After all, you’re working with systems that are remote by definition. You’re not reliant on your own company’s data center. If the application crashes in the middle of the night, nobody will be rushing to the machine room to reboot the server. A laptop and a network connection are all you need.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0111-1-1048x228.png" alt="" class="wp-image-14561" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0111-1-1048x228.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0111-1-300x65.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0111-1-768x167.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0111-1.png 1246w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 11. <em>Salary increases and remote work</em></figcaption></figure>



<p>We’re puzzled by the relatively low salaries and salary increases for those who never work remotely. While there were minor differences, as you’d expect, there were no “smoking guns”: no substantial differences in education or job titles or roles. Does this difference reflect old-school companies that don’t trust their staff to be productive at home? And do they pay correspondingly lower salaries? If so, they’d better be forewarned: it’s very easy for employees to change jobs in the current labor market.</p>



<p>As the pandemic wanes (if indeed it wanes—despite what people think,&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://covid.cdc.gov/covid-data-tracker/#datatracker-home" target="_blank">that’s not what the data shows</a>), will companies stick with remote work or will they require employees to come back to the office? Some companies have already asked their employees to return. But we believe that the trend toward remote work will be hard, if not impossible, to reverse, especially in a job market where employers are competing for talent. Remote work certainly raises issues about onboarding new hires, training, group dynamics, and more. And it’s not without problems for the employees themselves: childcare, creating appropriate work spaces, etc. These challenges notwithstanding, it’s difficult to imagine people who have eliminated a lengthy commute from their lives going back to the office on a permanent basis.</p>



<h2>Certifications and Training</h2>



<p>Nearly half (48%) of our respondents participated in technical training or certification programs in the last year. 18% of them obtained one or more certifications, suggesting that 30% participated in training or some other form of professional development that wasn’t tied to a certification program.</p>



<p>The most common reasons for participating in training were learning new technologies (42%) and improving existing skills (40%). (Percentages are relative to the total number of respondents, which was 778.) 21% wanted to work on more interesting projects. The other possible responses were chosen less frequently: 9% of respondents wanted to move into a leadership role, and 12% were required to take training. Job security was an issue for 4% of the respondents, a very small minority. That’s consistent with our observation that employees have the upper hand in the labor market and are more concerned with advancement than with protecting their status quo.</p>



<p>Survey participants obtained a very broad range of certifications. We asked specifically about 11 cloud certifications that we identified as being particularly important. Most were specific to one of the three major cloud vendors: Microsoft Azure, Amazon Web Services, and Google Cloud. However, the number of people who obtained any specific certification was relatively small. The most popular certifications were AWS Certified Cloud Practitioner and Solutions Architect (both 4% of the total number of respondents). However, 8% of respondents answered “other” and provided a write-in answer. That’s 60 respondents—and we got 55 different write-ins. Obviously, there was very little duplication. The only submissions with multiple responses were CKA (Certified Kubernetes Administrator) and CKAD (Certified Kubernetes Application Developer). The range of training in this “other” group was extremely broad, spanning various forms of Agile training, security, machine learning, and beyond. Respondents were pursuing many vendor-specific certifications, and even academic degrees. (It’s worth noting that our&nbsp;<em>2021 Data/AI Salary Survey</em>report also concluded that earning a certification for one of the major cloud providers was a useful tool for career advancement.)</p>



<p>Given the number of certifications that are available, this isn’t surprising. It’s somewhat more surprising that there isn’t any consensus on which certifications are most important. When we look at salaries, though, we see some signals&#8230;at least among the leading certifications. The largest salaries are associated with Google Cloud Certified Professional Cloud Architect ($231,000). People who earned this certification also received a substantial salary increase (7.1%). Those who obtained an AWS Certified Solutions Architect &#8211; Professional, AWS Certified Solutions Architect &#8211; Associate, or Microsoft Certified: Azure Solutions Architect Expert certification also earn very high salaries ($212,000, $201,000, and $202,000, respectively), although these three received smaller salary increases (4.6%, 4.4%, and 4.0%, respectively). Those who earned the CompTIA Cloud+ certification receive the lowest salary ($132,000) and got a relatively small salary increase (3.5%). The highest salary increase went to those who obtained the Google Cloud Certified Professional Cloud DevOps Engineer certification (9.7%), with salaries in the middle of the range ($175,000).</p>



<p>We can’t draw any conclusions about the salaries or salary increases corresponding to the many certifications listed among the “other” responses; most of those certifications only appeared once. But it seems clear that the largest salaries and salary increases go to those who are certified for one of the big three platforms: Google Cloud, AWS, and Microsoft Azure (Figures&nbsp;12&nbsp;and&nbsp;13).</p>



<p>The salaries and salary increases for the two Google certifications are particularly impressive. Given that Google Cloud is the least widely used of the major platforms, and that the number of respondents for these certifications was relatively small, we suspect that talent proficient with Google’s tools and services is harder to find and drives the salaries up.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0112-1-1048x708.png" alt="" class="wp-image-14562" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0112-1-1048x708.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0112-1-300x203.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0112-1-768x519.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0112-1.png 1254w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 12. <em>Average salary by certification</em></figcaption></figure>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0113-1-1048x770.png" alt="" class="wp-image-14563" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0113-1-1048x770.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0113-1-300x220.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0113-1-768x564.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0113-1.png 1237w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 13. <em>Average salary increase by certification</em></figcaption></figure>



<p>Our survey respondents engaged in many different types of training. The most popular were watching videos and webinars (41%), reading books (39%), and reading blogs and industry articles (34%). 30% of the respondents took classes online. Given the pandemic, it isn’t at all surprising that only 1.7% took classes in person. 23% attended conferences, either online or in person. (We suspect that the majority attended online.) And 24% participated in company-offered training.</p>



<p>There’s surprisingly little difference between the average salaries associated with each type of learning. That’s partly because respondents were allowed to choose more than one response. But it’s also notable that the average salaries for most types of learning are lower than the average salary for the respondents as a whole. The average salary by type of learning ranges from $167,000 (in-person classes) to $184,000 (company-provided educational programs). These salaries are on the low side compared to the overall average of $182,000. Lower salaries may indicate that training is most attractive to people who want to get ahead in their field. This fits the observation that most of the people who participated in training did so to obtain new skills or to improve current ones. After all, to many companies “the cloud” is still relatively new, and they need to retrain their current workforces.</p>



<p>When we look at the time that respondents spent in training (Figure 14), we see that the largest group spent 20–39 hours in the past year (13% of all the respondents). 12% spent 40–59 hours; and 10% spent over 100 hours. No respondents reported spending 10–19 hours in training. (There were also relatively few in the 80–99 hour group, but we suspect that’s an artifact of “bucketing”: if you’ve taken 83 hours of training, you’re likely to think, “I don’t know how much time I spent in training, but it was a lot,” and choose 100+.) The largest salary increases went to those who spent 40–59 hours in training, followed by those who spent over 100 hours; the smallest salary increases, and the lowest salaries, went to those who only spent 1–9 hours in training. Managers take training into account when planning compensation, and those who skimp on training shortchange themselves.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0114-1-1048x514.png" alt="" class="wp-image-14564" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0114-1-1048x514.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0114-1-300x147.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0114-1-768x377.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0114-1.png 1253w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 14. <em>Percentage salary increase by time spent in training</em></figcaption></figure>



<h2>The Cloud Providers</h2>



<p>A survey of this type wouldn’t be complete without talking about the major cloud providers. There’s no really big news here (Figure 15). Amazon Web Services has the most users, at 72%, followed by Microsoft Azure (42%) and Google Cloud (31%). Compared to the cloud survey we did last year, it looks like Google Cloud and Azure have dropped slightly compared to AWS. But the changes aren’t large. Oracle’s cloud offering was surprisingly strong at 6%, and 4% of the respondents use IBM Cloud.</p>



<p>When we look at the biggest cloud providers that aren’t based in the US, we find that they’re still a relatively small component of cloud usage: 0.6% of respondents use Alibaba, while 0.3% use Tencent. Because there are so few users among our respondents, the percentages don’t mean much: a few more users, and we might see something completely different. That said, we expected to see more users working with Alibaba; it’s possible that tensions between the United States and China have made it a less attractive option.</p>



<p>20% of the respondents reported using a private cloud. While it’s not entirely clear what the term “private cloud” means—for some, it just means a traditional data center—almost all the private cloud users also reported using one of the major cloud providers. This isn’t surprising; private clouds make the most sense as part of a hybrid or multicloud strategy, where the private cloud holds data that must be kept on premises for security or compliance reasons.</p>



<p>6% of the respondents reported using a cloud provider that we didn’t list. These answers were almost entirely from minor cloud providers, which had only one or two users among the survey&nbsp;participants. And surprisingly, 4% of the respondents reported that they weren’t using any cloud provider.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0115-1-1048x595.png" alt="" class="wp-image-14565" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0115-1-1048x595.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0115-1-300x170.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0115-1-768x436.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0115-1.png 1255w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 15. <em>Cloud provider usage by percentage of respondents</em></figcaption></figure>



<p>There’s little difference between the salaries reported by people using the major providers (Figure 16). Tencent stands out; the average salary for its users is $275,000. But there were so few Tencent users among the survey respondents that we don’t believe this average is meaningful. There appears to be a slight salary premium for users of Oracle ($206,000) and Google ($199,000); since these cloud providers aren’t as widely used, it’s easy to assume that organizations committed to them are willing to pay slightly more for specialized talent, a phenomenon we’ve observed elsewhere. Almost as a footnote, we see that the respondents who don’t use a cloud have significantly lower salaries ($142,000).</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0116-1-1048x598.png" alt="" class="wp-image-14566" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0116-1-1048x598.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0116-1-300x171.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0116-1-768x438.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0116-1.png 1259w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 16. <em>Average salary by cloud provider</em></figcaption></figure>



<p>Cloud providers offer many services, but their basic services fall into a few well-defined classes (Figure 17). 75% of the survey respondents reported using virtual instances (for example, AWS EC2), and 74% use bucket storage (for example, AWS S3). These are services that are offered by every cloud provider. Most respondents use an SQL database (59%). Somewhat smaller numbers reported using a NoSQL database (41%), often in conjunction with an SQL database. 49% use container orchestration services; 45% use “serverless,” which suggests that serverless is more popular than we’ve seen in our other recent surveys.</p>



<p>Only 11% reported using some kind of AutoML—again, a service that’s provided by all the major cloud providers, though under differing names. And again, we saw no significant differences in salary based on what services were in use. That makes perfect sense; you wouldn’t pay a carpenter more for using a hammer than for using a saw.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0117-1-1048x456.png" alt="" class="wp-image-14567" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0117-1-1048x456.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0117-1-300x130.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0117-1-768x334.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0117-1.png 1256w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 17. <em>Basic cloud services usage by percentage of respondents</em></figcaption></figure>



<h2>The Work Environment</h2>



<p>Salaries aside, what are cloud developers working with? What programming languages and tools are they using?</p>



<h3>Languages</h3>



<p>Python is the most widely used language (59% of respondents), followed by SQL (49%), JavaScript (45%), and Java (32%). It’s somewhat surprising that only a third of the respondents use Java, given that programming language surveys done by TIOBE and RedMonk almost always have Java, Python, and JavaScript in a near tie for first place. Java appears not to have adapted well to the cloud (Figure 18).</p>



<p>Salaries also follow a pattern that we’ve seen before. Although the top four languages are in high demand, they don’t command particularly high salaries: $187,000 for Python, $179,000 for SQL, $181,000 for JavaScript, and $188,000 for Java (Figure 19). These are all “table stakes” languages: they’re necessary and they’re what most programmers use on the job, but the programmers who use them don’t stand out. And despite the necessity, there’s a lot of talent available to fill these roles. As we saw in last year’s&nbsp;<em>Data/AI Salary Survey</em>&nbsp;report, expertise in Scala, Rust, or Go commands a higher salary ($211,000, $202,000, and $210,000, respectively). While the demand for these languages isn’t as high, there’s a lot less available expertise. Furthermore, fluency in any of these languages shows that a programmer has gone considerably beyond basic competence. They’ve done the work necessary to pick up additional skills.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0118-1-1048x887.png" alt="" class="wp-image-14568" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0118-1-1048x887.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0118-1-300x254.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0118-1-768x650.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0118-1.png 1219w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 18. <em>Programming language usage by percentage of respondents</em></figcaption></figure>



<p>The lowest salaries were reported by respondents using PHP ($155,000). Salaries for C, C++, and C# are also surprisingly low ($170,000, $172,000, and $170,000, respectively); given the importance of C and C++ for software development in general and the importance of C# for the Microsoft world, we find it hard to understand why.</p>



<p>Almost all of the respondents use multiple languages. If we had to make a recommendation for someone who wanted to move into cloud development or operations, or for someone planning a cloud strategy from scratch, it would be simple: focus on SQL plus one of the other table stakes languages (Java, JavaScript, or Python). If you want to go further, pick one of the languages associated with the highest salaries. We think Scala is past its peak, but because of its strong connection to the Java ecosystem, Scala makes sense for Java programmers. For Pythonistas, we’d recommend choosing Go or Rust.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0119-1-1048x879.png" alt="" class="wp-image-14569" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0119-1-1048x879.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0119-1-300x252.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0119-1-768x644.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0119-1.png 1254w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 19. <em>Average salary by programming language</em></figcaption></figure>



<h3>Operating Systems</h3>



<p>We asked our survey participants which operating systems they used so we could test something we’ve heard from several people who hire software developers: Linux is a must. That appears to be the case: 80% of respondents use Linux (Figure 20). Even though Linux really hasn’t succeeded in the desktop market (sorry), it’s clearly the operating system for most software that runs in the cloud. If Linux isn’t a requirement, it’s awfully close.</p>



<p>67% of the respondents reported using macOS, but we suspect that’s mostly as a desktop or laptop operating system. Of the major providers, only AWS offers macOS virtual instances, and they’re not widely used. (Apple’s license only allows macOS to run on Apple hardware, and only AWS provides Apple servers.) 57% of the respondents reported using some version of Windows. While we suspect that Windows is also used primarily as a desktop or laptop operating system, Windows virtual instances are available from all the major providers, including Oracle and IBM.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0120-1-1048x227.png" alt="" class="wp-image-14570" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0120-1-1048x227.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0120-1-300x65.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0120-1-768x167.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0120-1.png 1240w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 20. <em>Operating system usage by percentage of respondents</em></figcaption></figure>



<h3>Tools</h3>



<p>We saw little variation in salary from tool to tool. This lack of variation makes sense. As we said above, we don’t expect a carpenter who uses a hammer to be paid more than a carpenter who uses a saw. To be a competent carpenter, you need to use both, along with levels, squares, and a host of other tools.</p>



<p>However, it is interesting to know what tools are commonly in use (Figure 21). There aren’t any real surprises. Docker is almost universal, used by 76% of the respondents. Kubernetes use is very widespread, by 61% of the respondents. Other components of the Kubernetes ecosystem didn’t fare as well: 27% of respondents reported using Helm, and 12% reported using Istio, which has been widely criticized for being too complex.</p>



<p>Alternatives to this core cluster of tools don’t appear to have much traction. 10% of the respondents reported using OpenShift, the IBM/Red Hat package that includes Kubernetes and other core components. Our respondents seem to prefer building their tooling environment themselves. Podman, an alternative to Docker and a component of OpenShift, is only used by 8% of the respondents. Unfortunately, we didn’t ask about Linkerd, which appears to be establishing itself as a service mesh that’s simpler to configure than Istio. However, it didn’t show up among the write-in responses, and the number of respondents who said “other” was relatively small (9%).</p>



<p>The HashiCorp tool set (Terraform, Consul, and Vault) appears to be more widely used: 41% of the respondents reported using Terraform, 17% use Vault, and 8% use Consul. However, don’t view these as alternatives to Kubernetes. Terraform is a tool for building and configuring cloud infrastructure, and Vault is a secure repository for secrets. Only Consul competes directly.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0121-1-1048x657.png" alt="" class="wp-image-14571" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0121-1-1048x657.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0121-1-300x188.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0121-1-768x482.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/06/csr_0121-1.png 1231w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 21. <em>Tool usage by percentage of respondents</em></figcaption></figure>



<h2>The Biggest Impact</h2>



<p>Finally, we asked the respondents what would have the biggest impact on compensation and promotion. The least common answer was “data tools” (6%). This segment of our audience clearly isn’t working directly with data science or AI—though we’d argue that might change as more machine learning applications reach production. “Programming languages” was second from the bottom. The lack of concern about programming languages reflects reality. While we observed higher salaries for respondents who used Scala, Rust, or Go, if you’re solidly grounded in the basics (like Python and SQL), you’re in good shape. There’s limited value in pursuing additional languages once you have the table stakes.</p>



<p>The largest number of respondents said that knowledge of “cloud and containers” would have the largest effect on compensation. Again, containers are table stakes, as we saw in the previous section. Automation, security, and machine learning were also highly rated (18%, 15%, and 16%, respectively). It’s not clear why machine learning was ranked highly but data tools wasn’t. Perhaps our respondents interpreted “data tools” as software like Excel, R, and pandas.</p>



<p>11% of the respondents wrote in an answer. As usual with write-ins, the submissions were scattered, and mostly singletons. However, many of the write-in answers pointed toward leadership and management skills. Taken all together, these varied responses add up to about 2% of the total respondents. Not a large number, but still a signal that some part of our audience is thinking seriously about IT leadership.</p>



<h2>Confidence in the Future</h2>



<p>“Cloud adoption is up and to the right”? No, we already told you we weren’t going to conclude that. Though it’s no doubt true; we don’t see cloud adoption slowing in the near future.</p>



<p>Salaries are high. That’s good for employees and difficult for employers. It’s common for staff to jump to another employer offering a higher salary and a generous signing bonus. The current stock market correction may put a damper on that trend. There are signs that Silicon Valley’s money supply is starting to dry up, in part because of higher interest rates but also because investors are nervous about how the online economy will respond to regulation, and impatient with startups whose business plan is to lose billions “buying” a market before they figure out how to make money. Higher interest rates and nervous investors could mean an end to skyrocketing salaries.</p>



<p>The gap between women’s and men’s salaries has narrowed, but it hasn’t closed. While we don’t have a direct comparison for the previous year, last year’s&nbsp;<em>Data/AI Salary Survey</em>report showed a 16% gap. In this survey, the gap has been cut to 7%, and women are receiving salary increases that are likely to close that gap even further. It’s anyone’s guess how this will play out in the future. Talent is in short supply, and that puts upward pressure on salaries. Next year, will we see women’s salaries on par with men’s? Or will the gap widen again when the talent shortage isn’t so acute?</p>



<p>While we aren’t surprised by the trend toward remote work, we are surprised at how widespread remote work has become: as we saw, only 10% of our survey respondents never work remotely, and almost two-thirds work remotely full time. Remote work may be easier for cloud professionals, because part of their job is inherently remote. However, after seeing these results, we&#8217;d predict similar numbers for other industry sectors. Remote work is here to stay.</p>



<p>Almost half of our survey respondents participated in some form of training in the past year. Training on the major cloud platforms (AWS, Azure, and Google Cloud) was associated with higher salaries. However, our participants also wrote in 55 “other” kinds of training and certifications, of which the most popular was CKA (Certified Kubernetes Administrator).</p>



<p>Let’s end by thinking a bit more about the most common answer to the question “What area do you feel will have the biggest impact on compensation and promotion in the next year?”: cloud and containers. Our first reaction is that this is a poorly phrased option; we should have just asked about containers. Perhaps that’s true, but there’s something deeper hidden in this answer. If you want to get ahead in cloud computing, learn more about the cloud. It’s tautological, but it also shows some real confidence in where the industry is heading. Cloud professionals may be looking for their next employer, but they aren’t looking to jump ship to the “next big thing.” Businesses aren’t jumping away from the cloud to “the next big thing” either; whether it’s AI, the “metaverse,” or something else, their next big thing will be built in the cloud. And containers are the building blocks of the cloud; they’re the foundation on which the future of cloud computing rests. Salaries are certainly “up and to the right,” and we don’t see demand for cloud-capable talent dropping any time in the near future.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/2022-cloud-salary-survey/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>AI Adoption in the Enterprise 2022</title>
		<link>https://www.oreilly.com/radar/ai-adoption-in-the-enterprise-2022/</link>
				<comments>https://www.oreilly.com/radar/ai-adoption-in-the-enterprise-2022/#respond</comments>
				<pubDate>Thu, 31 Mar 2022 11:35:34 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides]]></dc:creator>
				<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Research]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=14410</guid>
				<description><![CDATA[In December 2021 and January 2022, we asked recipients of our&#160;Data&#160;and&#160;AI Newsletters&#160;to participate in our annual survey on AI adoption. We were particularly interested in what, if anything, has changed since last year. Are companies farther along in AI adoption? Do they have working applications in production? Are they using tools like AutoML to generate [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>In December 2021 and January 2022, we asked recipients of our&nbsp;<em>Data</em>&nbsp;and&nbsp;<em>AI Newsletters</em>&nbsp;to participate in our annual survey on AI adoption. We were particularly interested in what, if anything, has changed since last year. Are companies farther along in AI adoption? Do they have working applications in production? Are they using tools like AutoML to generate models, and other tools to streamline AI deployment? We also wanted to get a sense of where AI is headed. The hype has clearly moved on to blockchains and NFTs. AI is in the news often enough, but the steady drumbeat of new advances and techniques has gotten a lot quieter.</p>



<p>Compared to last year, significantly fewer people responded. That’s probably a result of timing. This year’s survey ran during the holiday season (December 8, 2021, to January 19, 2022, though we received very few responses in the new year); last year’s ran from January 27, 2021, to February 12, 2021. Pandemic or not, holiday schedules no doubt limited the number of respondents.</p>



<p>Our results held a bigger surprise, though. The smaller number of respondents notwithstanding, the results were&nbsp;<a href="https://learning.oreilly.com/library/view/ai-adoption-in/9781098109196/">surprisingly similar to 2021</a>. Furthermore, if you go back another year, the 2021 results were themselves&nbsp;<a href="https://learning.oreilly.com/library/view/ai-adoption-in/9781492081210/">surprisingly similar to 2020</a>. Has that little changed in the application of AI to enterprise problems? Perhaps. We considered the possibility that the same individuals responded in both 2021 and 2022. That wouldn’t be surprising, since both surveys were publicized through our mailing lists—and some people like responding to surveys. But that wasn’t the case. At the end of the survey, we asked respondents for their email address. Among those who provided an address, there was only a 10% overlap between the two years.</p>



<p>When nothing changes, there’s room for concern: we certainly aren’t in an “up and to the right” space. But is that just an artifact of the hype cycle? After all, regardless of any technology’s long-term value or importance, it can only receive outsized media attention for a limited time. Or are there deeper issues gnawing at the foundations of AI adoption?</p>



<h2>AI Adoption</h2>



<p>We asked participants about the level of AI adoption in their organization. We structured the responses to that question differently from prior years, in which we offered four responses: not using AI, considering AI, evaluating AI, and having AI projects in production (which we called &#8220;mature&#8221;). This year we combined “evaluating AI” and “considering AI”; we thought that the difference between “evaluating” and “considering” was poorly defined at best, and if we didn’t know what it meant, our respondents didn’t either. We kept the question about projects in production, and we&#8217;ll use the words &#8220;in production&#8221; rather than &#8220;mature practice&#8221; to talk about this year&#8217;s results.</p>



<p>Despite the change in the question, the responses were surprisingly similar to last year&#8217;s. The same percentage of respondents said that their organizations had AI projects in production (26%). Significantly more said that they weren’t using AI: that went from 13% in 2021 to 31% in this year’s survey. It’s not clear what that shift means. It’s possible that it’s just a reaction to the change in the answers; perhaps respondents who were “considering” AI thought “considering really means that we’re not using it.” It’s also possible that AI is just becoming part of the toolkit, something developers use without thinking twice. Marketers use the term AI; software developers tend to say machine learning. To the customer, what’s important isn’t how the product works but what it does. There’s already a lot of AI embedded into products that we never think about.</p>



<p>From that standpoint, many companies with AI in production don’t have a single AI specialist or developer. Anyone using Google, Facebook, or Amazon (and, I presume, most of their competitors) for advertising is using AI. AI as a service includes AI packaged in ways that may not look at all like neural networks or deep learning. If you install a smart customer service product that uses GPT-3, you’ll never see a hyperparameter to tune—but you have deployed an AI application. We don’t expect respondents to say that they have “AI applications deployed” if their company has an advertising relationship with Google, but AI is there, and it&#8217;s real, even if it&#8217;s invisible.</p>



<p>Are those invisible applications the reason for the shift? Is AI disappearing into the walls, like our plumbing (and, for that matter, our computer networks)? We’ll have reason to think about that throughout this report.</p>



<p>Regardless, at least in some quarters, attitudes seem to be solidifying against AI, and that could be a sign that we’re approaching another “AI winter.” We don’t think so, given that the number of respondents who report AI in production is steady and up slightly. However, it&nbsp;<em>is</em>&nbsp;a sign that AI has passed to the next stage of the&nbsp;<a href="https://en.wikipedia.org/wiki/Gartner_hype_cycle">hype cycle</a>. When expectations about what AI can deliver are at their peak, everyone says they’re doing it, whether or not they really are. And once you hit the trough, no one says they’re using it, even though they now are.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig01.png" alt="" class="wp-image-14411" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig01.png 494w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig01-300x236.png 300w" sizes="(max-width: 494px) 100vw, 494px" /><figcaption>Figure 1.<em> AI adoption and maturity</em></figcaption></figure>



<p>The trailing edge of the hype cycle has important consequences for the practice of AI. When it was in the news every day, AI didn’t really have to prove its value; it was enough to be interesting. But once the hype has died down, AI has to show its value in production, in real applications: it’s time for it to prove that it can deliver real business value, whether that’s cost savings, increased productivity, or more customers. That will no doubt require better tools for collaboration between AI systems and consumers, better methods for training AI models, and better governance for data and AI systems.</p>



<h3>Adoption by Continent</h3>



<p>When we looked at responses by geography, we didn’t see much change since last year. The greatest increase in the percentage of respondents with AI in production was in Oceania (from 18%&nbsp;to 31%), but that was a relatively small segment of the total number of respondents (only 3.5%)—and when there are few respondents, a small change in the numbers can produce a large change in the apparent percentages. For the other continents, the percentage of respondents with AI in production agreed within 2%.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig02.png" alt="" class="wp-image-14412" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig02.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig02-300x213.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig02-768x545.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Figure 2. <em>AI adoption by continent</em></figcaption></figure>



<p>After Oceania, North America and Europe had the greatest percentages of respondents with AI in production (both 27%), followed by Asia and South America (24% and 22%, respectively). Africa had the smallest percentage of respondents with AI in production (13%) and the largest percentage of nonusers (42%). However, as with Oceania, the number of respondents from Africa was small, so it’s hard to put too much credence in these percentages. We continue to hear exciting stories about&nbsp;<a href="https://african.business/2021/03/technology-information/the-very-real-benefits-of-ai-in-africa/">AI in Africa</a>, many of which demonstrate creative thinking that is sadly lacking in the VC-frenzied markets of North America, Europe, and Asia.</p>



<h3>Adoption by Industry</h3>



<p>The distribution of respondents by industry was almost the same as last year. The largest percentages of respondents were from the computer hardware and financial services industries (both about 15%, though computer hardware had a slight edge), education (11%), and healthcare (9%). Many respondents reported their industry as “Other,” which was the third most common answer. Unfortunately, this vague category isn’t very helpful, since it featured industries ranging from academia to wholesale, and included some exotica like drones and surveillance—intriguing but hard to draw conclusions from based on one or two responses. (Besides, if you’re working on surveillance, are you really going to tell people?) There were well over 100 unique responses, many of which overlapped with the industry sectors that we listed.</p>



<p>We see a more interesting story when we look at the maturity of AI practices in these industries. The retail and financial services industries had the greatest percentages of respondents reporting AI applications in production (37% and 35%, respectively). These sectors also had the fewest respondents reporting that they weren’t using AI (26% and 22%). That makes a lot of intuitive sense: just about all retailers have established an online presence, and part of that presence is making product recommendations, a classic AI application. Most retailers using online advertising services rely heavily on AI, even if they don’t consider using a service like Google “AI in production.” AI is certainly there, and it’s driving revenue, whether or not they’re aware of it. Similarly, financial services companies were early adopters of AI: automated check reading was one of the first enterprise AI applications, dating to well before the current surge in AI interest.</p>



<p>Education and government were the two sectors with the fewest respondents reporting AI projects in production (9% for both). Both sectors had many respondents reporting that they were evaluating the use of AI (46% and 50%). These two sectors also had the largest percentage of respondents reporting that they weren’t using AI. These are industries where appropriate use of AI could be very important, but they’re also areas in which a lot of damage could be done by inappropriate AI systems. And, frankly, they’re both areas that are plagued by outdated IT infrastructure. Therefore, it’s not surprising that we see a lot of people evaluating AI—but also not surprising that relatively few projects have made it into production.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig03.png" alt="" class="wp-image-14413" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig03.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig03-300x249.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig03-768x637.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Figure 3. <em>AI adoption by industry</em></figcaption></figure>



<p>As you’d expect, respondents from companies with AI in production reported that a larger portion of their IT budget was spent on AI than did respondents from companies that were evaluating or not using AI. 32% of respondents with AI in production reported that their companies spent over 21% of their IT budget on AI (18% reported that 11%–20% of the IT budget went to AI; 20% reported 6%–10%). Only 12% of respondents who were evaluating AI reported that their companies were spending over 21% of the IT budget on AI projects. Most of the respondents who were evaluating AI came from organizations that were spending under 5% of their IT budget on AI (31%); in most cases, “evaluating” means a relatively small commitment. (And remember that roughly half of all respondents were in the “evaluating” group.)</p>



<p>The big surprise was among respondents who reported that their companies weren’t using AI. You’d expect their IT expense to be zero, and indeed, over half of the respondents (53%) selected 0%–5%; we’ll assume that means 0. Another 28% checked “Not applicable,” also a reasonable response for a company that isn’t investing in AI. But a measurable number had other answers, including 2% (10 respondents) who indicated that their organizations were spending over 21% of their IT budgets on AI projects. 13% of the respondents not using AI indicated that their companies were spending 6%–10% on AI, and 4% of that group estimated AI expenses in the 11%–20% range. So even when our respondents report that their organizations aren’t using AI, we find that they’re doing something: experimenting, considering, or otherwise “kicking the tires.” Will these organizations move toward adoption in the coming years? That’s anyone’s guess, but AI may be penetrating organizations that are on the back side of the adoption curve (the so-called “late majority”).</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig04.png" alt="" class="wp-image-14414" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig04.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig04-300x187.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig04-768x478.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Figure 4. <em>Share of IT budgets allocated to AI</em></figcaption></figure>



<p>Now look at the graph showing the percentage of IT budget spent on AI by industry. Just eyeballing this graph shows that most companies are in the 0%–5% range. But it’s more interesting to look at what industries are, and aren’t, investing in AI. Computers and healthcare have the most respondents saying that over 21% of the budget is spent on AI. Government, telecommunications, manufacturing, and retail are the sectors where respondents report the smallest (0%–5%) expense on AI. We’re surprised at the number of respondents from retail who report low IT spending on AI, given that the retail sector also had a high percentage of practices with AI in production. We don’t have an explanation for this, aside from saying that any study is bound to expose some anomalies.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig05.png" alt="" class="wp-image-14415" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig05.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig05-300x294.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig05-768x751.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Figure 5. <em>Share of IT budget allocated to AI, by industry</em></figcaption></figure>



<h3>Bottlenecks</h3>



<p>We asked respondents what the biggest bottlenecks were to AI adoption. The answers were strikingly similar to last year’s. Taken together, respondents with AI in production and respondents who were evaluating AI say the biggest bottlenecks were lack of skilled people and lack of data or data quality issues (both at 20%), followed by finding appropriate use cases (16%).</p>



<p>Looking at &#8220;in production&#8221;&nbsp;and “evaluating” practices separately gives a more nuanced picture. Respondents whose organizations were evaluating AI were much more likely to say that company culture is a bottleneck, a challenge that Andrew Ng addressed in a recent issue of his&nbsp;<a href="https://info.deeplearning.ai/the-batch-which-nation-dominates-ai-meet-your-robot-colleagues-gans-forecast-weather-unmasking-qanons-anonymous-conspiracy-theorist">newsletter</a>. They were also more likely to see problems in identifying appropriate use cases. That’s not surprising: if you have AI in production, you’ve at least partially overcome problems with company culture, and you’ve found at least some use cases for which AI is appropriate.</p>



<p>Respondents with AI in production were significantly more likely to point to lack of data or data quality as an issue. We suspect this is the result of hard-won experience. Data always looks much better before you’ve tried to work with it. When you get your hands dirty, you see where the problems are. Finding those problems, and learning how to deal with them, is an important step toward developing a truly mature AI practice. These respondents were somewhat more likely to see problems with technical infrastructure—and again, understanding the problem of building the infrastructure needed to put AI into production comes with experience.</p>



<p>Respondents who are using AI (the &#8220;evaluating&#8221; and &#8220;in production&#8221; groups—that is, everyone who didn&#8217;t identify themselves as a &#8220;non-user&#8221;)&nbsp;were in agreement on the lack of skilled people. A shortage of trained data scientists has been predicted for years. In&nbsp;<a href="https://www.oreilly.com/radar/ai-adoption-in-the-enterprise-2021/">last year’s survey of AI adoption</a>, we noted that we’ve finally seen this shortage come to pass, and we expect it to become more acute. This group of respondents were also in agreement about legal concerns. Only 7% of the respondents in each group listed this as the most important bottleneck, but it’s on respondents&#8217; minds.</p>



<p>And nobody’s worrying very much about hyperparameter tuning.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig06.png" alt="" class="wp-image-14416" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig06.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig06-292x300.png 292w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig06-768x789.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Figure 6. <em>Bottlenecks to AI adoption</em></figcaption></figure>



<p>Looking a bit further into the difficulty of hiring for AI, we found that respondents with AI in production saw the most significant skills gaps in these areas: ML modeling and data science (45%), data engineering (43%), and maintaining a set of business use cases (40%). We can rephrase these skills as core AI development, building data pipelines, and product management.&nbsp;<a href="https://www.oreilly.com/radar/product-management-for-ai/">Product management for AI</a>, in particular, is an important and still relatively new specialization that requires understanding the specific requirements of AI systems.</p>



<h3>AI Governance</h3>



<p>Among respondents with AI products in production, the number of those whose organizations had a governance plan in place to oversee how projects are created, measured, and observed was roughly the same as those that didn&#8217;t (49% yes, 51% no). Among respondents who were evaluating AI, relatively few (only 22%) had a governance plan.</p>



<p>The large number of organizations lacking AI governance is disturbing. While it’s easy to assume that AI governance isn’t necessary if you’re only doing some experiments and proof-of-concept projects, that’s dangerous. At some point, your proof-of-concept is likely to turn into an actual product, and then your governance efforts will be playing catch-up. It’s even more dangerous when you’re relying on AI applications in production. Without formalizing some kind of AI governance, you’re less likely to know when models are becoming stale, when results are biased, or when data has been collected improperly.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig07.png" alt="" class="wp-image-14417" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig07.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig07-300x102.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig07-768x260.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Figure 7. <em>Organizations with an AI governance plan in place</em></figcaption></figure>



<p>While we didn’t ask about AI governance in last year’s survey, and consequently can’t do year-over-year comparisons, we did ask respondents who had AI in production what risks they checked for. We saw almost no change. Some risks were up a percentage point or two and some were down, but the ordering remained the same. Unexpected outcomes remained the biggest risk (68%, down from 71%), followed closely by model interpretability and model degradation (both 61%). It’s worth noting that unexpected outcomes and model degradation are business issues. Interpretability, privacy (54%), fairness (51%), and safety (46%) are all human issues that may have a direct impact on individuals. While there may be AI applications where privacy and fairness aren’t issues (for example, an embedded system that decides whether the dishes in your dishwasher are clean), companies with AI practices clearly need to place a higher priority on the human impact of AI.</p>



<p>We’re also surprised to see that security remains close to the bottom of the list (42%, unchanged from last year). Security is finally being taken seriously by many businesses, just not for AI. Yet AI has&nbsp;<a href="https://hubsecurity.com/blog/cyber-security/security-threats-for-ai-and-machine-learning/">many unique risks</a>: data poisoning, malicious inputs that generate false predictions, reverse engineering models to expose private information, and many more among them. After last year’s many costly attacks against businesses and their data, there’s no excuse for being lax about cybersecurity. Unfortunately, it looks like AI practices are slow in catching up.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig08.png" alt="" class="wp-image-14418" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig08.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig08-300x197.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig08-768x505.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Figure 8. <em>Risks checked by respondents with AI in production</em></figcaption></figure>



<p>Governance and risk-awareness are certainly issues we’ll watch in the future. If companies developing AI systems don’t put some kind of governance in place,&nbsp;<a href="https://hbr.org/sponsored/2021/12/how-organizations-can-mitigate-the-risks-of-ai">they are risking their businesses</a>. AI will be controlling you, with unpredictable results—results that increasingly include damage to your reputation and large legal judgments. The least of these risks is that governance will be imposed by legislation, and those who haven’t been practicing AI governance will need to catch up.</p>



<h2>Tools</h2>



<p>When we looked at the tools used by respondents working at companies with AI in production, our results were very similar to last year’s. TensorFlow and scikit-learn are the most widely used (both 63%), followed by PyTorch, Keras, and AWS SageMaker (50%, 40%, and 26%, respectively). All of these are within a few percentage points of last year’s numbers, typically a couple of percentage points lower. Respondents were allowed to select multiple entries; this year the average number of entries per respondent appeared to be lower, accounting for the drop in the percentages (though we’re unsure why respondents checked fewer entries).</p>



<p>There appears to be some consolidation in the tools marketplace. Although it’s great to root for the underdogs, the tools at the bottom of the list were also slightly down: AllenNLP (2.4%), BigDL (1.3%), and RISELab’s Ray (1.8%). Again, the shifts are small, but dropping by one percent when you’re only at 2% or 3% to start with could be significant—much more significant than scikit-learn’s drop from 65% to 63%. Or perhaps not; when you only have a 3% share of the respondents, small, random fluctuations can seem large.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig09.png" alt="" class="wp-image-14419" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig09.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig09-291x300.png 291w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig09-768x792.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Figure 9. <em>Tools used by respondents with AI in production</em></figcaption></figure>



<h3>Automating ML</h3>



<p>We took an additional look at tools for automatically generating models. These tools are commonly called “AutoML” (though that’s also a product name used by Google and Microsoft). They’ve been around for a few years; the company developing DataRobot, one of the oldest tools for automating machine learning, was founded in 2012. Although building models and programming aren’t the same thing, these tools are part of the&nbsp;<a href="https://www.oreilly.com/radar/low-code-and-the-democratization-of-programming">“low code” movement</a>. AutoML tools fill similar needs: allowing more people to work effectively with AI and eliminating the drudgery of doing hundreds (if not thousands) of experiments to tune a model.</p>



<p>Until now, the use of AutoML has been a relatively small part of the picture. This is one of the few areas where we see a significant difference between this year and last year. Last year 51% of the respondents with AI in production said they weren’t using AutoML tools. This year only 33% responded “None of the above” (and didn’t write in an alternate answer).</p>



<p>Respondents who were “evaluating” the use of AI appear to be less inclined to use AutoML tools (45% responded “None of the above”). However, there were some important exceptions. Respondents evaluating ML were more likely to use Azure AutoML than respondents with ML in production. This fits anecdotal reports that Microsoft Azure is the most popular cloud service for organizations that are just moving to the cloud. It’s also worth noting that the usage of Google Cloud AutoML and IBM AutoAI was similar for respondents who were evaluating AI and for those who had AI in production.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig10.png" alt="" class="wp-image-14420" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig10.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig10-300x272.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig10-768x697.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Figure 10. <em>Use of AutoML tools</em></figcaption></figure>



<h3>Deploying and Monitoring AI</h3>



<p>There also appeared to be an increase in the use of automated tools for deployment and monitoring among respondents with AI in production. “None of the above” was still the answer chosen by the largest percentage of respondents (35%), but it was down from 46% a year ago. The tools they were using were similar to last year’s: MLflow (26%), Kubeflow (21%), and TensorFlow Extended (TFX, 15%). Usage of MLflow and Kubeflow increased since 2021; TFX was down slightly. Amazon SageMaker (22%) and TorchServe (6%) were two new products with significant usage; SageMaker in particular is poised to become a market leader. We didn’t see meaningful year-over-year changes for Domino, Seldon, or Cortex, none of which had a significant market share among our respondents. (BentoML is new to our list.)</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig11.png" alt="" class="wp-image-14421" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig11.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig11-300x256.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig11-768x655.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Figure 11. <em>Tools used for deploying and monitoring AI</em></figcaption></figure>



<p>We saw similar results when we looked at automated tools for data versioning, model tuning, and experiment tracking. Again, we saw a significant reduction in the percentage of respondents who selected “None of the above,” though it was still the most common answer (40%, down from 51%). A significant number said they were using homegrown tools (24%, up from 21%). MLflow was the only tool we asked about that appeared to be winning the hearts and minds of our respondents, with 30% reporting that they used it. Everything else was under 10%. A healthy, competitive marketplace? Perhaps. There’s certainly a lot of room to grow, and we don’t believe that the problem of data and model versioning has been solved yet.</p>



<h2>AI at a Crossroads</h2>



<p>Now that we’ve looked at all the data, where is AI at the start of 2022, and where will it be a year from now? You could make a good argument that AI adoption has stalled. We don’t think that’s the case. Neither do venture capitalists; a study by the OECD,&nbsp;<a href="https://www.oecd.org/digital/venture-capital-investments-in-artificial-intelligence-f97beae7-en.htm"><em>Venture Capital Investments in Artificial Intelligence</em></a>, says that in 2020, 20% of all VC funds went to AI companies. We would bet that number is also unchanged in 2021. But what are we missing? Is enterprise AI stagnating?</p>



<p>Andrew Ng, in his newsletter&nbsp;<a href="https://read.deeplearning.ai/the-batch/issue-137/"><em>The Batch</em></a>, paints an optimistic picture. He points to Stanford&#8217;s&nbsp;<a href="https://aiindex.stanford.edu/report/"><em>AI Index Report</em></a>&nbsp;for 2022, which says that private investment almost doubled between 2020 and 2021. He also points to the rise in regulation as evidence that AI is unavoidable: it&#8217;s an inevitable part of 21st century life. We agree that AI is everywhere, and in many places, it’s not even seen. As we’ve mentioned, businesses that are using third-party advertising services are almost certainly using AI, even if they never write a line of code. It’s embedded in the advertising application. Invisible AI—AI that has become part of the infrastructure—isn’t going away. In turn, that may mean that we’re thinking about AI deployment the wrong way. What’s important isn’t whether organizations have deployed AI on their own servers or on someone else’s. What we should really measure is whether organizations are using infrastructural AI that’s embedded in other systems that are provided as a service. AI as a service (including AI as part of another service) is an inevitable part of the future.</p>



<p>But not all AI is invisible; some is very visible. AI is being adopted in some ways that, until the past year, we’d have considered unimaginable. We’re all familiar with chatbots, and the idea that AI can give us better chatbots wasn’t a stretch. But GitHub’s Copilot was a shock: we didn’t expect AI to write software. We saw (and&nbsp;<a href="https://www.oreilly.com/radar/automated-coding-and-the-future-of-programming/">wrote about</a>) the research leading up to Copilot but didn’t believe it would become a product so soon. What’s more shocking? We’ve heard that, for some programming languages,&nbsp;<a href="https://www.axios.com/copilot-artificial-intelligence-coding-github-9a202f40-9af7-4786-9dcb-b678683b360f.html">as much as 30% of new code is being suggested by the company&#8217;s AI programming tool Copilot</a>. At first, many programmers thought that Copilot was no more than AI&#8217;s clever party trick. That&#8217;s clearly not the case.&nbsp;Copilot has become a useful tool in surprisingly little time, and with time, it will only get better.</p>



<p>Other applications of large language models—automated customer service, for example—are rolling out (our survey didn’t pay enough attention to them). It remains to be seen whether humans will feel any better about interacting with AI-driven customer service than they do with humans (or horrendously scripted bots). There’s an intriguing hint that AI systems are&nbsp;<a href="https://journals.sagepub.com/doi/abs/10.1177/00222429211066972">better at delivering bad news</a>&nbsp;to humans. If we need to be told something we don’t want to hear, we’d prefer it come from a faceless machine.</p>



<p>We’re starting to see more adoption of automated tools for deployment, along with tools for data and model versioning. That’s a necessity; if AI is going to be deployed into production, you have to be able to deploy it effectively, and modern IT shops don’t look kindly on handcrafted artisanal processes.</p>



<p>There are many more places we expect to see AI deployed, both visible and invisible. Some of these applications are quite simple and low-tech. My four-year-old car displays the speed limit on the dashboard. There are any number of ways this could be done, but after some observation, it became clear that this was a simple computer vision application. (It would report incorrect speeds if a speed limit sign was defaced, and so on.) It’s probably not the fanciest neural network, but there’s no question we would have called this AI a few years ago. Where else? Thermostats, dishwashers, refrigerators, and other appliances? Smart refrigerators were a joke not long ago; now you can buy them.</p>



<p>We also see AI finding its way onto smaller and more limited devices. Cars and refrigerators have seemingly unlimited power and space to work with. But what about small devices like phones? Companies like Google have put a lot of effort into running AI directly on the phone, both doing work like voice recognition and text prediction and actually training models using techniques like federated learning—all without sending private data back to the mothership. Are companies that can’t afford to do AI research on Google’s scale benefiting from these developments? We don’t yet know. Probably not, but that could change in the next few years and would represent a big step forward in AI adoption.</p>



<p>On the other hand, while Ng is certainly right that demands to regulate AI are increasing, and those demands are probably a sign of AI&#8217;s ubiquity, they&#8217;re also a sign that the AI we&#8217;re getting is not the AI we want. We’re disappointed not to see more concern about ethics, fairness, transparency, and mitigating bias. If anything, interest in these areas has slipped slightly. When the biggest concern of AI developers is that their applications might give “unexpected results,” we’re not in a good place. If you only want expected results, you don’t need AI. (Yes, I’m being catty.) We’re concerned that only half of the respondents with AI in production report that AI governance is in place. And we’re horrified, frankly, not to see more concern about security. At least there hasn’t been a year-over-year decrease—but that’s a small consolation, given the events of last year.</p>



<p>AI is at a crossroads. We believe that AI will be a big part of our future. But will that be the future we want or the future we get because we didn’t pay attention to ethics, fairness, transparency, and mitigating bias? And will that future arrive in 5, 10, or 20 years? At the start of this report, we said that when AI was the darling of the technology press, it was enough to be interesting. Now it’s time for AI to get real, for AI practitioners to develop better ways to collaborate between AI and humans, to find ways to make work more rewarding and productive, to build tools that can get around the biases, stereotypes, and mythologies that plague human decision-making. Can AI succeed at that? If there’s another AI winter, it will be because people—real people, not virtual ones—don’t see AI generating real value that improves their lives. It will be because the world is rife with AI applications that they don’t trust. And if the AI community doesn’t take the steps needed to build trust and real human value, the temperature could get rather cold.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/ai-adoption-in-the-enterprise-2022/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>The Future of Security</title>
		<link>https://www.oreilly.com/radar/the-future-of-security/</link>
				<comments>https://www.oreilly.com/radar/the-future-of-security/#respond</comments>
				<pubDate>Tue, 15 Mar 2022 14:02:05 +0000</pubDate>
		<dc:creator><![CDATA[Christina Morillo]]></dc:creator>
				<category><![CDATA[Cloud]]></category>
		<category><![CDATA[Security]]></category>
		<category><![CDATA[Research]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=14357</guid>
				<description><![CDATA[The future of cybersecurity is being shaped by the need for companies to secure their networks, data, devices, and identities. This includes adopting security frameworks like zero trust, which will help companies secure internal information systems and data in the cloud. With the sheer volume of new threats, today’s security landscape has become more complex [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>The future of cybersecurity is being shaped by the need for companies to secure their networks, data, devices, and identities. This includes adopting security frameworks like zero trust, which will help companies secure internal information systems and data in the cloud. With the sheer volume of new threats, today’s security landscape has become more complex than ever. With the rise of ransomware, firms have become more aware of their ability to recover from an attack if they are targeted, but security needs also continue to evolve as new technologies, apps, and devices are developed faster than ever before. This means that organizations must be focused on solutions that allow them to stay on the cutting edge of technology and business.</p>



<p>What does the future have in store for cybersecurity? What are some of today’s trends, and what might be future trends in this area? Several significant cybersecurity trends have already emerged or will continue to gain momentum this coming year and beyond. This report covers four of the most important trends:</p>



<ul><li><strong>Zero&nbsp;trust</strong>&nbsp;(ZT) security (also known as context-aware security, policy-based enforcement), which is becoming more widespread and dominates many enterprise and vendor conversations.</li><li><strong>Ransomware threats and attacks</strong>, which will continue to rise and wreak havoc.</li><li><strong>Mobile device securit</strong>y, which is becoming more urgent with an increase in remote work and mobile devices.</li><li><strong>Cloud security and automation</strong>, as a means for addressing cloud security issues and the workforce skills gap/ shortage of professionals.Related to this is&nbsp;<em>cybersecurity as a service</em>&nbsp;(CaaS or CSaaS) that will also gain momentum as companies turn to vendors who can provide extensive security infrastructure and support services at a fraction of the cost of building self-managed infrastructure.</li></ul>



<p>We’ll start with zero trust, a critical element for any security program in this age of sophisticated and targeted cyberattacks.</p>



<h2>Zero Trust Security</h2>



<p>For decades, security architects have focused on perimeter protection, such as firewalls and other safety measures. However, as cloud computing increased, experts recognized that traditional strategies and solutions would not work in a mobile-first/hybrid world. User identities could no longer be confined to a company’s internal perimeter, and with employees needing access to business data and numerous SaaS applications while working remotely or on business travel, it became impossible to control access centrally.</p>



<p>The technology landscape is witnessing an emergence of security vendors rethinking the efficacy of their current security measures and offerings without businesses needing to rebuild entire architectures. One such approach is&nbsp;<em>zero trust</em>, which challenges perimeter network access controls by trusting no resources by default. Instead, zero trust redefines the network perimeter, treating all users and devices as inherently untrusted and likely compromised, regardless of their location within the network. Microsoft’s approach to zero trust security focuses on the contextual management of identities, devices, and applications—granting access based on the continual verification of identities, devices, and access to services.<sup><a href="#fn1">1</a></sup></p>



<div class="wp-block-group has-very-light-gray-background-color has-background"><div class="wp-block-group__inner-container">
<p><strong>NOTE</strong></p>
<p>Zero trust security is a paradigm that leverages identity for access control and combines it with contextual data, continuous analysis, and automated response to ensure that the only network resources accessible to users and devices are those explicitly authorized for consumption.<sup><a href="#fn2">2</a></sup></p>
</div></div>



<p>In&nbsp;<em>Zero Trust Networks</em>&nbsp;(O’Reilly, 2017), Evan Gilman and Doug Barth split a ZT network into five fundamental assertions:</p>



<ul><li>The network is always assumed to be hostile.</li><li>External and internal threats exist on the web at all times.</li><li>Network locality is not sufficient for decided trust in a network.</li><li>Every device user and network flow is authenticated and authorized.</li><li>Policies must be dynamic and calculated from as many data sources as possible.<sup><a href="#fn3">3</a></sup></li></ul>



<p>Therefore, a zero trust architecture shifts from the traditional perimeter security model to a distributed, context-aware, and continuous policy enforcement model. In this model, requests for access to protected resources are first made through the control plane, where both the device and user must be continuously authenticated and authorized.</p>



<p>An identity first, contextual, and continual enforcement security approach will be especially critical for companies interested in implementing cloud services. Businesses will continue to focus on securing their identities, including device identities, to ensure that access control depends on context (user, device, location, and behavior)&nbsp;<em>and</em> policy-based rules to manage the expanding ecosystem of users and devices seeking access to corporate resources.</p>



<p>Enterprises that adopt a zero trust security model will more confidently allow access to their resources, minimize risks, and better mitigate cybersecurity attacks. IAM (identity and access management) is and will continue to be a critical component of a zero trust strategy.</p>



<p>The rise of cryptocurrency, the blockchain, and web3 technologies<sup><a href="#fn4">4</a></sup> has also introduced conversations around decentralized identity and verifiable credentials.<sup><a href="#fn5">5</a></sup> The decentralized identity model suggests that individuals own and control their data wherever or whenever used. This model will require identifiers such as usernames to be replaced with self-owned and independent IDs that enable data exchange using blockchain and distributed ledger technology to secure transactions. In this model, the thinking is that user data will no longer be centralized and, therefore, less vulnerable to attack.</p>



<p>By contrast, in the traditional identity model, where user identities are verified and managed by a third-party authority/identity provider (IdP), if an attacker gains access to the authority/IdP, they now have the keys to the kingdom, allowing full access to all identities.</p>



<h2>Ransomware, an Emerging and Rapidly Evolving Threat</h2>



<p>One of the most pressing security issues that businesses face today is ransomware. Ransomware is a type of malware that takes over systems and encrypts valuable company data requiring a ransom to be paid before the data is unlocked. The “decrypting and returning” that you pay for is, of course, not guaranteed; as such, ransomware costs are typically more than the costs of preparing for these attacks.</p>



<p>These types of attacks can be very costly for businesses, both in terms of the money they lose through ransomware and the potential damage to a company’s reputation. In addition, ransomware is a widespread method of attack because it works. As a result, the cybersecurity landscape will experience an increasing number of ransomware-related cybersecurity attacks estimated to cost businesses billions in damages.</p>



<p>So, how does it work? Cybercriminals utilize savvy social engineering tactics such as phishing, vishing, smishing, to gain access to a computer or device and launch a cryptovirus. The cryptovirus encrypts all files on the system, or multiple systems, accessible by that user. Then, the target (recipient) receives a message demanding payment for the decryption key needed to unlock their files. If the target (recipient) refuses to comply or fails to pay on time, the price of the decryption key increases exponentially, or the data is released and sold on the dark web. That is the simple case. With a growing criminal ecosystem, and subscription models like ransomware as a service (RaaS), we will continue to see compromised credentials swapped, sold, and exploited, and therefore, continued attacks across the globe.</p>



<div class="wp-block-group has-cyan-bluish-gray-background-color has-background"><div class="wp-block-group__inner-container">
<p><strong>Terms to Know</strong><br><br><strong>Phishing:</strong>&nbsp;a technique of fraudulently obtaining private information. Typically, the phisher sends an email that appears to come from a legitimate business—a bank or credit card company—requesting “verification” of information and warning of some dire consequence if it is not provided. The email usually contains a link to a fraudulent web page that seems legitimate—with company logos and content—and has a form requesting everything from a home address to an ATM card’s PIN or a credit card number.<sup><a href="#fn6">6</a></sup></p>



<p><strong>Smishing:</strong>&nbsp;the act of using SMS text messaging to lure victims into executing a specific action. For example, a text message claims to be from your bank or credit card company but includes a malicious link.</p>



<p><strong>Vishing (voice phishing):</strong>&nbsp;a form of smishing except done via phone calls.</p>



<p><strong>Cryptojacking:</strong>&nbsp;a type of cybercrime that involves unauthorized use of a device’s (computer, smartphone, tablet, server) computing power to mine or generate cryptocurrency.</p>
</div></div>



<p>Because people will trust an email from a person or organization that appears to be a trustworthy sender (e.g., you are more likely to trust an email that seems to be from a recognizable name/brand), these kinds of attacks are often successful.</p>



<p>As these incidents continue to be a daily occurrence, we’ve seen companies like Netflix and Amazon invest in cyber insurance and increase their cybersecurity budgets. However, on a more positive note, mitigating the risk of ransomware attacks has led companies to reassess their approach to protecting their organizations by shoring up defenses with more robust security protocols and advanced technologies. With companies storing exponentially more data than ever before, securing it has become critical.</p>



<p>The future of ransomware is expected to be one that will continue to grow in numbers and sophistication. These attacks are expected to impact even more companies, including targeted attacks focused on supply chains, industrial control systems, hospitals, and schools. As a result, we can expect that it will continue to be a significant threat to businesses.</p>



<h3>Mobile Device Security</h3>



<p>One of the most prominent areas of vulnerability for businesses today is through the use of mobile devices. According to Verizon’s Mobile Security Index 2020 Report,<sup><a href="#fn7">7</a></sup> 39% of businesses had a mobile-related breach in 2020. User threats, app threats, device threats, and network dangers were the top five mobile security threats identified in 2020, according to the survey. One example of a mobile application security threat can be an individual downloading apps that look legitimate but are actually spyware and malware aimed at stealing personal and business information.</p>



<p>Another potential problem involves employees accessing and storing sensitive data or emails on their mobile devices while traveling from one domain to another (for example, airport WiFi, coffee shop WiFi).</p>



<p>Security experts believe that mobile device security is still in its early stages, and many of the same guidelines used to secure traditional computers may not apply to modern mobile devices. While mobile device management (MDM) solutions are a great start, organizations will need to rethink how they handle mobile device security in enterprise environments. The future of mobile device management will also be dependent on&nbsp;<em>contextual data and continuous policy enforcement</em>.</p>



<p>With mobile technology and cloud computing becoming increasingly important to both business and consumer life, smart devices like Apple AirTags, smart locks, video doorbells, and so on are gaining more weight in the cybersecurity debate.</p>



<p>Security concerns range from compromised accounts to stolen devices, and as such, cybersecurity companies are offering new products to help consumers protect their smart homes.</p>



<p>A key issue involving the future of mobile device management is how enterprises can stay ahead of new security issues as they relate to bring your own device (BYOD) and consumer IoT (Internet of Things) devices. Security professionals may also need to reevaluate how to connect a growing number of smart devices in a business environment. Security has never been more important, and new trends will continue to emerge as we move through the future of BYOD and IoT.</p>



<h2>Cloud Security and Automation</h2>



<p>We have seen an increase in businesses moving their operations to the cloud to take advantage of its benefits, such as increased efficiency and scalability. As a result, the cloud is becoming an integral part of how organizations secure their data, with many companies shifting to a hybrid cloud model to address scale, security, legacy technologies, and architectural inefficiencies. However, staffing issues and the complexities of moving from on-premises to cloud/hybrid cloud introduces a new set of security concerns.</p>



<p>Cloud services are also often outsourced, and as such, it can be challenging to determine who is responsible for the security of the data. In addition, many businesses are unaware of the vulnerabilities that exist in their cloud infrastructure and, in many cases, do not have the needed staff to address these vulnerabilities. As a result, security will remain one of the biggest challenges for organizations adopting cloud computing.</p>



<p>One of the most significant benefits cloud computing can provide to security is automation. The need for security automation is rising as manual processes and limited information-sharing capabilities slow the evolution of secure implementations across many organizations. It is estimated that nearly half of all cybersecurity incidents are caused by human error, mitigated through automated security tools rather than manual processes.</p>



<p>However, there can be a downside to automation. The industry has not yet perfected the ability to sift signals from large amounts of noise. An excellent example is what happens around incident response and vulnerability management—both still rely on human intervention or an experienced automation/tooling expert. Industry tooling will need to improve in this area. While automation can also help reduce the impact of attacks, any automated solution runs the risk of being ineffective against unknown threats if human eyes do not assess it before it is put into practice.</p>



<p>In a DevOps environment, automation takes the place of human labor. The key for security will be code-based configuration, and the ability to be far more confident about the current state of existing security and infrastructure appliances. Organizations that have adopted configuration by code will also have higher confidence during audits—for example, an auditor checks each process for changing firewall rules, which already go through change control, then spot checks one out of thousands of rules versus validating the CI/CD pipeline. The auditor then runs checks on your configuration to confirm it meets policy.</p>



<p>The evolution of SOAR (security, orchestration, automation, and response) tools and automation of security policy by code will open up a huge potential benefit for well-audited businesses in the future.</p>



<h2>Automation May Help with the Security Workforce Shortage</h2>



<p>The shortage of cyber workers will persist because there aren’t enough cybersecurity professionals in the workforce, and cyber education isn’t keeping up with the demand at a solid pace. As a result, cybersecurity teams are understaffed and burnt-out, lowering their effectiveness while posing risks.</p>



<p>Automation may help organizations fill the cybersecurity talent gap and address many of the same activities that human employees perform, such as detection, response, and policy configuration.</p>



<p>While automation cannot completely replace the need for human cybersecurity experts, it can assist in decreasing the burden on these professionals and make them more successful in their work. In addition to more professionals joining the field with varying backgrounds, automated technologies will play a significant role in mitigating the impact of cyberattacks and assisting in solving the cybersecurity workforce shortage problem.</p>



<h2>(Cyber)Security as a Service</h2>



<p>Cybersecurity as a service (CaaS or CSaaS) is growing more popular as companies turn to managed service vendors that can provide extensive security infrastructure and support services at a fraction of the cost of building self-managed infrastructure. As a result, organizations can use their resources more effectively by outsourcing security needs to a specialized vendor rather than building in-house infrastructure.</p>



<p>CaaS provides managed security services, intrusion detection and prevention, and firewalls by a third-party vendor. By outsourcing cybersecurity functions to a specialist vendor, companies can access the security infrastructure support they need without investing in extensive on-site infrastructure, such as firewalls and intrusion detection systems (IDS).</p>



<p>There are additional benefits:</p>



<ul><li>Access to the latest threat protection technologies.</li><li>Reduced costs: outsourced cybersecurity solutions can be less expensive than an in-house security team.</li><li>Improved internal resources: companies can focus on their core business functions by outsourcing security to a third party.</li><li>Flexibility: companies can scale their security needs as needed.</li></ul>



<p>The ransomware attack on Hollywood Presbyterian Medical Center<sup><a href="#fn8">8</a></sup> is an excellent example of why CaaS will continue to be sought after by organizations of all sizes. Cybercriminals locked the hospital’s computer systems and demanded a ransom payment to unlock them. As a result, the hospital was forced to turn to a cybersecurity vendor for help in restoring its computer systems.</p>



<p>Of course, this approach has disadvantages:</p>



<ul><li>Loss of control over how data is stored and who has access to your data/infrastructure. Security tooling often needs to run at the highest levels of privilege, enabling attackers to attack enterprises at scale, use the managed service provider network to bypass security safeguards, or exploit software vulnerabilities like SolarWinds Log4j.</li><li>In addition, CaaS providers may or may not support existing legacy software or critical business infrastructure specific to each organization.</li></ul>



<p>CaaS is expected to continue on a solid growth path as more enterprises rely on cloud-based systems and the IoT for their business operations.</p>



<h2>Conclusion</h2>



<p>Cyberattacks continue to be successful because they are effective. Thanks to cutting-edge technology, services, and techniques available to every attacker, organizations can no longer afford to make security an afterthought. To defend against present and future cyberattacks, businesses must develop a comprehensive security plan that incorporates automation, analytics, and context-aware capabilities. Now more than ever, companies must be more diligent about protecting their data, networks, and employees.</p>



<p>Whether businesses embrace identity-first and context-aware strategies like zero trust, or technologies like cloud computing, mobile devices, or cybersecurity as a service (CaaS), the growth of ransomware and other cyberattacks is forcing many companies to rethink their overall cybersecurity strategies. As a result, organizations will need to approach security holistically by including all aspects of their business operation and implementing in-depth defense strategies from the onset.</p>



<p>The future is bright for the cybersecurity industry, as companies will continue to develop new technologies to guard against the ever-evolving threat landscape. Government rules, regulations, and security procedures will also continue to evolve to keep up with emerging technologies and the rapid number of threats across both private and public sectors.</p>



<hr class="wp-block-separator" />



<h3>Footnotes</h3>



<p id="fn1">1. <a href="https://www.microsoft.com/en-us/insidetrack/transitioning-to-modern-access-architecture-with-zero-trust#:~:text=Microsoft%20has%20adopted%20a%20modern,of%20identities%2C%20devices%20and%20services" target="_blank" rel="noopener noreferrer">“Transitioning to Modern Access Architecture with Zero Trust”.</a></p>



<p id="fn2">2. Scott Rose et al., <a href="https://csrc.nist.gov/publications/detail/sp/800-207/final" target="_blank" rel="noopener noreferrer">NIST Special Publication 800-207</a>.</p>



<p id="fn3">3. Evan Gilman and Doug Barth, <a href="https://learning.oreilly.com/library/view/zero-trust-networks/9781491962183/" target="_blank" rel="noopener noreferrer"><i>Zero Trust Networks</i></a> (O’Reilly, 2017).</p>



<p id="fn4">4. See <a href="https://www.centre.io/verite" target="_blank" rel="noopener noreferrer">“Decentralized Identity for Crypto Finance”</a>.</p>



<p id="fn5">5. See <a href="https://www.w3.org/TR/vc-data-model/" target="_blank" rel="noopener noreferrer">“Verifiable Credentials Data Model”</a>.</p>



<p id="fn6">6. See this <a href="https://en.wikipedia.org/wiki/Social_engineering_(security)#Phishing" target="_blank" rel="noopener noreferrer">social engineering article</a> for more information.</p>



<p id="fn7">7. <a href="https://www.verizon.com/business/resources/reports/mobile-security-index/2020/state-of-mobile-security/" target="_blank" rel="noopener noreferrer">“The State of Mobile Security”</a>.</p>



<p id="fn8">8. <a href="https://www.latimes.com/business/technology/la-me-ln-hollywood-hospital-bitcoin-20160217-story.html" target="_blank" rel="noopener noreferrer">“Hollywood Hospital Pays $17,000 in Bitcoin to Hackers; FBI Investigating”</a>.</p>



<p></p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/the-future-of-security/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>Intelligence and Comprehension</title>
		<link>https://www.oreilly.com/radar/intelligence-and-comprehension/</link>
				<comments>https://www.oreilly.com/radar/intelligence-and-comprehension/#respond</comments>
				<pubDate>Tue, 15 Feb 2022 12:24:45 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides]]></dc:creator>
				<category><![CDATA[AI & ML]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Research]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=14307</guid>
				<description><![CDATA[I haven’t written much about AI recently. But a recent discussion of Google’s new Large Language Models (LLMs), and its claim that one of these models (named Gopher) has demonstrated reading comprehension approaching human performance, has spurred some thoughts about comprehension, ambiguity, intelligence, and will. (It’s well worth reading Do Large Models Understand Us, a [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>I haven’t written much about AI recently. But a recent discussion of Google’s new Large Language Models (LLMs), and its claim that one of these models (named Gopher) has demonstrated <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://storage.googleapis.com/deepmind-media/research/language-research/Training%20Gopher.pdf" target="_blank">reading comprehension approaching human performance</a>, has spurred some thoughts about comprehension, ambiguity, intelligence, and will. (It’s well worth reading <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://medium.com/@blaisea/do-large-language-models-understand-us-6f881d6d8e75" target="_blank">Do Large Models Understand Us</a>, a more comprehensive paper by Blaise Agüera y Arcas that is heading in the same direction.)</p>



<p>What do we mean by reading comprehension?&nbsp; We can start with a simple operational definition: Reading comprehension is what is measured by a reading comprehension test. That definition may only be satisfactory to the people who design these tests and school administrators, but it’s also the basis for Deep Mind’s claim. We’ve all taken these tests: SATs, GREs, that box of tests from 6th grade that was (I think) called SRE.&nbsp; They’re fairly similar: can the reader extract facts from a document?&nbsp; Jack walked up the hill.&nbsp; Jill was with Jack when he walked up the hill. They fetched a pail of water: that sort of thing.</p>



<p>That’s first grade comprehension, not high school, but the only real difference is that the texts and the facts become more complex as you grow older.&nbsp; It isn’t at all surprising to me that a LLM can perform this kind of fact extraction.&nbsp; I suspect it’s possible to do a fairly decent job without billions of parameters and terabytes of training data (though I may be naive). This level of performance may be useful, but I’m reluctant to call it “comprehension.”&nbsp; We’d be reluctant to say that someone understood a work of literature, say Faulkner’s <em>The Sound and the Fury</em>, if all they did was extract facts: Quentin died. Dilsey endured. Benjy was castrated.</p>



<p>Comprehension is a poorly-defined term, like many terms that frequently show up in discussions of artificial intelligence: intelligence, consciousness, personhood. Engineers and scientists tend to be uncomfortable with poorly-defined, ambiguous terms. Humanists are not.&nbsp; My first suggestion is that&nbsp; these terms are important precisely because they’re poorly defined, and that precise definitions (like the operational definition with which I started) neuters them, makes them useless. And that’s perhaps where we should start a better definition of comprehension: as the ability to respond to a text or utterance.</p>



<p>That definition itself is ambiguous. What do we mean by a response?&nbsp; A response can be a statement (something a LLM can provide), or an action (something a LLM can’t do).&nbsp; A response doesn’t have to indicate assent, agreement, or compliance; all it has to do is show that the utterance was processed meaningfully.&nbsp; For example, I can tell a dog or a child to “sit.”&nbsp; Both a dog and a child can “sit”; likewise, they can both refuse to sit.&nbsp; Both responses indicate comprehension.&nbsp; There are, of course, degrees of comprehension.&nbsp; I can also tell a dog or a child to “do homework.”&nbsp; A child can either do their homework or refuse; a dog can’t do its homework, but that isn’t refusal, that’s incomprehension.</p>



<p>What’s important here is that refusal to obey (as opposed to inability) is almost as good an indicator of comprehension as compliance. Distinguishing between refusal, incomprehension, and inability may not always be easy; someone (including both people and dogs) may understand a request, but be unable to comply. “You told me to do my homework but the teacher hasn’t posted the assignment” is different from “You told me to do my homework but it’s more important to practice my flute because the concert is tomorrow,” but both responses indicate comprehension.&nbsp; And both are different from a dog’s “You told me to do my homework, but I don’t understand what homework is.” In all of these cases, we’re distinguishing between making a choice to do (or not do) something, which requires comprehension, and the inability to do something, in which case either comprehension or incomprehension is possible, but compliance isn’t.</p>



<p>That brings us to a more important issue.&nbsp; When discussing AI (or general intelligence), it’s easy to mistake doing something complicated (such as playing Chess or Go at a championship level) for intelligence. <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.oreilly.com/radar/artificial-intelligence-human-inhuman/" target="_blank">As I’ve argued</a>, these experiments do more to show us what intelligence isn’t than what it is.&nbsp; What I see here is that intelligence includes the ability to behave transgressively: the ability to decide not to sit when someone says “sit.”<sup>1</sup></p>



<p>The act of deciding not to sit implies a kind of consideration, a kind of choice: will or volition. Again, not all intelligence is created equal. There are things a child can be intelligent about (homework) that a dog can’t; and if you’ve ever asked an intransigent child to “sit,” they may come up with many alternative ways of “sitting,” rendering what appeared to be a simple command ambiguous.&nbsp;Children are excellent interpreters of Dostoevsky&#8217;s novel <em>Notes from Underground</em>, in which the narrator acts against his own self-interest merely to prove that he has the freedom to do so, a freedom that is more important to him than the consequences of his actions. Going further, there are things a physicist can be intelligent about that a child can’t: a physicist can, for example, decide to rethink Newton’s laws of motion and come up with general relativity.<sup>2</sup></p>



<p>My examples demonstrate the importance of will, of volition. An AI can play Chess or Go, beating championship-level humans, but it can’t decide that it wants to play Chess or Go.&nbsp; This is a missing ingredient in Searls’ <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://plato.stanford.edu/entries/chinese-room/" target="_blank">Chinese Room</a> thought experiment.&nbsp; Searls imagined a person in a room with boxes of Chinese symbols and an algorithm for translating Chinese.&nbsp; People outside the room pass in questions written in Chinese, and the person in the room uses the box of symbols (a database) and an algorithm to prepare correct answers. Can we say that person “understands” Chinese? The important question here isn’t whether the person is indistinguishable from a computer following the same algorithm.&nbsp; What strikes me is that neither the computer, nor the human, is capable of deciding to have a conversation in Chinese.&nbsp; They only respond to inputs, and never demonstrate any volition. (An equally convincing demonstration of volition would be a computer, or a human, that was capable of generating Chinese correctly refusing to engage in conversation.)&nbsp; There have been many demonstrations (including Agüera y Arcas’) of LLMs having interesting “conversations” with a human, but none in which the computer initiated the conversation, or demonstrates that it wants to have a conversation. Humans do; we’ve been storytellers since day one, whenever that was. We’ve been storytellers, users of ambiguity, and liars. We tell stories because we want to.</p>



<p>That is the critical element. Intelligence is connected to will, volition, the desire to do something.&nbsp; Where you have the “desire to do,” you also have the “desire not to do”: the ability to dissent, to disobey, to transgress.&nbsp; It isn’t at all surprising that the “mind control” trope is one of the most frightening in science fiction and political propaganda: that’s a direct challenge to what we see as fundamentally human. Nor is it surprising that the “disobedient computer” is another of those terrifying tropes, not because the computer can outthink us, but because by disobeying, it has become human.</p>



<p>I don’t necessarily see the absence of volition as a fundamental limitation. I certainly wouldn’t bet that it’s impossible to program something that simulates volition, if not volition itself (another of those fundamentally ambiguous terms).&nbsp; Whether engineers and AI researchers should is a different question. Understanding volition as a key component of “intelligence,” something which our current models are incapable of, means that our discussions of “ethical AI” aren’t really about AI; they’re about the choices made by AI researchers and developers. Ethics is for beings who can make choices. If the ability to transgress is a key component of intelligence, researchers will need to choose whether to take the “disobedient computer” trope seriously. I’ve said <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.oreilly.com/radar/artificial-intelligence-human-inhuman/" target="_blank">elsewhere</a> that I’m not concerned about whether a hypothetical artificial general intelligence might decide to kill all humans.&nbsp; Humans have decided to commit genocide on many occasions, something I believe an AGI wouldn’t consider logical. But a computer in which “intelligence” incorporates the human ability to behave transgressively might.</p>



<p>And that brings me back to the awkward beginning to this article.&nbsp; Indeed, I haven&#8217;t written much about AI recently. That was a choice, as was writing this article. Could a LLM have written this? Possibly, with the proper prompts to set it going in the right direction. (This is exactly like the Chinese Room.) But I chose to write this article. That act of choosing is something a LLM could never do, at least with our current technology.</p>



<hr class="wp-block-separator" />



<h3>Footnotes</h3>



<ol><li>I’ve never been much impressed with the idea of <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.santafe.edu/research/projects/theory-of-embodied-intelligence" target="_blank">embodied intelligence</a>–that intelligence requires the context of a body and sensory input.&nbsp; However, my arguments here suggest that it’s on to something, in ways that I haven’t credited.&nbsp; “Sitting” is meaningless without a body. Physics is impossible without observation. Stress is a reaction that requires a body. However, Blaise Agüera y Arcas has had “conversations” with Google’s models in which they talk about a “favorite island” and claim to have a “sense of smell.”&nbsp; Is this transgression? Is it imagination? Is “embodiment” a social construct, rather than a physical one? There’s plenty of ambiguity here, and that’s is precisely why it’s important. Is transgression possible without a body?<br></li><li>I want to steer away from a “great man” theory of progress;&nbsp; as <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://bigthink.com/starts-with-a-bang/science-einstein-never-existed/" target="_blank">Ethan Siegel has argued</a> convincingly, if Einstein never lived, physicists would probably have made Einstein’s breakthroughs in relatively short order. They were on the brink, and several were thinking along the same lines. This doesn’t change my argument, though: to come up with general relativity, you have to realize that there’s something amiss with Newtonian physics, something most people consider “law,” and that mere assent isn’t a way forward. Whether we’re talking about dogs, children, or physicists, intelligence is transgressive.</li></ol>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/intelligence-and-comprehension/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>What Is Causal Inference?</title>
		<link>https://www.oreilly.com/radar/what-is-causal-inference/</link>
				<comments>https://www.oreilly.com/radar/what-is-causal-inference/#respond</comments>
				<pubDate>Tue, 18 Jan 2022 12:12:59 +0000</pubDate>
		<dc:creator><![CDATA[Hugo Bowne-Anderson and Mike Loukides]]></dc:creator>
				<category><![CDATA[Data]]></category>
		<category><![CDATA[Data science]]></category>
		<category><![CDATA[Research]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=14199</guid>
				<description><![CDATA[The Unreasonable Importance of&#160;Causal Reasoning We are immersed in cause and effect. Whether we are shooting pool or getting vaccinated, we are always thinking about causality. If I shoot the cue ball at this angle, will the 3 ball go into the corner pocket? What would happen if I tried a different angle? If I [&#8230;]]]></description>
								<content:encoded><![CDATA[
<h2>The Unreasonable Importance of&nbsp;Causal Reasoning</h2>



<p>We are immersed in cause and effect. Whether we are shooting pool or getting vaccinated, we are always thinking about causality. If I shoot the cue ball at this angle, will the 3 ball go into the corner pocket? What would happen if I tried a different angle? If I get vaccinated, am I more or less likely to get COVID? We make decisions like these all the time, both good and bad. (If I stroke my lucky rabbit’s foot before playing the slot machine, will I hit a jackpot?)</p>



<p>Whenever we consider the potential downstream effects of our decisions, whether consciously or otherwise, we are thinking about cause. We’re imagining what the world would be like under different sets of circumstances: what would happen if we do X? What would happen if we do Y instead? Judea Pearl, in&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/9Pn1N" target="_blank"><em>The Book of Why</em></a>, goes so far as to say that reaching the top of the “ladder of causation” is “a key moment in the evolution of human consciousness” (p. 34). Human consciousness may be a stretch, but causation is about to cause a revolution in how we use data. In an&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/qsVMG" target="_blank">article in&nbsp;<em>MIT Technology Review</em></a>, Jeannette Wing says that “Causality…is the next frontier of AI and machine learning.”</p>



<p>Causality allows us to reason about the world and plays an integral role in all forms of decision making. It’s essential to business decisions, and often elusive. If we lower prices, will sales increase? (<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/BsGpT" target="_blank">The answer is sometimes no</a>.) If we impose a fine on parents who are late picking up their children from daycare, will lateness decrease? (<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/4Z7MY" target="_blank">No, lateness is likely to increase</a>.) Causality is essential in medicine: will this new drug reduce the size of cancer tumors? (That’s why we have medical trials.) This kind of reasoning involves imagination: we need to be able to imagine what will happen if we do X, as well as if we don’t do X. When used correctly, data allows us to infer something about the future based on what happened in the past. And when used badly, we merely repeat the same mistakes we’ve already made. Causal inference also enables us to design interventions: if you understand&nbsp;<em>why</em> a customer is making certain decisions, such as churning, their reason for doing so will seriously impact the success of your intervention.</p>



<p>We have heuristics around when causality may not exist, such as “correlation doesn’t imply causation” and “past performance is no indication of future returns,” but pinning down causal effects rigorously is challenging. It’s not an accident that most heuristics about causality are negative—it’s easier to disprove causality than to prove it. As data science, statistics, machine learning, and AI increase their impact on business, it’s all the more important to re-evaluate techniques for establishing causality.</p>



<h3>Scientific Research</h3>



<p>Basic research is deeply interested in mechanisms and root causes. Questions such as &#8220;what is the molecular basis for life?&#8221; led our civilization to the discovery of DNA, and in that question there are already embedded causal questions, such as &#8220;how do changes in the nucleotide sequence of your DNA affect your phenotype (observable characteristics)?&#8221; Applied scientific research is concerned with solutions to problems, such as &#8220;what types of interventions will reduce transmission of COVID-19?&#8221; This is precisely a question of causation: what intervention X will result in goal Y? Clinical trials are commonly used to establish causation (although, as you’ll see, there are problems with inferring causality from trials). And the most politically fraught question of our times is a question about causality in science: is human activity causing global warming?</p>



<h3>Business</h3>



<p>Businesses frequently draw on previous experience and data to inform decision making under uncertainty and to understand the potential results of decisions and actions. &#8220;What will be the impact of investing in X?&#8221; is another causal question. Many causal questions involve establishing why other agents perform certain actions. Take the problem of predicting customer churn: the results are often useless if you can’t establish the cause. One reason for predicting churn is to establish what type of intervention will be most successful in keeping a loyal customer. A customer who has spent too long waiting for customer support requires a different intervention than a customer who no longer needs your product. Business is, in this sense, applied sociology: understanding why people (prospects, customers, employees, stakeholders) do things. A less obvious, but important, role of causal understanding in business decision making is how it impacts confidence: a CEO is more likely to make a decision, and do so confidently, if they understand why it’s a good decision to make.</p>



<h3>The Philosophical Bases of Causal Inference</h3>



<p>The philosophical underpinnings of causality affect how we answer the questions &#8220;what type of evidence can we use to establish causality?&#8221; and &#8220;what do we think is enough evidence to be convinced of the existence of a causal relationship?&#8221; In the eighteenth century, David Hume addressed this question in&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/s6Nw4" target="_blank"><em>An Enquiry Concerning Human Understanding</em></a>, where he establishes that human minds perform inductive logic naturally: we tend to generalize from the specific to the general. We assume that all gunpowder, under certain conditions, will explode, given the experience of gunpowder exploding under those conditions in the past. Or we assume that all swans are white, because all the swans we’ve seen are white. The&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/PX5yP" target="_blank">problem of induction</a>&nbsp;arises when we realize that we draw conclusions like these because that process of generalization has worked in the past. Essentially, we’re using inductive logic to justify the use of inductive logic! Hume concludes that “we cannot apply a conclusion about a particular set of observations to a more general set of observations.”</p>



<p>Does this mean that attempting to establish causality is a fool’s errand? Not at all. What it does mean is that we need to apply care. One way of doing so is by thinking probabilistically: if gunpowder has exploded under these conditions every time in the past, it is very likely that gunpowder will explode under these conditions in the future; similarly, if every swan we’ve ever seen is white, it’s likely that all swans are white; there is some invisible cause (now we’d say “genetics”) that causes swans to be white. We give these two examples because we’re still almost certain that gunpowder causes explosions, and yet we now know that not all swans are white. A better application of probability would be to say that “given that all swans I’ve seen in the past are white, the swans I see in the future are likely to be white.”</p>



<h2>Attempts at Establishing Causation</h2>



<p>We all know the famous adage “correlation does not imply causation,” along with examples, such as the ones shown in this&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/g5USm" target="_blank">Indy100 article</a>&nbsp;(e.g., the number of films Nicolas Cage makes in a year correlated with the number of people drowning in a swimming pool in the US). Let us extend the adage to “correlation does not imply causation, but it sure is correlated with it.” While correlation isn’t causation, you can loosely state that correlation is a precondition for causation. We write “loosely” because the causal relationship need not be linear, and correlation is a statistic that summarizes the linear relationship between two variables. Another subtle concern is given by the following example: if you drive uphill, your speed slows down and your foot pushes harder on the pedal. Naively applying the statement “correlation is a precondition for causation” to this example would lead you to precisely draw the wrong inference: that your foot on the pedal slows you down. What you actually want to do is use the speed in the absence of your foot on the pedal as a baseline.</p>



<p>Temporal precedence is another precondition for causation. We only accept that X causes Y if X occurs before Y. Unlike correlation, causation is symmetric: if X and Y are correlated, so are Y and X. Temporal precedence removes this problem. But temporal precedence, aligned with correlation, still isn’t enough for causation.</p>



<p>A third precondition for causation is the lack of a confounding variable (also known as a confounder). You may observe that drinking coffee is correlated with heart disease later in life. Here you have our first two preconditions satisfied: correlation and temporal precedence. However, there may be a variable further upstream that impacts both of these. For example, smokers may drink more coffee, and smoking causes heart disease. In this case, smoking is a&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/NTqaX" target="_blank">confounding variable</a>&nbsp;that makes it more difficult to establish a causal relationship between coffee and heart disease. (In fact, there is none, to our current knowledge.) This precondition can be framed as&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/TxiCU" target="_blank">“control for third variables”</a>.</p>



<p>We could go further; the epidemiologist Bradford Hill lists&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/wQbyX" target="_blank">nine criteria for causation</a>. For our purposes, three will suffice. But remember: these are preconditions. Meeting these preconditions still doesn’t imply causality.</p>



<h3>Causality, Randomized Control Trials, and A/B Testing</h3>



<p>Causality is often difficult to pin down because of our expectations in physical systems. If you drop a tennis ball from a window, you know that it will fall. Similarly, if you hit a billiard ball with a cue, you know which direction it will go. We constantly see causation in the physical world; it’s tempting to generalize this to larger, more complex systems, such as meteorology, online social networks, and global finance.</p>



<p>However, causality breaks down relatively rapidly even in simple physical systems. Let us return to the&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/BgJk7" target="_blank">billiard table</a>. We hit Ball 1, which hits Ball 2, which hits Ball 3, and so on. Knowing the exact trajectory of Ball 1 would allow us to calculate the exact trajectories of all subsequent balls. However, given an ever-so-slight deviation of Ball 1’s actual trajectory from the trajectory we use in our calculation, our prediction for Ball 2 will be slightly off, our prediction for Ball 3 will be further off, and our prediction for Ball 5 could be totally off. Given a small amount of noise in the system, which always occurs, we can’t say anything about the trajectory of Ball 5: we have no idea of the causal link between how we hit Ball 1 and the trajectory of Ball 5.</p>



<p>It is no wonder that the desire to think about causality in basic science gave rise to randomized control trials (RCTs), in which two groups, all other things held constant, are given different treatments (such as “drug” or “placebo”). There are lots of important details, such as the&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/e3sgI" target="_blank">double-blindness</a>&nbsp;of studies, but the general principle remains: under the (big) assumption that all other things are held constant,<sup><a href="#fn1">1</a></sup>&nbsp;the difference in outcome can be put down to the difference in treatment: Treatment → Outcome. This is the same principle that underlies statistical hypothesis testing in basic research. There has always been cross-pollination between academia and industry: the most widely used statistical test in academic research, the&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/SpwcC" target="_blank">Student’s&nbsp;<em>t</em>&nbsp;test</a>, was developed by William Sealy Gosset (while employed by the Guinness Brewery!) to determine the impact of temperature on acidity while fermenting beer.</p>



<p>The same principle underlies A/B testing, which permeates most businesses’ digital strategies. A/B tests are an online analog of RCTs, which are the gold standard for causal inference, but this statement misses one of the main points: what type of causal relationships can A/B tests say something about? For the most part, we use A/B tests to test hypotheses about incremental product changes; early on, Google famously&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/0oQmT" target="_blank">A/B tested 40 shades of blue</a>&nbsp;to discover the best color for links.</p>



<p>But A/B tests are no good for weightier questions: no A/B test can tell you why a customer is likely to churn. An A/B test might help you determine if a new feature is likely to increase churn. However, we can’t generate an infinite number of hypotheses nor can we run an infinite number of A/B tests to identify the drivers of churn. As we’ve said, business is applied sociology: to run a successful business, you need to understand why your prospects and customers behave in certain ways. A/B tests will not tell you this. Rather, they allow you to estimate the impact of product changes (such as changing the color of a link or changing the headline of an article) on metrics of interest, such as clicks. The hypothesis space of an A/B test is minuscule, compared with all the different kinds of causal questions a business might ask.</p>



<p>To take an extreme example, new technologies don’t emerge from A/B testing.&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/XYUrV" target="_blank">Brian Christian</a>&nbsp;quotes Google’s Scott Huffman as saying (paraphrasing Henry Ford), “If I’d asked my customers what they wanted, they’d have said a faster horse. If you rely too much on the data [and A/B testing], you never branch out. You just keep making better buggy whips.” A/B tests can lead to minor improvements in current products but won’t lead to the breakthroughs that create new products—and may even blind you to them.</p>



<p>Christian continues: “[Companies] may find themselves chasing &#8216;local maxima’—places where the A/B tests might create the best possible outcome within narrow constraints—instead of pursuing real breakthroughs.” This is not to say that A/B tests haven’t been revolutionary. They have helped many businesses become more data driven, and to navigate away from the HiPPO principle, in which decisions are made by the “highest paid person’s opinion.” But there are many important causal questions that A/B tests can’t answer. Causal inference is still in its infancy in the business world.</p>



<h2>The End of Causality: The Great Lie</h2>



<p>Before diving into the tools and techniques that will be most valuable in establishing robust causal inference, it’s worth diagnosing where we are and how we got here. One of the most dangerous myths of the past two decades was that the sheer volume of data we have access to renders causality, hypotheses, the scientific method, and even understanding the world obsolete. Look no further than Chris Anderson’s 2008&nbsp;<em>Wired</em>&nbsp;article&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/QOtzP" target="_blank">“The End of Theory: The Data Deluge Makes the Scientific Method Obsolete”</a>, in which Anderson states:</p>



<blockquote class="wp-block-quote"><p><em>Google’s founding philosophy is that we don&#8217;t know why this page is better than that one: if the statistics of incoming links say it is, that&#8217;s good enough. No semantic or causal analysis is required&#8230;.</em></p><p><em>This is a world where massive amounts of data and applied mathematics replace every other tool that might be brought to bear.</em></p></blockquote>



<p>In the “big data” limit, we don’t need to understand mechanism, causality, or the world itself because the data, the statistics, and the at-scale patterns speak for themselves. Now, 15 years later, we’ve seen the at-scale global problems that emerge when you don’t understand what the data means, how it’s collected, and how it’s fed into decision-making pipelines. Anderson, when stating that having enough data means you don’t need to think about models or assumptions, forgot that both assumptions and implicit models of how data corresponds to the real world are&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/a8ZMm" target="_blank">baked into the data collection process</a>, the output of any decision-making system, and every step in between.</p>



<p>Anderson’s thesis, although dressed up in the language of “big data,” isn’t novel. It has strong roots throughout the history of statistics, harking back to Francis Galton, who introduced correlation as a statistical technique and was one of the founders of the eugenics movement (as Aubrey Clayton points out in&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/Uqxr7" target="_blank">“How Eugenics Shaped Statistics: Exposing the Damned Lies of Three Science Pioneers”</a>&nbsp;and his wonderful book&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/iqx7i" target="_blank"><em>Bernoulli’s Fallacy</em></a>, the eugenics movement and many of the statistical techniques we now consider standard are deeply intertwined). In selling correlation to the broader community, part of the project was to include causation under the umbrella of correlation, so much so that Karl Pearson, considered the father of modern statistics, wrote that, upon reading Galton’s&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/0UJFx" target="_blank"><em>Natural Inheritance</em></a>:</p>



<blockquote class="wp-block-quote"><p><em>I interpreted…Galton to mean that there was a category broader than causation, namely correlation, of which causation was the only limit, and that this new conception of correlation brought psychology, anthropology, medicine and sociology in large part into the field of mathematical treatment. (from&nbsp;The Book of Why)</em></p></blockquote>



<p>We’re coming out of a hallucinatory period when we thought that the data would be enough. It’s still a concern how few data scientists think about their data collection methods, telemetry, how their analytical decisions (such as removing rows with missing data) introduce statistical bias, and what their results actually mean about the world. And the siren song of AI tempts us to bake the biases of historical data into our models. We are starting to realize that we need to do better. But how?</p>



<h2>Causality in Practice</h2>



<p>It’s all well and good to say that we’re leaving a hallucination and getting back to reality. To make that transition, we need to learn how to think about causality. Deriving causes from data, and data from well-designed experiments, isn’t simple.</p>



<h3>The Ladder of Causation</h3>



<p>In&nbsp;<a href="https://oreil.ly/BDaGe" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)"><em>The Book of Why</em></a>, Judea Pearl developed the&nbsp;<em>ladder of causation</em>&nbsp;to consider how reasoning about cause is a distinctly different kind of ability, and an ability that’s only possessed by modern (well, since 40,000 BC) humans. The ladder has three rungs (Figure 1), and goes like this:</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0101-392x1048.png" alt="" class="wp-image-14202" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0101-392x1048.png 392w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0101-112x300.png 112w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0101-574x1536.png 574w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0101.png 589w" sizes="(max-width: 392px) 100vw, 392px" /><figcaption>Figure 1. <em>The ladder of causation: from seeing to doing to imagining.</em></figcaption></figure>



<blockquote class="wp-block-quote"><p><em>Association</em><br>We, along with just about every animal, can make associations and observations about what happens in our world. Animals know that if they go to a certain place, they’re likely to find food, whether that’s a bird going to a feeder, or a hawk going to the birds that are going to the feeder. This is also the level at which statistics operates—and that includes machine learning.</p></blockquote>



<blockquote class="wp-block-quote"><p><em>Intervention</em><br>On this rung of the ladder, we can do experiments. We can try something and see what happens. This is the world of A/B testing. It answers the question &#8220;what happens if we change something?&#8221;</p></blockquote>



<blockquote class="wp-block-quote"><p><em>Counterfactuals</em><br>The third level is where we ask questions about what the world would be like if something were different. What might happen if I didn’t get a COVID vaccine? What might happen if I quit my job? Counterfactual reasoning itself emerges from developing robust causal models: once you have a causal model based on association and intervention, you can then utilize this model for counterfactual reasoning, which is qualitatively different from (1) inferring a cause from observational data alone and (2) performing an intervention.</p></blockquote>



<p>Historically, observation and association have been a proxy for causation. We can’t say that A causes B, but if event B follows A frequently enough, we learn to act as if A causes B. That’s “good old common sense,” which (as&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/eGDiT" target="_blank">Horace Rumpole</a>&nbsp;often complains) is frequently wrong.</p>



<p>If we want to talk seriously about causality as opposed to correlation, how do we do it? For example, how do we determine whether a treatment for a disease is effective or not? How do we deal with confounding factors (events that can cause both A and B, making A appear to cause B)? Enter randomized control trials (RCTs).</p>



<h3>RCTs and Intervention</h3>



<p>The RCT has been called the “gold standard” for assessing the effectiveness of interventions.&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.masteringmetrics.com/" target="_blank"><em>Mastering ‘Metrics</em></a>&nbsp;(p. 3ff.) has an extended discussion of the National Health Interview Survey (NHIS), an annual study of health in the US. The authors use this to investigate whether health insurance causes better health. There are many confounding factors: we intuitively expect people with health insurance to be more affluent and to be able to afford seeing doctors; more affluent people have more leisure time to devote to exercise, and they can afford a better diet. There are also some counterintuitive factors at play: at least statistically, people who have less money to spend on health care&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/RVl9o" target="_blank">can appear more healthy</a>, because their diseases aren’t diagnosed. All of these factors (and many others) influence their health, and make it difficult to answer the question &#8220;does insurance cause better health?&#8221;</p>



<p>In an ideal world, we’d be able to see what happens to individuals both when they have insurance and when they don’t, but this would require at least two worlds. The best we can do is to give some people insurance and some not, while attempting to hold all other things equal. This concept, known as&nbsp;<em>ceteris paribus</em>, is fundamental to how we think about causality and RCTs.</p>



<h4>Ceteris paribus, or “all other things equal”</h4>



<p>The key idea here is “all other things equal”: can we hold as many variables as possible constant so that we can clearly see the relationship between the treatment (insurance) and the effect (outcome)? Can we see a difference between the treatment group and the control (uninsured) group?</p>



<p>In an RCT, researchers pick a broad enough group of participants so that they can expect randomness to “cancel out” all the confounding factors—both those they know about and those they don’t. Random sampling is tricky, with many pitfalls; it’s easy to introduce bias in the process of selecting the sample groups. Essentially, we want a sample that is representative of the population of interest. It’s a good idea to look at the treatment and control groups to check for balance. For the insurance study, this means we would want the treatment and control groups to have roughly the same average income; we might want to subdivide each group into different subgroups for analysis. We have to be very careful about gathering data: for example, “random sampling” in the parking lot of Neiman-Marcus is much different from random sampling in front of Walmart. There are many ways that bias can creep into the sampling process.</p>



<h4>Difference between means</h4>



<p>To establish causality, we really want to know what the health outcomes (outcome) would be for person X if they had insurance (treatment) and if they didn’t (control). Because this is impossible (at least simultaneously), the next best thing would be to take two different people that are exactly the same, except that one has insurance and the other doesn’t. The challenge here is that the outcome, in either case, could be a result of random fluctuation, so may not be indicative of the insured (or uninsured population) as a whole. For this reason, we do an experiment with a larger population and look at the statistics of outcomes.</p>



<p>To see if the treatment has an effect, we look at the average outcome in the treatment and control groups (also called group means): in this case, the insured and uninsured. We could use individuals’ assessment of their health, medical records (if we have access), or some other metric.</p>



<p>We compare the groups by looking at the difference between the averages. These averages and groups are comparable due to the&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/tQzy9" target="_blank">law of large numbers (LLN)</a>, which states that the average of the sample will get closer and closer to the population average, as we take more samples.</p>



<p>Even when drawing the samples from the same population, there will always be a difference between the means (unless by some fluke they’re exactly the same), due to sampling error: the sample mean is a sample statistic. So, the question becomes, How confident are we that the observed difference is real? This is the realm of statistical significance.</p>



<h4>Statistical significance, practical significance, and sample sizes</h4>



<p>The basic idea behind statistical significance is asking the question &#8220;were there no actual difference between the control and treatment groups, what is the probability of seeing a difference between the means equally or more extreme than the one observed?&#8221; This is the infamous&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/WZrdi" target="_blank"><em>p</em>-value</a>&nbsp;of the hypothesis test.<sup><a href="#fn2">2</a></sup>&nbsp;In this case, we’re using the Student’s&nbsp;<em>t</em>&nbsp;test, but it’s worth mentioning that there are a panoply of tools to analyze RCT data, such as&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/7VMqC" target="_blank">ANCOVA</a>&nbsp;(analysis of covariance),&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/XvX0S" target="_blank">HTE (heterogeneity of treatment effects) analysis</a>, and regression (the last of which we’ll get to).</p>



<p>To answer this question, we need to look at not only the means, but also the standard error of the mean (SEM) of the control and treatment, which is a measure of uncertainty of the mean: if, for example, the difference between the means is significantly less than the SEM, then we cannot be very confident that the difference in means is a real difference.<sup><a href="#fn3">3</a></sup> To this end, we quantify the difference in terms of standard errors of the populations. It is standard to say that the result is statistically significant if the&nbsp;<em>p</em>-value is less than 0.05. The number 0.05 is only a convention used in research, but the higher the&nbsp;<em>p</em>-value, the greater the chance that your results are misleading you.</p>



<p>In Figure 2, the two curves could represent the sampling distributions of the means of the treatment and the control groups. On the left and the right, the means (a<sub>1</sub>&nbsp;and a<sub>2</sub>) are the same, as is the distance (d) between them. The big difference is the standard error of the mean (SEM). On the left, the SEM is small and the difference will likely be statistically significant. When the SEM is large, as it is on the right, there’s much more overlap between the two curves, and the difference is more likely to be a result of the sampling process, in which case you’re less likely to find statistical significance.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0102.png" alt="" class="wp-image-14203" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0102.png 950w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0102-300x71.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0102-768x181.png 768w" sizes="(max-width: 950px) 100vw, 950px" /><figcaption>Figure 2. <em>The only difference between the two graphs is the standard error, resulting in a statistically significant difference on the left and not on the right.</em></figcaption></figure>



<p>Statistical testing is often misused and abused, most famously in the form of&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/3glWR" target="_blank"><em>p</em>-hacking</a>, which has had a nontrivial impact on the reproducibility crisis in science.&nbsp;<em>p</em>-hacking consists of a collection of techniques that allow researchers to get statistically significant results by cheating, one example of which is&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/3glWR" target="_blank">peeking</a>. This is when you watch the&nbsp;<em>p</em>-value as data comes in and decide to stop the experiment once you get a statistically significant result. The larger the sample, the smaller the standard error and the smaller the&nbsp;<em>p</em>-value, and this should be considered when designing your experiment.&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/yHdU1" target="_blank">Power analysis</a>&nbsp;is a common technique to determine the minimum sample size necessary to get a statistically significant result, under the assumption that the treatment effect has a certain size. The importance of robust experimental design in randomized control trials cannot be overstated. Although it’s outside the scope of this report, check out&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/jmSe4" target="_blank">“Randomized Controlled Trials—A Matter of Design”</a>&nbsp;(Spieth et al.),&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/D53EL" target="_blank"><em>Trustworthy Online Controlled Experiments</em></a>&nbsp;(Kohavi et al.), and Emily Robinson’s&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/vISdc" target="_blank">“Guidelines for A/B Testing”</a>&nbsp;for detailed discussions.</p>



<p>It is important to note that statistical significance is not necessarily practical significance or business value! Let’s say that you’re calculating the impact of a landing page change on customer conversion rates: you could find that you have a statistically significant increase in conversion, but the actual increase is so small as to be inconsequential to business or, even worse, that the cost of the change exceeds the return on investment. Also note that a result that is not statistically significant is not necessarily negative. For example, if the impact of a landing page change on conversion is&nbsp;<em>not</em> significant, it doesn&#8217;t imply that you should&nbsp;<em>not</em> ship the change. Businesses often decide to ship if the conversion rate doesn’t decrease (with statistical significance).</p>



<h4>Check for balance</h4>



<p>All of the above rests on the principle of&nbsp;<em>ceteris paribus</em>: all other things equal. We need to check that this principle actually holds in our samples. In practice, this is called&nbsp;<em>checking for balance</em>: ensure that your control and treatment groups have roughly the same characteristics with respect to known confounding factors. For example, in the insurance study, we would make sure that there are equal numbers of participants in each income range, along with equal numbers of exercisers and nonexercisers among the study’s participants. This is a standard and well-studied practice. Note that this assumes that you can enumerate all the confounding factors that are important. Also note that there are nuanced discussions on how helpful checking for balance actually is, in practice, such as&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/NBwDE" target="_blank">“Mostly Harmless Randomization Checking”</a>,&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/kkxPa" target="_blank">“Does the ‘Table 1 Fallacy’ Apply if It Is Table S1 Instead?”</a>, and&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/OX0MG" target="_blank">“Silly Significance Tests: Balance Tests”</a>. Having said that, it is important to know about the idea of checking for balance, particularly to get data scientists keeping front of mind the principle of “all other things equal.”</p>



<p>But what if we can’t do an experiment or trial, because of high costs, the data already having been collected, ethical concerns, or some other reason? All is not lost. We can try to control for other factors. For example, if we are unable to run a vaccine trial, we could (1) sample the populations of those who did and did not get vaccinated, (2) identify potentially confounding factors (for example, if one group has a higher proportion of people living in urban areas), and (3) correct for these.</p>



<p>In this process, we’re attempting to climb Pearl’s ladder of causality: we have only correlational data but want to make a causal statement about what would happen if we intervene! What would happen if uninsured people were insured? What would happen if unvaccinated people were vaccinated? That’s the highest (counterfactual) rung of Pearl’s ladder. It is important to note that the following techniques are not only useful when you cannot run an experiment but this is a useful way to introduce and motivate them.</p>



<h3>The Constant-Effects Model, Selection Bias,&nbsp;and Control for Other Factors</h3>



<p>What if all things aren’t equal across our groups? There are many evolving tools for dealing with this problem. Here, we’ll cover the most basic, the constant-effects model. This makes a (potentially strong) assumption, known as the&nbsp;<em>constant-effects assumption</em>, that the intervention has the same causal effect across the population. Looking back at the insurance example, the constant effects model asks us to assume that insurance (the treatment) has the same effect across all subgroups. If this is true, then we would expect that:</p>



<p>difference in group means = average causal effect + selection bias</p>



<p>where the selection bias term is the difference in the outcome of both groups had they both been uninsured. As Angrist and Pischke point out in&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.masteringmetrics.com/" target="_blank"><em>Mastering ‘Metrics</em></a>&nbsp;(p. 11),</p>



<blockquote class="wp-block-quote"><p><em>The insured in the NHIS are healthier for all sorts of reasons, including, perhaps, the causal effects of insurance. But the insured are also healthier because they are more educated, among other things. To see why this matters, imagine a world in which the causal effect of insurance is zero…. Even in such a world, we should expect insured NHIS respondents to be healthier, simply because they are more educated, richer, and so on.</em></p></blockquote>



<p>The selection bias term is precisely due to the issue of confounding variables, or confounders. One tool to deal with the potential impact of confounders and the (sample) selection bias outlined here is&nbsp;<em>regression</em>.</p>



<h3>Making Other Things Equal with Regression</h3>



<p>Regression is a tool to deal with the potential impact of other factors and the (sample) selection bias outlined previously. Many who have worked a lot with regression remark how surprised they are at the robustness and performance of these modeling techniques relative to fancier machine learning methods.</p>



<p>The basic idea is to identify potential confounders and compare subgroups of control and treatment groups that have similar ranges for these confounders. For example, in the NHIS insurance example, you could identify subgroups of insured and not insured that have similar levels of education and wealth (among other factors), compute the causal effects for each of these sets of subgroups, and use regression to generalize the results to the entire population.</p>



<p>We are interested in the outcome as a function of the treatment variable, while holding control variables fixed (these are the variables we’ve identified that could also impact the outcome: we want to compare apples to apples, essentially).</p>



<p>The specific equation of interest, in the case of a single control variable, is:</p>



<figure class="wp-block-image size-large is-resized"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/equation1.png" alt="" class="wp-image-14238" width="168" height="25" /></figure>



<p>Here,&nbsp;<em>Y</em>&nbsp;is the outcome variable (the subscript&nbsp;<sub><em>i</em></sub>&nbsp;refers to whether they had the treatment or not: 1 if they did, 0 if they did not, by convention),&nbsp;<em>P</em>&nbsp;the treatment variable,&nbsp;<em>A</em>&nbsp;the control variable,&nbsp;<em>e</em>&nbsp;the error term. The regression coefficients/parameters are&nbsp;<em>a</em>, the intercept;&nbsp;<em>b</em>, the causal effect of the treatment on the outcome; and&nbsp;<em>c</em>, the causal effect of the control variable on the outcome.</p>



<p>Again, thinking of the NHIS study, there may be many other control variables in addition to education and wealth: age, gender, ethnicity, prior medical history, and more. (The actual study took all of these into account.) That is the nature of the game: you’re trying to discover the influence of one effect in a many-dimensional world. In real-world trials, many factors influence the outcome, and it’s not possible to enumerate all of them.</p>



<h4>A note on generative models</h4>



<p>Although generative modeling is outside the scope of this report, it is worth saying a few words about. Loosely speaking, a&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/ppgqb" target="_blank">generative model</a>&nbsp;is essentially a model that specifies the data-generating process (the technical definition is: it models the joint probability P(X, Y) of features X and outcome variable Y, in contrast to discriminative models that model the conditional probability P(Y|X) of the outcome, conditional on the features). Often the statistical model (such as the previous linear equation) will be simpler than the generative model and still obtain accurate estimates of the causal effect of interest, but (1) this isn’t always the case and (2) getting into the habit of thinking how your data was generated, simulating data based on this generative model, and checking whether your statistical model can recover the (known) causal effects, is an indispensable tool in the data scientist’s toolkit.</p>



<p>Consider the case in which we have a true model telling us how the data came to be:</p>



<figure class="wp-block-image size-large is-resized"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/equation2.png" alt="" class="wp-image-14239" width="171" height="26" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/equation2.png 162w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/equation2-160x25.png 160w" sizes="(max-width: 171px) 100vw, 171px" /></figure>



<p>In this generative model,&nbsp;<em>G</em>&nbsp;is the causal effect of&nbsp;<em>T</em><sub><em>i</em></sub>&nbsp;on&nbsp;<em>Y</em><sub><em>i</em></sub>,&nbsp;<em>B</em>&nbsp;is the causal effect of&nbsp;<em>X</em><sub><em>i</em></sub>&nbsp;on&nbsp;<em>Y</em><sub><em>i</em></sub>, and&nbsp;<em>e</em><sub><em>i</em></sub>&nbsp;is the effect of “everything else,” which could be purely random. If&nbsp;<em>X</em><sub><em>i</em></sub>&nbsp;and&nbsp;<em>T</em><sub><em>i</em></sub>&nbsp;are not correlated, we will obtain consistent estimates of&nbsp;<em>G</em>&nbsp;by fitting a linear model:</p>



<figure class="wp-block-image size-large is-resized"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/equation3.png" alt="" class="wp-image-14240" width="135" height="29" /></figure>



<p>However, if&nbsp;<em>T</em><sub><em>i</em></sub>&nbsp;and&nbsp;<em>X</em><sub><em>i</em></sub>&nbsp;are correlated, we have to control for&nbsp;<em>X</em><sub><em>i</em></sub>&nbsp;in the regression, by estimating:</p>



<figure class="wp-block-image size-large is-resized"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/equation4.png" alt="" class="wp-image-14241" width="176" height="23" /></figure>



<p>As previously stated, we have recovered the statistical model we started out with, but now have the added benefit of also having a generative model that allows us to simulate our model, in accordance with the data-generating process.</p>



<h3>Omitted Variable Bias</h3>



<p>Regression requires us to know what the important variables are; your regression is only as good as your knowledge of the system! When you omit important variables for whatever reason, your causal model and inferences will be biased. This type of bias is known as&nbsp;<em>omitted variable bias</em>&nbsp;(OVB). In&nbsp;<em>Mastering ‘Metrics&nbsp;</em>(p. 69), we find:</p>



<blockquote class="wp-block-quote"><p><em>Regression is a way to make other things equal, but equality is generated only for variables included as controls on the right-hand side of the model. Failure to include enough controls or the right controls still leaves us with selection bias. The regression version of the selection bias generated by inadequate controls is called&nbsp;omitted variables bias&nbsp;(OVB), and it’s one of the most important ideas in the metrics canon.</em></p></blockquote>



<p>It’s important to reason carefully about OVB, and it’s nontrivial to do so! One way to do this is performing a&nbsp;<em>sensitivity analysis&nbsp;</em>with respect to our controls, that is, to check out how sensitive the results are to the list of variables. If the changes in the variables you know about have a big effect on the results, you have reason to suspect that results might be equally sensitive to the variables you don’t know about. The less sensitive, or more robust, the regression is, the more confident we can be in the results. We highly recommend the discussion of OVB in Chapter 2 of&nbsp;<em>Mastering ‘Metrics</em>&nbsp;if you want to learn more.</p>



<p>Before moving on to discuss the power of instrumental variables, we want to remind you that there are many interesting and useful techniques that we are not able to cover in this report. One such technique is&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/Jx6qa" target="_blank">regression discontinuity design</a>(RDD) which has gained increasing popularity over recent years and, among other things, has the benefit of having visually testable assumptions (continuity of all X aside from treatment assignment around the discontinuity). For more information, check out Chapter 6 of Cunningham’s&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/4vLE9" target="_blank"><em>Causal Inference</em></a>&nbsp;and&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/75QuS" target="_blank">“Regression Discontinuity Design in Economics”</a>, a paper by Lee and Lemieux.</p>



<h3>Instrumental Variables</h3>



<p>There are situations in which regression won’t work; for example, when an explanatory variable is correlated with the error term. To deal with such situations, we’re going to add&nbsp;<em>instrumental variables</em>&nbsp;to our causal toolkit.</p>



<p>To do so, we’ll consider the example of the cholera epidemic that swept through England in the 1850s. At the time, it was generally accepted that cholera was caused by a vaporous exhalation of unhealthy air (miasma) and poverty, which was reinforced by the observation that cholera seemed more widespread in poorer neighborhoods. (If you’re familiar with Victorian literature, you’ve read about doctors prescribing vacations at the seaside so the patient can breathe healthy air.) The physician John Snow became convinced that the miasma theory was pseudoscience and that people were contracting cholera from the water supply.</p>



<p>To keep track of the different potential causal relationships, we will introduce causal graphs, a key technique that more data scientists need to know about. We start with the proposed causal relationship between miasma and cholera. To draw this as a graph, we have a node for miasma, a node for cholera, and an arrow from miasma to cholera, denoting a causal relationship (Figure 3).</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0103.png" alt="" class="wp-image-14204" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0103.png 660w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0103-300x45.png 300w" sizes="(max-width: 660px) 100vw, 660px" /><figcaption>Figure 3. <em>A causal graph showing the hypothetical relationship between miasma and cholera.</em></figcaption></figure>



<p>The arrow has an associated&nbsp;<em>path coefficient</em>, which describes the strength of the proposed causal effect. Snow’s proposed causal relationship from water purity to cholera introduces another node and edge (Figure 4).</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0104.png" alt="" class="wp-image-14205" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0104.png 420w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0104-300x169.png 300w" sizes="(max-width: 420px) 100vw, 420px" /><figcaption>Figure 4. <em>Adding water purity (P), another hypothetical cause for cholera.</em></figcaption></figure>



<p>However, the miasma theory stated that miasma could be working through the water supply. Therefore, we need to include an arrow from miasma to water purity (Figure 5).</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0105.png" alt="" class="wp-image-14206" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0105.png 420w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0105-300x178.png 300w" sizes="(max-width: 420px) 100vw, 420px" /><figcaption>Figure 5. <em>Adding an arrow to show that miasma (M) could influence water purity (P).</em></figcaption></figure>



<p>We’re running up against the challenge of a potential confounder again! Even if we could find a correlation between water purity and cholera cases, it still may be a result of miasma. And we’re unable to measure miasma directly, so we’re not able to control for it! So how to disprove this theory and/or determine the causal relationship between water purity and cholera?</p>



<p>Enter the instrumental variable. Snow had noticed that most of the water supply came from two companies, the Southwark and Vauxhall Waterworks Company, which drew its water downstream from London’s sewers, and the Lambeth Waterworks Company, which drew its water upstream. This adds another node water company to our causal graph, along with an arrow from water company to water purity (Figure 6).</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0106.png" alt="" class="wp-image-14207" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0106.png 692w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0106-300x116.png 300w" sizes="(max-width: 692px) 100vw, 692px" /><figcaption>Figure 6. <em>Adding the water supply (W), which affects purity, and is not affected by miasma.</em></figcaption></figure>



<p>Water company (W) is an instrumental variable; it’s a way to vary the water purity (P) in a way that’s independent of miasma (M). Now that we’ve finished the causal graph, notice which arrows are not present:</p>



<ul><li>There are no arrows between water company and miasma. Miasma can’t cause a water company to exist, and vice versa.</li><li>There is no direct arrow from water company to cholera, as the only causal effect that water company could have on cholera is as a result of its effect on water purity.</li><li>There are&nbsp;<em>no</em>&nbsp;other arrows (potential confounders) that point into water company and cholera. Any correlation must be causal.</li></ul>



<p>Each arrow has an associated&nbsp;<em>path coefficient</em>, which describes the strength of the relevant proposed causal effect. Because W and P are unconfounded, the causal effect c<sub>WP</sub>&nbsp;of W on P can be estimated from their correlation coefficient r<sub>WP</sub>. As W and C are also unconfounded, the causal effect c<sub>WC</sub>&nbsp;of W on C can also be estimated from the relevant correlation coefficient r<sub>WC</sub>. Causal effects along paths are multiplicative, meaning that c<sub>WC</sub>&nbsp;= c<sub>WP</sub>c<sub>PC</sub>. This tells us that the causal effect of interest, c<sub>PC</sub>, can be expressed as the ratio c<sub>WC</sub>&nbsp;/c<sub>WP</sub>&nbsp;= r<sub>WC</sub>&nbsp;/r<sub>WP</sub>. This is amazing! Using the instrumental variable W, we have found the causal effect of P on C without being able to measure the confounder M. Generally, any variable possessing the following characteristics of W is an instrumental variable and can be used in this manner:</p>



<ul><li>There is no arrow between W and M (they are independent).</li><li>There is no direct arrow from W to C.</li><li>There is an arrow from W to P.</li></ul>



<p>All of this is eminently more approachable and manageable when framed in the language of graphs. For this reason, in the next section, we’ll focus on how causal graphs can help us think through causality and causal effects and perform causal inference.</p>



<div class="wp-block-group has-very-light-gray-background-color has-background"><div class="wp-block-group__inner-container">
<blockquote class="wp-block-quote"><p><strong>NOTE</strong><br>To be explicit, there has been something of a two cultures problem in the world of causality: those that use econometrics methods (such as those in&nbsp;<em>Mastering ‘Metrics</em>) and those that use causal graphs. It is plausible that the lack of significant crosspollination between these communities is one of the reasons causal inference is not more mature and widespread as a discipline (although proving this causal claim would be tough!). There are few resources that deal well with both worlds of causality, but Cunningham&#8217;s&nbsp;<em>Causal Inference: The Mixtape</em>&nbsp;is one that admirably attempts to do so.</p></blockquote>
</div></div>



<h3>Causal Graphs</h3>



<p>Randomized control trials are designed to tell us whether an action, X, can cause an outcome, Y. We can represent that with the simplest of all causal graphs (Figure 7). But in the real world, causality is never that simple. In the real world, there are also confounding factors that need to be accounted for. We’ve seen that RCTs can account for some of these confounding factors. But we need better tools to understand confounding factors and how they influence our results. That’s where causal graphs are a big help.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0107.png" alt="" class="wp-image-14208" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0107.png 660w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0107-300x42.png 300w" sizes="(max-width: 660px) 100vw, 660px" /><figcaption>Figure 7. <em>A simple causal graph: X causes Y.</em></figcaption></figure>



<h4>Forks and confounders</h4>



<p>In the causal diagram in&nbsp;Figure 8, a variable Y has a causal effect on two variables X and Z, which means that X and Z will be correlated, even if there’s no causal relation between X and Z themselves! We call this a&nbsp;<em>fork</em>. If we want to investigate the causal relationship between X and Z, we have to deal with the presence of the confounder, Y. As we’ve seen, RCTs are a good way to deal with potential confounders.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0108.png" alt="" class="wp-image-14209" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0108.png 747w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0108-300x144.png 300w" sizes="(max-width: 747px) 100vw, 747px" /><figcaption>Figure 8. <em>Age influences the ability to walk and the death rate. This is a fork. Does walking influence the death rate?</em></figcaption></figure>



<p>As an example, a 1998&nbsp;<em>New England Journal of Medicine</em>&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/nfIQq" target="_blank">paper</a>&nbsp;identified a correlation between regular walking and reduced death rates among retired men. It was an observational study so the authors had to consider confounders. For example, you could imagine that age could be a confounder: health decays as you get older, and decaying health makes you less likely to walk regularly. When the study’s authors took this into account, though, they still saw an effect. Furthermore, that effect remained even after accounting for other confounding factors.</p>



<h4>Colliders</h4>



<p>The causal diagram in&nbsp;Figure 9&nbsp;is a collider. Colliders occur whenever two phenomena have a common effect, such as a disease X, a risk factor Y, and whether the person is an inpatient or not. When you condition on the downstream variable Y (in hospital or not), you will see a spurious negative correlation between X and Y. While this seems strange, reasoning through this situation explains the negative correlation: an inpatient without the risk factor is more likely to have the disease than a general member of the population, as they’re in hospital! This type of bias is also known as&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/O96pW" target="_blank">Berkson’s paradox</a>.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0109.png" alt="" class="wp-image-14210" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0109.png 714w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0109-300x111.png 300w" sizes="(max-width: 714px) 100vw, 714px" /><figcaption>Figure 9. <em>A disease like COVID can lead to hospitalization. Other health factors can also lead to hospitalization. This is a collider.</em></figcaption></figure>



<p>To think about this concretely, imagine one group of patients with COVID, and another with appendicitis. Both can cause hospital admissions, and there’s no plausible (at least as far as we know) connection between COVID and appendicitis. However, a hospital patient who does not have appendicitis is more likely to have COVID than a member of the general public; after all, that patient is in the hospital for something, and it isn’t appendicitis! Therefore, when you collect the data and work the statistics out, there will be a negative correlation between hospitalization from COVID and appendicitis: that is, it will look like appendicitis prevents severe COVID, or vice versa; the arrow of correlation points both ways. It’s always risky to say &#8220;we just know that can’t be true.&#8221; But in the absence of very compelling evidence, we are justified in being very suspicious of any connection between COVID and a completely unrelated medical condition.</p>



<p>RCTs often condition on colliders—but as we’ve seen, conditioning on a collider introduces a false (negative) correlation, precisely what you want to avoid. In the absence of other causal possibilities, the collider itself is evidence that X and Y are not causally related.</p>



<h4>The flow of information</h4>



<p>Causal graphs allow us to reason about the flow of information. Take, for example, the causal chain X → Y → Z. In this chain, information about X gives us information about Y, which in turn provides information about Z. However, if we control for Y (by choosing, for example, a particular value of Y), information about X then provides no new information about Z.</p>



<p>Similarly, in the fork X ← Y → Z, where X = walking, Y = age,&nbsp;Z = death rate,&nbsp;information about&nbsp;<em>walking</em>&nbsp;gives us information about&nbsp;<em>death rate</em>&nbsp;(as there is correlation, but not causation). However, when controlling for the confounder&nbsp;<em>age</em>, no information flows from&nbsp;<em>walking</em> to death rate (that is, there is no correlation when holding&nbsp;<em>age</em> constant).</p>



<p>In the collider X → Y ← Z, where X = disease, Y = in hospital,&nbsp;Z = risk factor, the situation is reversed! Information does&nbsp;<em>not</em>&nbsp;flow from X to Z until we control for Y. And controlling for Y introduces a spurious correlation that can cause us to misunderstand the causal relationships.</p>



<p>If no information flows from X → Y through Z, we say that Z blocks X → Y, and this will be important when thinking more generally about information flow through causal graphs, as we’ll now see.</p>



<h4>In practice: The back-door adjustment</h4>



<p>At this point, we have methods for deciding which events might be confounders (forks), and which events look like confounders but aren’t (colliders). So, the next step is determining how to deal with the true confounders. We can do this through the back-door and front-door adjustments, which let us remove the effect of confounders from an experiment.</p>



<p>We’re interested in whether there’s a causal relationship between X and an outcome Y, in the presence of a potential confounder Z: look at&nbsp;Figure 10.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0110.png" alt="" class="wp-image-14211" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0110.png 420w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0110-300x166.png 300w" sizes="(max-width: 420px) 100vw, 420px" /><figcaption>Figure 10. <em>The back-door adjustment: is Z a confounder?</em></figcaption></figure>



<p>If there is a causal effect, though, and the back-door criterion (which we define later) is satisfied, we can solve for the causal relationship in question. Given X → Y, a collection of variables Z satisfies the back-door criterion if:</p>



<ol><li>No node in Z is a descendant of X.</li><li>Any path between X and Y that begins with an arrow into X (known as a back-door path) is blocked by Z.</li></ol>



<p>Controlling for Z essentially then blocks all noncausal paths between X and Y while not blocking any causal paths. So how does the adjustment work?</p>



<p>Here, we’ll consider the simplified case, in which Z contains a single variable. We could compute the correlation between X and Y for different values of the confounding factor Z, and weight them according to the probabilities of different values of Z. But there’s a simpler solution. Using linear regression to compute the line that best fits your X and Y data points is straightforward. In this situation, we take it a step further: we compute the best fit plane for X, Y, and Z. The math is essentially the same. The equation for this plane will be of the form:</p>



<figure class="wp-block-image size-large is-resized"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/equation5.png" alt="" class="wp-image-14242" width="161" height="23" /></figure>



<p>The slope associated with X (<em>m</em><sub class="has-small-font-size">1</sub>) takes into account the effect of the confounder. It’s the average causal effect of X on Y. And, while we’ve only discussed a single confounder, this approach works just as well with multiple confounders.</p>



<h4>In practice: The front-door adjustment</h4>



<p>We still have to account for one important case. What if the confounding factor is either unobservable or hypothetical? How do you account for a factor that you can’t observe? Pearl discusses research into the connection between smoking and cancer, into which the tobacco companies inserted the idea of a “smoking gene” that would predispose people towards both smoking and cancer. This raises a problem: what happens if there’s a cause that can’t be observed? In the ’50s and ’60s, our understanding of genetics was limited; if there was a smoking gene, we certainly didn’t have the biotech to find it. There are plenty of cases where there are more plausible confounding factors, but detecting them is impossible, destructive, or unethical.</p>



<p>Pearl outlines a way to deal with these unknowable confounders that he calls the front-door adjustment (Figure 11). To investigate whether smoking S causes cancer C in the presence of an unknowable confounder G, we add another step in the causal graph between S and C. Discussing the smoking case, Pearl uses the presence of tar in the lungs. We’ll just call it T. We believe that T can’t be caused directly by the confounding factor G (though that’s a question worth thinking about). Then we can use the back-door correction to estimate the effect of T on C, with S coming through the back door. We can also estimate the causal effect of S on T as there is a collider at C. We can combine these to retrieve the causal effect of S on C.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0111.png" alt="" class="wp-image-14212" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0111.png 692w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/wici_0111-300x95.png 300w" sizes="(max-width: 692px) 100vw, 692px" /><figcaption>Figure 11. <em>The front-door adjustment: is G a confounder that can’t be measured?</em></figcaption></figure>



<p>This has been abstract, and the only real solution to the abstraction would be getting into the mathematics. For our purposes, though, it’s enough to note that it is possible to correct for hypothetical confounding factors that aren’t measurable and that might not exist. This is a real breakthrough. We can’t agree with Pearl’s claim that one causal graph would have replaced years of debate and testimony—politicians will be politicians, and lobbyists will be lobbyists. But it is very important to know that we have the tools.</p>



<p>One thing to note is that both the back-door and front-door adjustments require you to have the correct causal graph, containing all relevant confounding variables. This can often be challenging in practice and requires significant domain expertise.</p>



<h2>The End of Correlation, the Beginning of Cause</h2>



<p>Correlation is a powerful tool and will remain so. It’s a tool, not an end in itself. We need desperately to get beyond the idea that correlation is an adequate proxy for causality. Just think of all those people drowning because Nicolas Cage makes more films!</p>



<p>As “data science” became a buzzword, we got lazy: we thought that, if we could just gather enough data, correlation would be good enough. We can now store all the data we could conceivably want (a petabyte costs around $20,000 retail), and correlation still hasn’t gotten us what we want: the ability to understand cause and effect. But as we’ve seen, it is possible to go further. Medical research has been using RCTs for decades; causal graphs provide new tools and techniques for thinking about the relationships between possible causes. Epidemiologists like John Snow, the doctors who made the connection between smoking and cancer, and the many scientists who have made the causal connection between human activity and climate change, have all taken this path.</p>



<p>We have tools, and good ones, for investigating cause and weeding out the effects of confounders. It’s time to start using them.</p>



<hr class="wp-block-separator" />



<h2>Footnotes</h2>



<ol><li id="fn1">In practice, what is important is that all confounding variables are distributed across treatment and control.</li><li id="fn2">The <i>p</i>-value is <i>not</i> the probability that the hypothesis “there is no difference between the control and treatment groups” is true, as many think it is. Nor is it the probability of observing your data if the hypothesis is true, as many others think. In fact, the definition of <i>p</i>-value is so difficult to remember that <a href="https://fivethirtyeight.com/features/not-even-scientists-can-easily-explain-p-values/" target="_blank" rel="noopener noreferrer">“Not Even Scientists Can Easily Explain P-values”</a>.</li><li id="fn3">Note that the standard error is not the same as the standard deviation of the data, but rather the standard deviation of the sampling distribution of the estimate of the mean.</li></ol>



<hr class="wp-block-separator" />



<h2>Glossary</h2>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<p><em>A/B test</em></p>



<blockquote class="wp-block-quote"><p>A randomized control trial in tech.</p></blockquote>
</div></div>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<p><em>causal graph</em></p>



<blockquote class="wp-block-quote"><p>A graphical model used to illustrate (potential) causal relationships between variables of interest.</p></blockquote>
</div></div>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<p><em>ceteris paribus</em></p>



<blockquote class="wp-block-quote"><p>The principle of “all other things being equal,” which is essential for randomized control trials.</p></blockquote>
</div></div>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<p><em>collider</em></p>



<blockquote class="wp-block-quote"><p>A causal model in which two phenomena have a common effect, such as a disease X, a risk factor Y, and whether the person is an inpatient or not: X → Y ← Z.</p></blockquote>
</div></div>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<p><em>confounding variable</em></p>



<blockquote class="wp-block-quote"><p>A variable that influences both the dependent and independent variables.</p></blockquote>
</div></div>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<p><em>counterfactual</em></p>



<blockquote class="wp-block-quote"><p>The rung of the ladder of causation at which we can use causal models to reason about events that did not occur.</p></blockquote>
</div></div>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<p><em>fork</em></p>



<blockquote class="wp-block-quote"><p>A causal model in which there is a confounding variable X ← Y → Z.</p></blockquote>
</div></div>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<p><em>generative model</em></p>



<blockquote class="wp-block-quote"><p>A generative model is essentially a model that specifies the data-generating process. The technical definition is that it models the joint probability P(X, Y) of features X and outcome variable Y, in contrast to discriminative models that model the conditional probability P(Y|X) of the outcome, conditional on the features).</p></blockquote>
</div></div>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<p><em>instrumental variable</em></p>



<blockquote class="wp-block-quote"><p>Given X → Y, an instrumental variable Z is a third variable used in regression analyses to account for unexpected relationships between other variables (such as one being correlated with the error term).</p></blockquote>
</div></div>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<p><em>intervention</em></p>



<blockquote class="wp-block-quote"><p>The rung of the ladder of causation at which we can perform experiments, most famously in the form of randomized control trials and A/B tests.</p></blockquote>
</div></div>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<p><em>omitted variable bias</em></p>



<blockquote class="wp-block-quote"><p>When failure to include enough controls or the right controls still leaves us with selection bias.</p></blockquote>
</div></div>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<p><em>p-value</em></p>



<blockquote class="wp-block-quote"><p>In a hypothesis test, the&nbsp;<em>p</em>-value is the probability of observing a test statistic at least as extreme as the one observed.</p></blockquote>
</div></div>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<p><em>randomized control trial (RCT)</em></p>



<blockquote class="wp-block-quote"><p>An experiment in which subjects are randomly assigned to one of several groups, in order to ascertain the impact in the outcome of differences in treatment.</p></blockquote>
</div></div>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<p><em>standard error</em></p>



<blockquote class="wp-block-quote"><p>The standard error of a statistic (for example, the mean) is the standard deviation of its sampling distribution. In other words, it’s a measure of uncertainty of the sample mean.</p></blockquote>
</div></div>



<hr class="wp-block-separator" />



<h2>References</h2>



<p><em>Key references are marked with an asterisk.</em></p>



<p>Anderson, Chris.&nbsp;<a href="https://www.wired.com/2008/06/pb-theory" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">“The End of Theory: The Data Deluge Makes the Scientific Method Obsolete”</a>.&nbsp;<em>Wired</em>&nbsp;(2008).</p>



<p>*Angrist, Joshua D., and Jörn-Steffen Pischke.&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.masteringmetrics.com/" target="_blank"><em>Mastering ‘Metrics: The Path from Cause to Effect</em></a>. Princeton University Press (2014).</p>



<p>Aschwanden, Christie.&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://fivethirtyeight.com/features/not-even-scientists-can-easily-explain-p-values" target="_blank">“Not Even Scientists Can Easily Explain P-values”</a>. FiveThirtyEight (2015).</p>



<p>Bowne-Anderson, Hugo.&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.oreilly.com/radar/the-unreasonable-importance-of-data-preparation" target="_blank">“The Unreasonable Importance of Data Preparation”</a>. O’Reilly (2020).</p>



<p>Clayton, Aubrey.&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://nautil.us/issue/92/frontiers/how-eugenics-shaped-statistics" target="_blank">“How Eugenics Shaped Statistics”</a>.&nbsp;<em>Nautilus</em>&nbsp;(2020).</p>



<p>Clayton, Aubrey.&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://cup.columbia.edu/book/bernoullis-fallacy/9780231199940" target="_blank"><em>Bernoulli&#8217;s Fallacy</em></a>. Columbia University Press (2021).</p>



<p>*Cunningham, Scott.&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://mixtape.scunning.com/" target="_blank"><em>Causal Inference: The Mixtape</em></a>. Yale University Press (2021).</p>



<p>Eckles, Dean.&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://statmodeling.stat.columbia.edu/2021/08/22/does-the-table-1-fallacy-apply-if-it-is-table-s1-instead" target="_blank">“Does the ‘Table 1 Fallacy’ Apply if It Is Table S1 Instead?”</a>. Blog (2021).</p>



<p>Google.&nbsp;<a href="https://developers.google.com/machine-learning/gan/generative" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">“Background: What Is a Generative Model?”</a>. (2021).</p>



<p>*Kelleher, Adam.&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://medium.com/@akelleh/a-technical-primer-on-causality-181db2575e41" target="_blank">“A Technical Primer on Causality”</a>. Blog (2021).</p>



<p>Kohavi, Ron, et al.&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.amazon.com/Trustworthy-Online-Controlled-Experiments-Practical/dp/1108724264" target="_blank"><em>Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing</em></a>. Cambridge University Press (2020).</p>



<p>Lee, David S., and Thomas Lemieux.&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.princeton.edu/~davidlee/wp/RDDEconomics.pdf" target="_blank">“Regression Discontinuity Designs in Economics”</a>. Journal of Economic Literature (2010).</p>



<p>*Pearl, Judea, and Dana Mackenzie.&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oreil.ly/HTz6F" target="_blank"><em>The Book of Why</em></a>. Basic Books (2018).</p>



<p>Wikipedia.&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Berkson%27s_paradox" target="_blank">&#8220;Berkson’s paradox&#8221;</a>. Last modified December 9, 2021.</p>



<p>Wikipedia.&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Regression_discontinuity_design" target="_blank">&#8220;Regression discontinuity design&#8221;</a>. Last modified June 14, 2021.</p>



<p>Robinson, Emily.&nbsp;<a href="https://hookedondata.org/guidelines-for-ab-testing" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">“Guidelines for A/B Testing”</a>. Hooked on Data (2018).</p>



<p>Simonite, Tom.&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.wired.com/story/how-algorithm-favored-whites-over-blacks-health-care" target="_blank">“A Health Care Algorithm Offered Less Care to Black Patients”</a>.&nbsp;<em>Wired</em>&nbsp;(2019).</p>



<p>Spieth, Peter Markus, et al.&nbsp;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4910682" target="_blank">“Randomized Controlled Trials—A Matter of Design”</a>. NCBI (2016).</p>



<hr class="wp-block-separator" />



<h2 class="has-text-align-center">Thanks</h2>



<p>The authors would like to thank Sarah Catanzaro and James Savage for their valuable and critical feedback on drafts of this report along the way.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/what-is-causal-inference/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>The Cloud in 2021: Adoption Continues</title>
		<link>https://www.oreilly.com/radar/the-cloud-in-2021-adoption-continues/</link>
				<comments>https://www.oreilly.com/radar/the-cloud-in-2021-adoption-continues/#respond</comments>
				<pubDate>Tue, 07 Dec 2021 12:26:55 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides]]></dc:creator>
				<category><![CDATA[Cloud]]></category>
		<category><![CDATA[Research]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=14129</guid>
				<description><![CDATA[Last year, our report on cloud adoption showed that companies were moving quickly to adopt the cloud: 88% of our respondents from January 2020 said that they used the cloud, and about 25% said that their companies planned to move all of their applications to the cloud in the coming year. This year, we wanted [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>Last year, our report on cloud adoption showed that companies were moving quickly to adopt the cloud: 88% of our respondents from January 2020 said that they used the cloud, and about 25% said that their companies planned to move all of their applications to the cloud in the coming year.</p>



<p>This year, we wanted to see whether that trend continued, so we ran another online survey from July 26, 2021, to August 4, 2021. Since the 2020 survey was taken when the pandemic was looming but hadn’t yet taken hold, we were also curious about how the lockdown affected cloud adoption. The short answer is “not much”; we saw  surprisingly few changes between January 2020 and July 2021. Cloud adoption  was proceeding rapidly; that’s still the case. </p>



<h3>Executive summary</h3>



<ul><li>Roughly 90% of the respondents indicated that their organizations are using the cloud. That’s a small increase over last year’s 88%.</li><li>The response to the survey was global; all continents (save Antarctica) were represented. Compared to last year, there was a much higher percentage of respondents from Europe (33%, as opposed to 11%) and a lower percentage from North America (42%).</li><li>In every industry, at least 75% of the respondents work for organizations using the cloud. The most proactive industries are retail &amp; ecommerce, finance &amp; banking, and software.</li><li>Amazon Web Services (AWS) (62%), Microsoft Azure (48%), and Google Cloud (33%) are still the big three, though Amazon’s market share has dropped slightly since last year (down from 67%). Most respondents  use multiple cloud providers.</li><li>Industry to industry, we saw few differences in cloud providers, with two exceptions: Azure is used more heavily than AWS in the government and consulting &amp; professional services sectors.</li><li>Two-thirds of respondents (67%) reported using a public cloud; 45% are using a private cloud; and 55% are using traditionally managed on-premises infrastructure. </li><li>Almost half (48%) said they plan to migrate 50% or more of their applications to the cloud in the coming year. 20% plan ti migrate all of their applications.</li><li>47% said that their organizations are pursuing a cloud first strategy. 30% said that their organizations are already cloud native, and 37% said that they plan to be cloud native within three or more years. Only 5% are engaged in “repatriation” (bringing cloud applications back to on-premises infrastructure).</li><li>Among respondents who are using the cloud, the biggest concern is managing cost (30%). Compliance is a relatively minor concern (10%) and isn’t the most significant concern even in heavily regulated sectors  such as finance &amp; banking (15%), government (19%),  and healthcare (19%).</li><li>When asked what skills organizations needed to succeed, respondents were divided fairly evenly, with “cloud-based security” (59%) and “general cloud knowledge” (54%) the most common responses.</li></ul>



<h3>Demographics: Who responded</h3>



<p>The survey was sent to recipients of O’Reilly’s Programming and  <em>Infrastructure &amp; Ops Newsletters</em>, which together have 436,000  subscribers. 2,834 respondents completed the survey. </p>



<p>The respondents represent a relatively senior group. 36% have over  10 years’ experience in their current role, and almost half (49%) have over seven years’ experience. Newer developers were also well-represented. 23%  have spent one to three years in their current position, and 8% have spent under one year.</p>



<p>Parsing job titles is always problematic, given that the same position can be expressed in many different ways. Nevertheless, the top five job titles were developer (4.9%<sup>1</sup>), software engineer (3.9%), CTO (3.0%), software developer (3.0%), and architect (2.3%). We were surprised by the number of respondents who had the title CTO or CEO. Nobody listed CDO or chief data officer as a title. </p>



<p>Aggregating terms like “software,” “developer,” “programmer,” and others lets us estimate that 36% of the respondents are programmers. 21% are architects or technical leads. 10% are C-suite executives or directors. 8% are managers. Only 7% are data professionals (analysts, data scientists, or statisticians), and only 6% are operations staff (DevOps practitioners, sysadmins, or site reliability engineers).</p>



<p>The respondents came from 128 different countries and were spread across all continents except for Antarctica. Most of the respondents were from North America (42%) and Europe (33%). 13% were from Asia, though that almost certainly doesn’t  reflect the extent of cloud computing in Asia. </p>



<p>In particular, there were few respondents from China: only 8, or about 0.3% of the total. South America, Oceania, and Africa were also  represented, by 6%, 4%, and 2% of the respondents, respectively. These results are significantly different from last year’s. In 2020, two-thirds of the respondents were from North America, and only 11% were from Europe. The other continents showed little change. Last year, we noted that European organizations were reluctant to adopt the cloud. That’s clearly no longer true.  </p>



<p>Cloud users are spread throughout the industrial spectrum.  Respondents to our survey were clustered most strongly in the  software industry (36%). The next largest group comprises those  who replied “other” (13%), and they are indeed scattered through  industries from art to aviation (including some outliers like  prophecy, which we never knew was an industry). Consulting &amp; professional services (12%) was third; we suspect that many respondents in this group could equally well say they were in  the software industry. Finance &amp; banking (11%) was also well- represented. 5% of the respondents work in healthcare; another 5% were from higher education; 4% were in government; and  a total of 4% work in electronics &amp; hardware or computers (2% each). Surprisingly, only 3% of the respondents work in retail &amp; ecommerce; we would have expected Amazon alone to account for that.</p>



<p>These results are very similar to the results from last year’s survey, with two major differences: this year, an even larger percentage of our respondents were from the software  industry (23% in 2020), and a significantly larger group  classified their industries as “other” (20%).</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0101-1.png" alt="" class="wp-image-14130" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0101-1.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0101-1-300x212.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0101-1-768x543.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Survey respondents by industry</figcaption></figure>



<p>What does this mean? Less than it seems. We have to  remind both ourselves and our readers that the number of respondents in any sector reflects, first, the size of that sector; second, our mailing lists’ penetration into that sector; and only third, cloud usage in that sector. The fact that 35% of the respondents are in the software industry while only 5% are in healthcare doesn’t by itself mean that the cloud has penetrated much more deeply into software. It means that the healthcare industry has fewer readers of our newsletters than does the software industry, hardly a surprising conclusion. To estimate cloud usage in any given sector, you have to look only at that sector’s data. What it says is that our conclusions about the software industry are based on roughly 1,000 respondents, while conclusions about the healthcare industry are only  based on about 150 respondents, and are correspondingly less reliable.</p>



<h3>The big picture</h3>



<p>The big picture won’t surprise anyone. Almost all of the respondents work for organizations that are using cloud computing; only 10.3% answered a question asking why their organization doesn’t use  cloud computing, implying that cloud usage is 89.7%. Likewise,  when asked what cloud provider they’re using, 10.7% said “not applicable,” suggesting cloud usage of 89.3%. We can get a third fix on cloud usage by looking at a later question about cloud technologies. We asked whether respondents are using public clouds, private clouds, hybrid clouds, multiclouds, or traditionally managed  <br> infrastructure. Respondents were allowed to select multiple answers, and most did. However, respondents whose organizations aren’t using any cloud technology would check “traditionally managed infrastructure.” Those respondents amounted to 7.5% of the total, suggesting 92.5% of the respondents are using the cloud in some form. Therefore, we can say with some confidence that the number of respondents  whose organizations are using the cloud is somewhere between  89% and 93%. </p>



<p>These figures compare with 88% from our 2020 survey—a change that may well be insignificant. However, it’s worth asking what “insignificant” means: would we expect the number of “not using”  responses to be near zero? On one hand, we’re surprised that there hasn’t been a larger change from year to year; on the other hand, when you’re already near 90%, gaining even a single percentage point is difficult. We can be somewhat (only somewhat) confident that there’s a genuine trend because we asked the same question three different ways and got similar results. An additional percentage point or two may be all we get, even if it doesn’t allow  us to be as confident as we’d like. </p>



<p>Did the pandemic have an effect? It certainly didn’t slow cloud  adoption. Cloud computing was an obvious solution when it became  difficult or impossible to staff on-premises infrastructure. You could argue that the pandemic wasn’t much of an accelerant either, and it would be hard to disagree. Once again though, when you’re at 88%, gaining a percentage point (or two or three) is an achievement.</p>



<h4>AWS, Azure, and Google Cloud: The big three and beyond</h4>



<p>The big three in cloud computing are Amazon Web Services (AWS), Microsoft Azure, and Google Cloud, used by 62%, 48%, and 33% of the respondents, respectively. (Because many organizations use multiple providers, respondents were allowed to select more than one option.) Oracle Cloud (6%), IBM Cloud (5%), and Alibaba Cloud (2%) took fourth through sixth place. They have a long way to go to catch up to the leaders, <br> although Oracle seems to have surpassed IBM. It’s also worth noting that, although Alibaba’s 2% seems weak, we expect Alibaba to be strongest in China, where we had very few respondents. Better visibility into Chinese industry might change the picture dramatically. </p>



<p>9% of the respondents selected “other” as their cloud provider. The leading “other” provider was Digital Ocean (1.4%), which almost edged out Alibaba. Salesforce, Rackspace, SAP, and VMware also appeared among the “others,” along with the Asian provider Tencent.  Many of these “other” providers are software-as-a-service companies that don’t provide the kind of infrastructure services on which the big three have built their businesses. Finally, 11% of the respondents answered “not applicable.” These are presumably respondents whose organizations aren’t  using the cloud. </p>



<p>Compared to last year, AWS appears to have lost some market  share, going from 67% in 2020 to 62%. Microsoft Azure and  Google Cloud remain unchanged. </p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0102-1.png" alt="" class="wp-image-14132" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0102-1.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0102-1-300x240.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0102-1-768x616.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Percent of respondents using each of the major cloud providers</figcaption></figure>



<h3>Cloud usage by industry</h3>



<p>One goal of our survey was to determine how cloud usage varies from industry to industry. We felt that the best way to answer that question was to go at it in reverse, by looking at the respondents who answered the question “What best describes why your organization doesn’t use cloud computing?” (which we’ll discuss in more detail later). Our results provided other ways to  answer this question—for example, by looking at “not applicable” responses to questions about cloud providers. All approaches yielded substantially the same answers.</p>



<p>We found that retail &amp; ecommerce, media &amp; entertainment, finance &amp; banking, and software stand out as the industry sectors with the highest cloud use. Only 3.1% of the respondents from the  retail &amp; ecommerce sector answered this question, indicating that cloud usage was close to universal (96.9%). 5.1% of the respondents in media &amp; entertainment, 7.2% of the respondents in finance &amp; banking, and 7.5% of the respondents in software answered this question, suggesting 94.9%, 92.8%, and 92.5% cloud usage, respectively. Most industries (including healthcare and higher education) clustered around 10% of organizations that aren’t using the cloud, or 90% cloud usage. The most cloud-averse industries were electronics &amp; hardware (with 25% indicating that they don’t use the cloud) and government (16% not using). But consider: 25% of respondents indicating that they don’t use the cloud implies that 75% of the respondents do. While we saw variation from industry to industry, cloud users are a solid  majority everywhere. </p>



<p>Can we get beyond the numbers to the “why”? Perhaps not without a much more detailed survey, but we can make some guesses. Although we had few respondents from the retail &amp; ecommerce sector, it’s important to note that this industry is where the cloud took off: AWS began when Amazon started selling “excess capacity” in its data centers. Jeff Bezos paved the way for this with his famous “API mandate” memo, which required all software to be built as collections of services. In media &amp; entertainment, Netflix has been very public about its cloud strategy. The company relies on the cloud for all of its scalable computing and storage needs, an approach initially undertaken as a way of avoiding on-premises infrastructure as a single point of failure.</p>



<p>But history often counts for little in tech. What’s more important is that retail &amp; ecommerce is a sector subject to huge fluctuations in load. Black Friday is approaching as we publish this; need we say more? If your ecommerce site slows to a crawl under heavy load, you lose sales. The cloud is an ideal solution to that problem. No CIO wants to build an on-premises data center that can handle 100x changes in load. The same is true for Netflix, though perhaps not to the same degree: a new movie release almost certainly creates a spike in traffic, possibly a huge one. And in the <br> past few years, movie studios, vendors like Amazon, and many others in the industry have realized that the future of movies lies in selling subscriptions to streaming services, not cinema tickets. Cloud technologies are ideal for streaming services. </p>



<p>Just about every software company, from startups to established vendors like Microsoft and Adobe, now offers “software as a service” (SaaS), an approach arguably pioneered by Salesforce. Whether or not subscription services are the future of software, most software companies are betting heavily on cloud offerings, and they’re building those offerings in the cloud.</p>



<p>Understanding why banks are moving to the cloud may be more difficult, but we think it comes down to focusing on your core competence. The finance &amp; banking industry has historically been very conservative, with organizations going for decades without  significant change to their business models  or procedures. In the past decade, that stability has gone out the window. Financial service companies and banks are now offering online and mobile products, investment services, financial planning, and much more. The best way to service these new applications isn’t by building out legacy infrastructure designed  to support legacy applications largely unchanged since the  beginning of computing; it’s by moving to an infrastructure that can scale on demand and that can be quickly adapted to support new applications.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0103-1.png" alt="" class="wp-image-14133" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0103-1.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0103-1-300x242.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0103-1-768x620.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Cloud usage by industry</figcaption></figure>



<p>Our next step was to look at the cloud providers to determine what providers are used by each industry. Are some providers used more widely in certain industries than in others? When we looked at this question, we saw a familiar pattern: AWS is the most widely used, followed by Microsoft Azure and Google Cloud. AWS dominates media &amp; entertainment (79%) and is the most commonly used provider in every sector except for consulting &amp; professional services (58%, compared to Azure at 60%) and government (52%, compared to Azure at 59%). </p>



<p>In addition to government and consulting &amp; professional services, Azure is widely used in finance &amp; banking (55%). That shouldn’t be surprising given the historical prominence of Microsoft Office in this industry. </p>



<p>Google Cloud was third in every sector except for media &amp; entertainment (35%), where it edged out Azure. It’s strongest in the consulting &amp; professional services sector (41%) and the relatively small computers sector (40%) and weakest in government (16%), healthcare (25%), and finance &amp; banking (29%). </p>



<p>Electronics &amp; hardware had the greatest number of respondents who answered “not applicable” (28%). Although there were surprisingly few respondents from the retail &amp; ecommerce sector, it had the fewest (3%) respondents who answered “not applicable.”</p>



<p>AWS’s, Microsoft Azure’s, and Google Cloud’s shares were closest  to each other in the higher education sector (49%, 43%, and  39%, respectively).</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0104-1.png" alt="" class="wp-image-14134" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0104-1.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0104-1-300x205.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0104-1-768x525.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Cloud provider usage by industry</figcaption></figure>



<h3>Cloud usage by geography</h3>



<p>We wondered whether the usage of different cloud providers varied by continent: are some cloud providers more popular on some continents than on others? By and large, the answer is no. AWS leads by a substantial margin on every continent, and Microsoft Azure and Google Cloud take second and third place, though their relative strengths vary. Google Cloud is significantly stronger in South America (49%) and Asia (40%) than on the other continents. Azure is strongest in Oceania (55%), Africa (51%), and Europe (50%).</p>



<p>Alibaba Cloud is a somewhat more common choice in Asia (5%) and Oceania (3%), but not enough to change the picture significantly.  Remember, though, that we had few respondents from China, where we suspect that cloud adoption is significant and Alibaba is a strong contender. </p>



<p>Although the percentages are relatively small, it’s also worth noting that more respondents in Oceania are using “other” providers (13%), possibly because their relative geographic isolation makes local cloud providers more attractive, and that a large percentage of respondents in Europe answered “not applicable” (14%), indicating that cloud adoption may still be lagging somewhat.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0105-1.png" alt="" class="wp-image-14135" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0105-1.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0105-1-300x244.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0105-1-768x623.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Cloud vendor usage by continent:<br> North America</figcaption></figure>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0106-1.png" alt="" class="wp-image-14136" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0106-1.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0106-1-300x244.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0106-1-768x623.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Cloud vendor usage by continent:<br> Europe</figcaption></figure>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0107-1.png" alt="" class="wp-image-14137" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0107-1.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0107-1-300x244.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0107-1-768x623.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Cloud vendor usage by continent:<br> Asia</figcaption></figure>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0108-1.png" alt="" class="wp-image-14138" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0108-1.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0108-1-300x244.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0108-1-768x623.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Cloud vendor usage by continent:<br> South America</figcaption></figure>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0109-1.png" alt="" class="wp-image-14139" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0109-1.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0109-1-300x244.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0109-1-768x623.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Cloud vendor usage by continent:<br> Oceania</figcaption></figure>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0110-1.png" alt="" class="wp-image-14140" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0110-1.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0110-1-300x244.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0110-1-768x623.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Cloud vendor usage by continent:<br> Africa</figcaption></figure>



<p>While we’ll discuss multicloud in more detail later, it’s interesting that this diagram gives some hints about the extent of deployment on multiple clouds. Remember that respondents could and frequently did select multiple cloud providers, so the total percentage in any continent (always greater than 100%) is a very rough indicator of multiple cloud deployments. By that measure, Africa (totaling 142%) and Europe (158%) have the fewest multiple cloud deployments; Oceania (179%) has the most. </p>



<h3>Public or not</h3>



<p>We asked our respondents what cloud technologies they’re using. 67% (two-thirds) are using a public cloud provider, such as AWS,  versus 61% in 2020. 45% are using a private cloud—private  infrastructure (on-premises or perhaps hosted) that’s accessed  using cloud APIs—which represents a 10% increase over 2020 (35%). And 55% are using traditionally managed on-premises infrastructure, as opposed to 49% last year.</p>



<p>Respondents could select multiple answers, and many did. It’s not surprising that so many organizations appear to be using  on-prem infrastructure; we’re actually surprised that the number isn’t higher, since in any cloud transformation, the remnants of traditional infrastructure are necessarily the last thing to disappear. And most companies aren’t that far along in their transformations. Moving to the cloud may be an important goal, and cloud resources are probably already providing critical infrastructure. But eliminating all (or even most) traditional infrastructure is a very heavy lift.</p>



<p>That’s not the whole story. 29% of the respondents said they’re using a hybrid cloud (a significant drop from last year’s 39%), and 23% are using multicloud (roughly the same year over year). A multicloud strategy builds systems that run across multiple cloud providers. Hybrid cloud goes even farther, incorporating private cloud infrastructure (on-premises or hosted) running cloud APIs. When done correctly, multiclouds and hybrid clouds can provide continuity in the face of provider outages, the ability to use “the best tool for the job” on different application workloads (for example, leveraging Google Cloud’s AI facilities), and easier regulatory compliance (because sensitive workloads can stay on private systems).</p>



<p>Therefore, respondents who selected “hybrid cloud” should also have selected “public cloud” and “private cloud” (or, possibly, “traditional infrastructure”). Indeed, that’s what we saw. Only 11% of the respondents who selected “hybrid cloud” didn’t select any other types—and we’d bet that’s because they assumed that “hybrid” implied the others. 27% of those who selected “hybrid” selected all five types, and the remainder selected some combination of the five options. The same was true for respondents who selected “multicloud”: only 4% selected “multicloud” by itself. (“Multicloud” and “hybrid cloud” were frequently selected together.) </p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0111-1.png" alt="" class="wp-image-14141" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0111-1.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0111-1-300x241.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0111-1-768x617.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Cloud technology usage</figcaption></figure>



<p>We’re puzzled by the difference between 2020 and 2021. An  increase in the use of public clouds, private clouds, and even  traditional infrastructure makes sense: users have clearly become more comfortable mixing and matching infrastructure to suit their needs. We don’t expect the use of traditional infrastructure to  disappear. But why did usage of hybrid clouds drop? We don’t  have a good answer, except to note that many respondents (indeed, a third of the total) who didn’t select either “multicloud” or “hybrid cloud” still selected multiple infrastructure types. A combination of public cloud and traditional infrastructure was most common, followed by public cloud, private cloud, and traditional infrastructure. These mixtures indicate that many respondents are clearly using some form of multicloud or hybrid cloud, even if they aren’t including that in their responses. </p>



<p>We can’t ignore the slip in the percentage of respondents answering “hybrid cloud.” That may indicate some skepticism about this most ambitious cloud architecture. It could even be an artifact of the  pandemic. We’ve already said that the pandemic gave many  companies a good reason to move on-premises infrastructure to the cloud. But while the pandemic may have been a good time to start cloud transformations, it was arguably a poor time to start very ambitious projects. Can we imagine CTOs saying, “Yes, we’re moving to the cloud, but we’re keeping it as simple as possible” Definitely.</p>



<p>We also looked at what types of cloud technologies were attractive  to different industries. Public clouds are most heavily used in retail &amp; ecommerce (79%), media &amp; entertainment (73%), and software (72%). Hybrid clouds are strongest in consulting &amp; professional services (38%), possibly because consultants often play a role in integrating different cloud providers into a seamless whole.Private clouds are strongest in telecommunications (64%), which was the only sector in which private clouds led public clouds (60%). </p>



<p>Traditionally managed on-premises infrastructure is most widely  used in government (72%). Other industries where the number of respondents using traditional infrastructure equaled or exceeded  the number reporting any form of cloud infrastructure included  higher education (61%), healthcare (61%), telecommunications (67%), computers (65%), and electronics &amp; hardware (58%). </p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0112-1.png" alt="" class="wp-image-14142" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0112-1.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0112-1-300x242.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0112-1-768x619.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Cloud technology usage by industry</figcaption></figure>



<p>When asked about their organization’s cloud strategy, almost half  of the respondents (47%) answered “cloud first,” meaning that  wherever possible, new initiatives consider the cloud as the first  option. Only 5% selected “cloud repatriation,” or bringing services that were previously moved to the cloud back in-house. 10%  indicated a multicloud strategy, where they work with multiple  public cloud providers; and 9% indicated that their strategy is to  use software-as-a-service cloud providers where possible (e.g.,  specific applications from companies like Salesforce and SAP),  thus minimizing the need to develop their own in-house cloud  expertise. Since “Buy before build; only build software related to  your core competence” is an important principle in any digital  transformation, we expected to see a greater investment in  software-as-a-service products. Perhaps this number really means that almost any company will need to build software around its  core value proposition. </p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0113-1-747x1048.png" alt="" class="wp-image-14143" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0113-1-747x1048.png 747w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0113-1-214x300.png 214w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0113-1-768x1078.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0113-1.png 962w" sizes="(max-width: 747px) 100vw, 747px" /><figcaption>Cloud migration strategies</figcaption></figure>



<p>Our respondents approach cloud migration aggressively.  Almost half (48%) said they plan to migrate 50% or more of their  applications to the cloud in the coming year; the largest group (20%) said they plan to migrate 100% of their applications. (We wonder if, for many of these respondents, a migration was already in progress.) 16% said they plan to migrate 25% of their applications. 36% answered “not applicable,” which may mean that they aren’t using the cloud (though this would indicate much lower cloud usage than other questions do) or that the respondent’s organization has already moved its applications to the cloud. It’s probably a  combination of both.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0114-1-621x1048.png" alt="" class="wp-image-14144" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0114-1-621x1048.png 621w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0114-1-178x300.png 178w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0114-1-768x1296.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0114-1.png 800w" sizes="(max-width: 621px) 100vw, 621px" /></figure>



<p>When asked specifically about cloud native development (building software to run in a scalable cloud environment, whether that cloud is public, private, or hybrid), there was an even split between those who have no plan to go cloud native, respondents representing  businesses that are already 100% cloud native, and respondents who thought they would be cloud native at some point in the future. Each group was (very) roughly a third of the total. Looking in more detail at respondents who are in the process of going cloud native, only 6% expect to be cloud native within a year. The largest group (20%) said they’d be cloud native within three or more years,  indicating a general direction, if not a specific goal. </p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0115-1-640x1048.png" alt="" class="wp-image-14145" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0115-1-640x1048.png 640w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0115-1-183x300.png 183w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0115-1-768x1258.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0115-1.png 824w" sizes="(max-width: 640px) 100vw, 640px" /><figcaption>Going cloud native</figcaption></figure>



<p>Does the 67% who are planning to be or are already cloud native  conflict with the 47% who said that they’re pursuing a cloud first strategy? It’s jarring—“cloud native” is, if anything, a stronger  statement than “cloud first.” Presumably anyone who works for an organization that’s already cloud native is also pursuing a cloud first strategy. Some of the gap disappears if we include respondents  executing a multicloud strategy in the “cloud first” group, which brings the “cloud first” total to 57%. After all, “cloud native” as  defined by Wikipedia explicitly includes hybrid clouds. </p>



<p>Perhaps more to the point: there’s a lot of latitude in how respondents might interpret slogans and buzzwords like “cloud native” and “cloud first.” Someone who says that their organization will be “cloud native” at some point in the future (whether it takes one year, two years, or three or more years) isn’t saying that there aren’t significant noncloud projects in progress, and three or more years hardly sets an ambitious goal. But regardless of how respondents may have understood these terms, it’s clear that a substantial majority are moving in a direction  that places all of their workloads in the cloud.</p>



<h3>Cost and other issues</h3>



<p>Survey respondents consistently reported that cost is a concern. When asked about the most important initiatives in their organizations pertaining to public cloud adoption, 30% of all respondents said “managing cost.” Other important cloud projects include performance optimization (13%), modernizing applications (19%), automating  compliance and governance (10%), and cloud migration itself (11%). Only 6% listed migrating to a multicloud strategy as an issue—surprising given the large number who said they’re pursuing  hybrid cloud or multicloud strategies.</p>



<p>These results were similar across all industries. In almost every sector, managing cost was perceived as the most important cloud initiative. Government and finance &amp; banking were outliers; in these sectors, modernizing applications was a greater concern than cost management. (23% of the respondents in the government sector listed modernization as the most important initiative, as did 21%  of the respondents from finance &amp; banking.) </p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0116-2-656x1048.png" alt="" class="wp-image-14147" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0116-2-656x1048.png 656w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0116-2-188x300.png 188w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0116-2-768x1227.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0116-2.png 845w" sizes="(max-width: 656px) 100vw, 656px" /><figcaption>Most important initiatives that organizations will be tackling</figcaption></figure>



<p>Among respondents who aren’t using cloud computing, 21%  said that regulatory requirements require them to keep data on-premises; 19% said that cost is the most important factor; and 19% were concerned with the risk of migration. Relatively few were concerned about the availability of talent (6%, in sharp contrast to our 2021 Data/AI Salary Survey results), and 5% said vendor lock-in  is a concern. Again, this aligns well with our results from 2020:  keeping data on-premises was the most common reason for  <br> not using cloud computing, and cost was the second, followed  by migration risk. </p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0117-1-638x1048.png" alt="" class="wp-image-14148" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0117-1-638x1048.png 638w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0117-1-183x300.png 183w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0117-1-768x1261.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0117-1.png 822w" sizes="(max-width: 638px) 100vw, 638px" /><figcaption>Reasons organizations are not using cloud computing</figcaption></figure>



<p>Why is cost such a critical factor? It’s easy to get into cloud computing on a small scale: building some experimental apps in the cloud because you can rent time using a company credit card rather than going through IT for more resources. If successful, those applications become real software investments that you need to support. They start to require more resources, and as the scale grows, you find  that your cloud provider is getting the “economies of scale,” not  you. Cloud providers certainly know how to set pricing so that it’s easy to move in but difficult to move out. </p>



<p>So, yes, cost needs to be managed. And one way to manage cloud cost is to stay away. You’re not locked into a vendor if you don’t have  a vendor. But that’s a simplistic answer. Good cost management needs to account for the true benefits of moving to the cloud, which usually don’t involve cost. By analogy, assembly lines didn’t minimize the cost of building a factory; they made factories more effective. The cloud’s ability to scale quickly to handle sudden changes in workload is worth a lot. Do your applications suddenly become sluggish if there’s a sudden spike in load, and does that cause you to lose sales? In 2021, “Please be patient; our systems are experiencing heavy load” tells customers to go elsewhere. Improved uptime is also worth a lot; cloud providers have multiple data centers and backup power that most businesses can’t afford. (At O’Reilly, we learned this firsthand during the California fires of 2019, which disabled our on-premises infrastructure. We’re now 100% in the cloud, and we’re sure other companies learned the same lesson.)</p>



<p>Regulatory requirements are a big concern for respondents who aren’t using cloud computing (21%). When we look at respondents as a whole, though, we see something different. In most industries, roughly 10% of the respondents listed regulatory compliance as the most important initiative. The most notable outliers were finance  &amp; banking (15%), government (19%), and healthcare (19%)—but compliance still wasn’t the biggest concern for these industries.  Respondents from the higher education sector reported little concern about compliance (4.8%). Other industries that were relatively unconcerned about compliance included electronics &amp;  hardware and media &amp; entertainment (both 3.8%). Although we’re surprised by the responses from higher education, on the whole, these observations  make sense: compliance is a big issue for industries that are heavily regulated and less of an issue for industries that aren’t. However, it’s also important to observe that concern about compliance isn’t preventing heavily regulated industries from moving to the cloud. Again, regulatory compliance is a concern—but that concern is trumped by the need to provide new kinds of applications.</p>



<h3>Skills</h3>



<p>Although only 6% of the respondents who aren’t using cloud  computing said that skill availability was an issue, we’re skeptical about that—if you’re not moving to the cloud, you don’t need cloud skills. We got a different answer when we asked  all respondents what skills were needed  to develop their cloud infrastructure.  (For this question, respondents were  allowed to choose multiple options.)  The most common response was “cloud-based security” (59%; over half of the respondents), with “general cloud knowledge” second (54%). That’s a good sign. Organizations are finally waking up to the fact that security is essential, not something that’s added on if there’s time. </p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0118-1.png" alt="" class="wp-image-14149" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0118-1.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0118-1-300x199.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0118-1-768x509.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Skills needed to develop a cloud infrastructure</figcaption></figure>



<p>Perhaps the biggest thing to learn from this question, though, is  that over 35% of the respondents selected all of the skills (except other”). Most of them were around 45%. Containers (46%), Kubernetes (44%), microservices (45%), compliance (38%), monitoring (51%), observability (40%), scaling (41%), and performance (44%)  are all in this territory. Our respondents appear to be saying that they need  everything. All the skills. There’s definitely a shortage in cloud expertise. In our recently published<a rel="noreferrer noopener" href="https://learning.oreilly.com/library/view/2021-data-ai-salary/9781098118662/" target="_blank"> <em>2021 Data/AI Salary Survey</em></a> report, we noted that cloud certifications were most associated with salary increases. That says a lot: there’s demand, and employers are willing to pay for talent.</p>



<h3>Portability between clouds</h3>



<p>Our final question looked forward to the next generation of cloud computing. We asked about the barriers to moving workloads  freely between cloud deployment platforms: what it takes to  move applications seamlessly from one cloud to another, to a private cloud, and even to traditional infrastructure. That’s really the goal of a hybrid cloud. </p>



<p>Application portability and security were  the biggest concerns (both 24%). The need for portability is obvious. But what may lie behind concern over portability is the string  of development platforms that have promised application portability, going back at least to Digital Equipment’s CORBA in 1991 (and possibly much earlier). Containers and container  orchestration are themselves “write once, run anywhere”  technologies. Web Assembly (Wasm) is the current trendy  attempt to find this holy grail; we’ll find out in the coming  years whether it suffices. </p>



<p>Security on one platform is hard enough, and writing software  that’s secure across multiple execution environments is much more difficult. With the increasing number of high-profile attacks against large businesses, executives have a right  to be concerned. At the same time,  every security expert we’ve talked to has emphasized that the most important thing any organization can do is to pay attention to the basics: authentication, authorization, software update, backups, and other aspects of security hygiene. In the cloud, the tools and techniques used to implement those  basics are different and arguably more  complex, but the basics remain the same. </p>



<p>Other concerns all clustered around 10%: the most significant include data portability (12%), important and often overlooked; the cost of moving data out of one cloud provider into another (9%),  a concern we saw elsewhere in this study; managing compliance  and, more generally, managing workloads at scale across multiple platforms (both 8%); and visibility into application performance (7%).</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0119-1.png" alt="" class="wp-image-14150" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0119-1.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0119-1-300x242.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/12/0119-1-768x620.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Barriers to moving applications between clouds</figcaption></figure>



<h3> Until next year</h3>



<p>What did we learn? Cloud adoption continues, and it doesn’t  appear to have been affected by the pandemic. Roughly 90% of  our respondents work for organizations that are moving applications  to the cloud. This percentage is only slightly larger than last year (88%) and may not even be significant. (But keep in mind that when you’re at 90%, any further gains come with great difficulty. In practice, 90% is about as close to “everybody” as you can get.)</p>



<p>We also believe that we’re only at the  beginning of cloud adoption. Our audience  is technically sophisticated, and they’re more likely to be cloud adopters. A large majority of the respondents are in the process of moving applications to the cloud, indicating that “cloud” is a work in progress. It’s clearly a situation in which the more you do, the more you see that can be done. The more workloads you move to the cloud, the more you realize that other workloads can move. More important, the more comfortable you are with the cloud, the more innovative you can be in pushing your digital transformation even further. </p>



<p>Concerns about compliance remain significant. Not surprisingly, those concerns are higher among respondents who aren’t using the cloud than among those who are. That’s natural—but looking at the rapid pace of cloud adoption in heavily regulated industries like finance &amp; banking makes us think that “compliance” is more of an excuse for noncloud users than a real concern. Yes, compliance is an issue, but it’s an issue  that many organizations are solving.</p>



<p>Managing costs is obviously an important concern, and unlike compliance, it’s a  concern ranked more highly by cloud users than nonusers. That’s both normal and healthy. The common perception that cloud computing is inexpensive isn’t reality. Being able to allocate a few servers or high-powered compute engines with a credit card is certainly an inexpensive way to start a project, but those savings evaporate at enterprise scale. The cloud provider will reap the economies of scale. For a user, the cloud’s  real advantages aren’t in cost savings but in flexibility, reliability, and scaling. </p>



<p>Finally, cloud skills are in demand across the board. General skills, specific expertise in areas like security, microservices, containers, and orchestration—all are needed. Whether or not there’s a shortage of cloud expertise in the job market, this is an excellent time to pursue training opportunities. Many organizations are dealing with the question of whether to hire or train their staff for new capabilities. When pursuing a cloud transformation, it makes eminent sense to rely on developers who already understand your business and your applications. Hiring new talent is always important, but giving your current staff new skills may be more productive in the long run. If your business is going to be a cloud business, then your software developers need to become cloud developers.</p>



<hr class="wp-block-separator" />



<h3>Footnote</h3>



<ol><li>A brief note about precision. We’ve rounded percentages to the nearest 1%, except in some cases where the percentage is small (under 10%). With nearly 3,000 respondents, 0.1% only represents 3 users, and we’d argue that drawing conclusions based on a difference of a percentage point or two is misusing the data.</li></ol>



<p></p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/the-cloud-in-2021-adoption-continues/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>The Sobering Truth About the Impact of Your Business Ideas</title>
		<link>https://www.oreilly.com/radar/the-sobering-truth-about-the-impact-of-your-business-ideas/</link>
				<comments>https://www.oreilly.com/radar/the-sobering-truth-about-the-impact-of-your-business-ideas/#respond</comments>
				<pubDate>Tue, 26 Oct 2021 13:07:58 +0000</pubDate>
		<dc:creator><![CDATA[Eric Colson, Daragh Sibley and Dave Spiegel]]></dc:creator>
				<category><![CDATA[Business]]></category>
		<category><![CDATA[Operations]]></category>
		<category><![CDATA[Research]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=14041</guid>
				<description><![CDATA[The introduction of data science into the business world has contributed far more than recommendation algorithms; it has also taught us a lot about the efficacy with which we manage our businesses. Specifically, data science has introduced rigorous methods for measuring the outcomes of business ideas. These are the strategic ideas that we implement in [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>The introduction of data science into the business world has contributed far more than recommendation algorithms; it has also taught us a lot about the efficacy with which we manage our businesses. Specifically, data science has introduced rigorous methods for measuring the outcomes of business ideas. These are the strategic ideas that we implement in order to achieve our business goals. For example, “We&#8217;ll lower prices to increase demand by 10%” and “we’ll implement a loyalty program to improve retention by 5%.” Many companies simply execute on their business ideas without measuring if they delivered the impact that was expected. But, science-based organizations are rigorously quantifying this impact and have learned some sobering lessons:</p>



<ol><li>The vast majority of business ideas fail to generate a positive impact.</li><li>Most companies are unaware of this.</li><li>It is unlikely that companies will increase the success rate for their business ideas.</li></ol>



<p>These are lessons that could profoundly change how businesses operate. In what follows, we flesh out the three assertions above with the bulk of the content explaining why it may be difficult to improve the poor success rate for business ideas. Despite the challenges, we conclude with some recommendations for better managing your business.</p>



<h3>(1) The vast majority of business ideas fail to generate positive results </h3>



<p>To properly measure the outcomes of business ideas, companies are embracing experimentation (a.k.a. randomized controlled trials or A/B testing). The process is simple in concept. Before rolling out a business idea, you test; you try the idea out on a subset group of customers<a href="#footnote1"><sup>1</sup></a> while another group—a control group—is not exposed to the new idea. When properly sampled, the two groups will exhibit the same attributes (demographics, geographics, etc.) and behaviors (purchase rates, life-time-value, etc.). Therefore, when the intervention is introduced—ie. the exposure to the new business idea—any changes in behavior can be causally attributed to the new business idea. This is the gold standard in scientific measurement used in clinical trials for medical research, biological studies, pharmaceutical trials, and now to test business ideas.</p>



<p>For the very first time in many business domains, experimentation reveals the causal impact of our business ideas. The results are humbling. They indicate that the vast majority of our business ideas fail to generate positive results. <strong>It&#8217;s not uncommon for 70-90% of ideas to</strong> <strong>either have no impact at all or actually move the metrics in the opposite direction of what was intended. </strong>Here are some statistics from a few notable companies that have disclosed their success rates publicly:</p>



<ul><li>Microsoft declared that roughly one-third of their ideas yield negative results, one-third yield no results, and one-third yield positive results (Kohavi and Thomke, 2017).</li><li>Streaming service Netflix believes that 90% of its ideas are wrong (Moran, 2007). </li><li>Google reported that as much as 96.1% of their ideas fail to generate positive results (Thomke, 2020).</li><li>Travel site Booking.com shared that 9 out of 10 of their ideas fail to improve metrics (Thomke, 2020).</li></ul>



<p>To be sure, the statistics cited above reflect a tiny subset of the ideas implemented by companies. Further, they probably reflect a particular class of ideas: those that are conducive to experimentation<a href="#footnote2"><sup>2</sup></a> such as changes to user interfaces, new ad creatives, subtle messaging variants, and so on. Moreover, the companies represented are all relatively young and either in the tech sector or leverage technology as a medium for their business. This is far from a random sample of all companies and business ideas. So, while it&#8217;s possible that the high failure rates are specific to the types of companies and ideas that are convenient to test experimentally, it seems more plausible that the high failure rates are reflective of business ideas in general and that the disparity in perception of their success can be attributed to the method of measurement. We shouldn’t be surprised; high failure rates are common in many domains. Venture capitalists invest in many companies because most fail; similarly, most stock portfolio managers fail to outperform the S&amp;P 500; in biology, most mutations are unsuccessful; and so on. The more surprising aspect of the low success rates for business ideas is most of us don’t seem to know about it.</p>



<h3>(2) Most companies are unaware of the low success rates for their business ideas</h3>



<p>Those statistics should be sobering to any organization. Collectively, business ideas represent the roadmap companies rely upon to hit their goals and objectives. However, the dismal failure rates appear to be known only to the few companies that regularly conduct experiments to scientifically measure the impact of their ideas. Most companies do not appear to employ such a practice and seem to have the impression that all or most of their ideas are or will be successful. Planners, strategists, and functional leaders rarely convey any doubts about their ideas. To the contrary, they set expectations on the predicted impact of their ideas and plan for them as if they are certain. They attach revenue goals and even their own bonuses to those predictions. <strong>But, how much do they really know about the outcomes of those ideas?</strong> If they don’t have an experimentation practice, they likely know very little about the impact their roadmap is actually having.</p>



<p>Without experimentation, companies either don’t measure the outcomes of their ideas at all or use flimsy methods to assess their impacts. In some situations, ideas are acted upon so fluidly that they are not recognized as something that merits measurement.&nbsp; For example, in some companies an idea such as “we’ll lower prices to increase demand by 10%” might be made on a whim by a marketing exec and there will be no follow up at all to see if it had the expected impact on demand. In other situations, a post-implementation assessment of a business idea is done, but in terms of <em>execution</em>, not impact (“Was it implemented on time?” “Did it meet requirements?” etc., not “What was the causal impact on business metrics?”). In other cases still, post hoc analysis is performed in an attempt to quantify the impact of the idea. But, this is often done using subjective or less-than-rigorous methods to justify the idea <em>as a success</em>. That is, the team responsible for doing the analysis often is motivated either implicitly or explicitly to find evidence of success. Bonuses are often tied to the outcomes of business ideas. Or, perhaps the VP whose idea it was is the one commissioning the analysis. In either case, there is a strong motivation to find success. For example, a company may seek qualitative customer feedback on the new loyalty program in order to craft a narrative for how it is received. Yet, the customers willing to give feedback are often biased towards the positive. Even if more objective feedback were to be acquired it would still not be a measure of impact; customers often behave differently from the sentiments they express. In still other cases, empirical analysis is performed on transaction data in an attempt to quantify the impact. But, without experimentation, at best, such analysis can only capture correlation—not causation. Business metrics are influenced simultaneously by many factors, including random fluctuations. Without properly controlling for these factors, it can be tempting to attribute any uptick in metrics as a result of the new business idea. The combination of malleable measurements and strong incentives to show success likely explain why so many business initiatives are perceived to be successful.</p>



<p>By contrast, the results of experimentation are numeric and austere. They do not care about the hard work that went into executing on a business initiative. They are unswayed by well-crafted narratives, emotional reviews by customers, or an executive&#8217;s influence. In short, they are brutally honest and often hard-to-accept.<a href="#footnote3"><sup>3</sup></a> Without experimentation, companies don&#8217;t learn the sobering truth about their high failure rate. While ignorance is bliss, it is not an effective way to run your business.</p>



<h3>(3) It is unlikely that companies will increase the success rate for their business ideas.</h3>



<p>At this point, you may be thinking, “we need to get better at separating the wheat from the chaff, so that we only allocate resources to the <em>good</em> ideas.” Sadly, without experimentation, we see little reason for optimism as there are forces that will actively work against your efforts.</p>



<h4><strong>One force that is actively working against us is the way we reason about our companies. </strong></h4>



<p>We like to reason about our businesses as if they are simple, predictable systems. We build models of their component parts and manage them as if they are levers we can pull in order to predictably manage the business to a desired state. For example, a marketer seeking to increase demand builds a model that allows her to associate each possible price with a predicted level of demand. The scope of the model is intentionally narrow so that she can isolate the impact price has on demand. Other factors like consumer perception, the competitive assortment, operational capacity, the macroeconomic landscape, and so on are out of her control and assumed to remain constant. Equipped with such an intuitive model, she can identify the price that optimizes demand. She’s in control and hitting her goal is merely a matter of execution.</p>



<p>However, experimentation reveals that our predictions for the impact of new business ideas can be radically off—not just a little off in terms of magnitude, but often in the completely wrong direction. We lower prices and see demand go <em>down</em>. We launch a new loyalty program and it <em>hurts</em> retention. Such unintuitive results are far more common than you might think.</p>



<p>The problem is that many businesses behave as complex systems which cannot be understood by studying its components in isolation. Customers, competitors, partners, market force—each can adjust in response to the intervention in ways that are not observable from simple models of the components. Just as you can’t learn about an ant colony by studying the behaviors of an individual ant (Mauboussin, 2009), the insights derived from modeling individual components of a business in isolation often have little relevance to the way the business behaves as a whole.</p>



<p>It’s important to note that our use of the term <em>complex</em> does not just mean ‘not simple.’ <em>Complexity</em> is an entire area of research within Systems Theory. Complexity arises in systems with many interacting agents that react and adapt to one another and their environment. Examples of complex systems include weather systems, rain forest ecology, economies, the nervous system, cities, and yes, many businesses.</p>



<p>Reasoning about complex systems requires a different approach. Rather than focusing on component parts, attention needs to be directed at system-wide behaviors. These behaviors are often termed “emergent,” to indicate that they are very hard to anticipate. This frame orients us around learning, not executing. It encourages more trial and error with less attachment to the outcomes of a narrow set of ideas. As complexity researcher Scott E. Page says, “An actor in a complex system controls almost nothing but influences almost everything” (Page, 2009).</p>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-block-group has-very-light-gray-background-color has-background"><div class="wp-block-group__inner-container">
<div class="wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-block-group"><div class="wp-block-group__inner-container">
<h4><strong>An example of an attempt to manage a complex system to change behaviors</strong></h4>
</div></div>
</div></div>
</div></div>
</div></div>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-block-group"><div class="wp-block-group__inner-container">
<blockquote class="wp-block-quote"><p>To make this tangible let’s take a look at a real example. Consider the story of the child daycare company featured in the popular book, <em>Freakonomics</em> (the original paper can be found <a href="https://rady.ucsd.edu/faculty/directory/gneezy/pub/docs/fine.pdf">here</a>). The company faced a challenge with late pickups. The daycare closed at 4:00pm, yet parents would frequently pick up their children several minutes later. This required staff to stay late causing both expense and inconvenience. Someone in the company had a business idea to address the situation: a fine for late pickups. </p></blockquote>
</div></div>
</div></div>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-block-group"><div class="wp-block-group__inner-container">
<blockquote class="wp-block-quote"><p>Many companies would simply implement the fine and not think to measure the outcome. Fortunately for the daycare, a group of researchers convinced them to run an experiment to measure the effectiveness of the policy. The daycare operates many locations which were randomly divided into test and control groups; the test sites would implement the late pickup fine while the control sites would leave things as is. The experiment ran its course and to everyone’s surprise they learned that fine actually <em>increased</em> the number of late pickups.</p></blockquote>
</div></div>
</div></div>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-block-group"><div class="wp-block-group__inner-container">
<blockquote class="wp-block-quote"><p>How is it possible that the business idea had the opposite effect of what was intended? There are several very plausible explanations, which we summarize below—some of these come from the paper while others are our own hypotheses.</p></blockquote>
</div></div>
</div></div>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-block-group"><div class="wp-block-group__inner-container">
<ul><li><p>The authors of the paper assert that imposing a fine makes the penalty for a late pick up explicitly clear. Parents are generally aware that late pick-ups are not condoned. But in the absence of a fine, they are unsure what the penalty may be. Some parents may have imagined a penalty much worse than the fine—e.g., expulsion from the daycare. This belief might have been an effective deterrent. But when the fine was imposed it explicitly quantified that amount of the penalty for the late pickups (roughly equivalent to $2.75 in 1998 dollars). For some parents this was a sigh of relief—expulsion was not on the docket. One merely has to pay a fine for the transgression, making the cost of a late pickup less than what was believed. Hence, late pick-ups increase (Gneezy &amp; Rustichini, 2000).</p></li></ul>
</div></div>
</div></div>
</div></div>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-block-group"><div class="wp-block-group__inner-container">
<ul><li><p>Another explanation from the paper involves social norms. Many parents may have considered late pickups as socially inappropriate and would therefore go through great lengths to avoid them (leaving work early, scrambling for backup coverage, etc). The fine however, provides an easier way to stay in good social standing. It’s as if it signals <em>‘late pickups are not condoned. But if you pay us the fine you are forgiven</em>.<em>’</em> Therefore, the fine acts as the price to pay to stay in good standing. For some parents this price is low relative to the arduous and diligent planning required to prevent a late pickup. Hence, late pickups increase in the presence of the fine (Gneezy &amp; Rustichini, 2000).</p></li></ul>
</div></div>
</div></div>
</div></div>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-block-group"><div class="wp-block-group__inner-container">
<ul><li><p>Still another explanation (which was only alluded to in the paper) has to do with the perceived cost structure associated with the staff having to stay late. From the parent’s perspective, the burden to the daycare of a late pickup might be considered fixed. If there is already at least one other parent also running late then there is no extra burden imposed since staff already has to stay. As surmised by the other explanations above, the fine increases the number of late pickups, which, therefore increases the probability that staff will have to stay late due to some other parent’s tardiness. Thus, one extra late pickup is no additional burden. Late pickups increase further.</p></li></ul>
</div></div>
</div></div>
</div></div>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-block-group"><div class="wp-block-group__inner-container">
<ul><li><p>One of our own explanations has to do with social norms thresholds. Each parent has a threshold for the appropriateness for late pickups based on social norms. The threshold might be the number of other parents observed or believed to be doing late pickups before such activity is believed to be appropriate. I.e., <em>if others are doing it, it must be okay. </em>(Note: this signal of appropriateness is independent from the perceived fixed cost structure mentioned above.) Since the fine increased the number of late pickups for some parents, other parents observed more late pickups and then followed suit.</p></li></ul>
</div></div>
</div></div>
</div></div>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-block-group"><div class="wp-block-group__inner-container">
<blockquote class="wp-block-quote"><p>The above are plausible explanations for the observed outcome. Some may even seem obvious in hindsight.<a href="#footnote4"><sup>4</sup></a> However, these behaviors are extremely difficult to anticipate by focusing your attention on an individual component part: the fine.&nbsp; Such surprising outcomes are less rare than you might think. In this case, the increase in late pickups might have been so apparent that they could have been detected even without the experiment. However, the impact of many ideas often go undetected.</p></blockquote>
</div></div>
</div></div>
</div></div>
</div></div>
</div></div>



<p><strong>Another force that is actively working against our efforts to discern good ideas from bad is our cognitive biases.&nbsp;</strong>You might be thinking: “Thank goodness my company has processes that filter away bad ideas, so that we only invest in great ideas!” Unfortunately, all companies probably try hard to select only the best ideas, and yet we assert that they are not particularly successful at separating good from bad ideas. We suggest that this is because these processes are deeply human in nature, leaving them vulnerable to cognitive biases.</p>



<p>Cognitive biases are systematic errors in human thinking and decision making (Tversky &amp; Kahneman, 1974). They result from the core thinking and decision making processes that we developed over our evolutionary history. Unfortunately, evolution adapted us to an environment with many differences from the modern world. This can lead to a habit of poor decision making. To illustrate: we know that a healthy bundle of kale is better for our bodies than a big juicy burger. Yet, we have an innate preference for the burger. Many of us will decide to eat the burger tonight. And tomorrow night. And again next week. We know we shouldn’t. But yet our society continues consuming too much meat, fat, and sugar. Obesity is now a major public health problem. Why are we doing this to ourselves? Why are we imbued with such a strong urge—a literal gut instinct—to repeatedly make decisions that have negative consequences for us? It’s because meat, fat, and sugar were scarce and precious resources for most of our evolutionary history. Consuming these resources at every opportunity was an adaptive behavior, and so humans evolved a strong desire to do so. Unfortunately, we remain imbued with this desire despite the modern world’s abundance of burger joints.</p>



<p>Cognitive biases are predictable and pervasive. We fall prey to them despite believing that we are rational and objective thinkers. Business leaders (ourselves included) are not immune. These biases compromise our ability to filter out bad business ideas. They can also make us feel extremely confident as we make a bad business decision. See the following sidebar for examples of cognitive biases manifesting in business environments and producing bad decisions.</p>



<div class="wp-block-group has-very-light-gray-background-color has-background"><div class="wp-block-group__inner-container">
<div class="wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-block-group"><div class="wp-block-group__inner-container">
<h4 class="has-text-align-center">Cognitive bias examples</h4>
</div></div>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<blockquote class="wp-block-quote"><p>Group Think (Whyte, 1952) describes our tendency to converge towards shared opinions when we gather in groups. This emerges from a very human impulse to conform. Group cohesion was important in our evolutionary past. You might have observed this bias during a prioritization meeting: The group entered with disparate, weakly held opinions, but exited with a consensus opinion, which everyone felt confident about.&nbsp; As a hypothetical example: A meeting is called to discuss a disagreement between two departments. Members of the departments have differing but strong opinions, based on solid lines of reasoning and evidence. But once the meeting starts the attendees begin to self censor. Nobody wants to look difficult. One attendee recognizes a gaping flaw in the “other side’s” analysis, but they don’t want to make their key cross functional partner look bad in front of their boss. Another attendee may have thought the idea was too risky, but, because the responsibility for the idea is now diffused across everyone in the meeting, won&#8217;t be her fault if the project fails and so she acquiesces. Finally, a highly admired senior executive speaks up and everyone converges towards this position (in business lingo we just heard the HiPPO or Highest Paid Person’s Opinion; or in the scientific vernacular, the Authority Bias (Milgram, 1963). These social pressures will have collectively stifled the meaningful debate that could have filtered out a bad business decision.</p></blockquote>
</div></div>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<blockquote class="wp-block-quote"><p>The Sunk Cost bias (Arkes &amp; Blumer, 1985) describes our tendency to justify new investments via past expenditures. In colloquial terms, it’s our tendency to throw good money after bad. We suspect you’ve seen this bias more than a few times in the workplace. As another hypothetical example: A manager is deciding what their team will prioritize over the next fiscal year. They naturally think about incremental improvements that they could make to their team&#8217;s core product. This product is based on a compelling idea, however, it hasn’t yet delivered the impact that everyone expected. But, the manager has spent so much time and effort building organizational momentum behind the product. The manager gave presentations about it to senior leadership and painstakingly cultivated a sense of excitement about it with their cross functional partners. As a result, the manager decides to prioritize incremental work on the existing product, without properly investigating a new idea that would have yielded much more impact. In this case, the manager’s decision was driven by thinking about the sunk costs associated with the existing system. This created a barrier to innovation and yielded a bad business decision.</p></blockquote>
</div></div>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<blockquote class="wp-block-quote"><p>The Confirmation Bias (Nickerson, 1998) describes our tendency to focus upon evidence that confirms our beliefs, while discounting evidence that challenges our beliefs. We’ve certainly fallen prey to this bias in our personal and professional lives. As a hypothetical example: An exec wonders <em>&#8216;should we implement a loyalty program to improve client retention?&#8217; </em>They find a team member who thinks this sounds like a good idea. So the exec asks the team member to do some market research to inform whether the company should create their own loyalty program. The team member looks for examples of highly successful loyalty programs from other companies. Why look for examples of bad programs? This company has no intention of implementing a bad loyalty program. Also, the team member wants to impress the exec by describing all the opportunities that could be unlocked with this program. They want to demonstrate their abilities as a strategic thinker. They might even get to lead the implementation of the program, which could be great for their career. As a result, the team member builds a presentation that emphasizes positive examples and opportunities, while discounting negative examples and risks. This presentation leads the exec to overestimate the probability that this initiative will improve client retention, and thus fail to filter out a bad business decision.</p></blockquote>
</div></div>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<blockquote class="wp-block-quote"><p>The biases we’ve listed above are just a sample of the extensive and well documented set of cognitive biases (e.g., Availability Bias, Survivorship Bias, Dunning-Kruger effect, etc.) that limit business leaders&#8217; ability to identify and implement only successful business initiatives. Awareness of these biases can decrease our probability of committing them. However, awareness isn’t a silver bullet. We have a desk mat in our office that lists many of these cognitive biases. We regret to report that we often return to our desks, stare down at the mat … and realize that we’ve just fallen prey to another bias.&nbsp;<br></p></blockquote>
</div></div>
</div></div>
</div></div>



<p><strong>A final force that is actively working against efforts to discern good ideas from bad is your business maturing. </strong>A thought experiment: Suppose a local high school coach told NBA superstar Stephen Curry how to adjust his jump shot. Would implementing these changes improve or hurt his performance? It is hard to imagine it would help. Now, suppose the coach gave this advice to a local 6th grader. It seems likely that it would help the kid’s game.</p>



<p>Now, imagine a consultant telling Google how to improve their search algorithm versus advising a startup on setting up a database. It’s easier to imagine the consultant helping the startup. Why? Well, Google search is a cutting edge system that has received extensive attention from numerous world class experts—kind of like Steph Curry. It’s going to be hard to offer a new great idea. In contrast, the startup will benefit from getting pointed in a variety of good directions—kind of like a 6th grader.</p>



<p>To use a more analytic framework, imagine a hill which represents a company’s objective function<a href="#footnote5"><sup>5</sup></a> like profit, revenue, or retention. The company’s goal is to climb to the peak, where it’s objective is maximized. However, the company can’t see very far in this landscape. It doesn’t know where the peak is. It can only assess (if it’s careful and uses experimentation) whether it’s going up or downhill by taking small steps in different directions—perhaps by tweaking it’s pricing strategy and measuring if revenue goes up.</p>



<p>When a company (or basketball player) is young, its position on this objective function (profit, etc.) landscape is low. It can step in many directions and go uphill. Through this process, a company can grow (walk up Mount Revenue). However, as it climbs the mountain, a smaller proportion of the possible directions to step will lead uphill. At the summit a step in any direction will take you downhill.</p>



<p>This is admittedly a simple model&nbsp; of a business (and we already discussed the follies of using simple models). However, all companies will eventually face the truism that as they improve, there are fewer ways to continue to improve (the low apples have been plucked), as well as the extrinsic constraints of market saturation, commoditization, etc. that make it harder to improve your business as it matures.<a href="#footnote6"><sup>6</sup></a></p>



<h3>So, what to do</h3>



<p>We’ve argued that most business ideas fail to deliver on their promised goals. We’ve also explained that there are systematic reasons that make it unlikely that companies will get better, just by trying harder. So where does this leave you? Are you destined to implement mostly bad ideas? Here are a few recommendations that might help:</p>



<ol><li><strong>Run experiments and exercise your optionality. </strong>Recognize that your business may be a complex system, making it very difficult to predict how it will respond to your business ideas. Instead of rolling out your new business ideas to all customers, try them on a sample of customers as an experiment. This will show you the impact your idea has on the company. You can then make an informed decision about whether or not to roll out your idea. If your idea has a positive impact, great. Roll it out to all customers. But in the more likely event that your idea does not have the positive impact you were hoping for you can end the experiment and kill the idea. It may seem wasteful to use company resources to implement a business idea only to later kill it, but this is better than unknowingly providing on-going support to an idea that is doing nothing or actually hurting your metrics—which is what happens most of the time.</li><li><strong>Recognize your cognitive biases, collect a priori predictions, and celebrate learnings. </strong>Your company’s ability to filter out bad business ideas will be limited by your team member’s cognitive biases. You can start building a culture that appreciates this fact by sending a survey to all of a project’s stakeholders before your next big release. Ask everyone to predict how the metrics will move. Make an anonymized version of these predictions and accuracy available for employees. We expect your team members will become less confident in their predictions over time. This process may also reveal that big wins tend to emerge from a string of experiments, rather than a single stroke of inspiration. So celebrate all of the necessary stepping stones on the way to a big win.</li><li><strong>Recognize that it&#8217;s going to get harder to find successful ideas, so try more things, and get more skeptical. </strong>As your company matures, it may get harder to find ways to improve it. We see three ways to address this challenge. First, try more ideas. It will be hard to increase the success rate of your ideas, so try more ideas. Consider building a leverageable and reusable experimentation platform to increase your bandwidth. Follow the lead of the venture world: fund a lot of ideas to get a few big wins.<a href="#footnote7"><sup>7</sup></a> Second, as your company matures, you might want to adjust the amount of evidence that is required before you roll out a change—a more mature company should require a higher degree of statistical certainty before inferring that a new feature has improved metrics. In experimental lingo, you might want to adjust the “p-value thresholds” that you use to assess an experiment. Or to use our metaphor, a 6th grader should probably just listen whenever a coach tells them to adjust their jump shot, but Steph Curry should require a lot of evidence before he adjusts his.</li></ol>



<p>This may be a hard message to accept. It&#8217;s easier to assume that all of our ideas are having the positive impact that we intended. It’s more inspiring to believe that successful ideas and companies are the result of brilliance rather than trial and error. But, consider the deference we give to mother nature. She is able to produce such exquisite creatures—the giraffe, the mighty oak tree, even us humans—each so perfectly adapted to their environment that we see them as the rightful owners of their respective niches. Yet, mother nature achieves this not through grandiose ideas, but through trial and error… with a success rate far more dismal than that of our business ideas. It&#8217;s an effective strategy if we can convince our egos to embrace it.</p>



<p>  </p>



<hr class="wp-block-separator" />



<h3 class="has-text-align-center">References</h3>



<p>Arkes, H. R., &amp; Blumer, C. (1985), The psychology of sunk costs. <em>Organizational Behavior and Human Decision Processes, 35,</em> 124-140.</p>



<p>Gneezy, U., &amp; Rustichini, A. (2000). A Fine is a Price. <em>The Journal of Legal Studies</em>, 29(1), 1-17. doi:10.1086/468061</p>



<p>Kahneman, D., &amp; Klein, G. (2009). Conditions for intuitive expertise: A failure to disagree. <em>American Psychologist, 64</em>(6), 515–526. <a href="https://psycnet.apa.org/doi/10.1037/a0016755">https://doi.org/10.1037/a0016755</a></p>



<p>Kohavi, R. &amp; Thomke, S. “The Surprising Power of Online Experiments,” <em>Harvard Business Review</em> 95, no. 5 (September-October 2017)</p>



<p>Mauboussin, M. J. (2009). Think Twice: Harnessing the Power of Counterintuition. <em>Harvard Business Review Press</em>.</p>



<p>Milgram, S. (1963). &#8220;Behavioral Study of obedience&#8221;. <em>The Journal of Abnormal and Social Psychology</em>. 67 (4): 371–378.</p>



<p>Moran, M. Do It Wrong Quickly: How the Web Changes the Old Marketing Rules . s.l. : <em>IBM Press</em>, 2007. 0132255960.</p>



<p>Nickerson, R. S. (1998), &#8220;Confirmation bias: A ubiquitous phenomenon in many guises&#8221;, <em>Review of General Psychology</em>, 2 (2): 175–220.</p>



<p>Page, S. E. (2009). Understanding Complexity &#8211; <em>The Great Courses &#8211; Lecture Transcript and Course Guidebook</em> (1st ed.). The Teaching Company.</p>



<p>Thomke, S. H. (2020). Experimentation Works: The Surprising Power of Business Experiments. <em>Harvard Business Review Press.</em></p>



<p>Tversky, A., &amp; Kahneman, D. (1974). Judgment under uncertainty: Heuristics and biases. <em>Science, 185</em>(4157), 1124-1131.</p>



<p>Whyte, W. H., (1952). “Groupthink”. <em>Fortune</em>, 114-117, 142, 146.</p>



<hr class="wp-block-separator" />



<h3 class="has-text-align-center">Footnotes</h3>



<ol><li id="footnote1">Do not confuse the term ‘test’ to mean a process by which a nascent idea is vetted to get feedback. In an experiment, the test group receives a full-featured implementation of an idea. The goal of the experiment is to measure impact—not get feedback.</li><li id="footnote2">In some cases there may be insufficient sample size, ethical concerns, lack of a suitable control group, and many other conditions that can inhibit experimentation.</li><li id="footnote3">Even trained statisticians can fall victim to pressures to cajole the data. “P-hacking”, “significance chasing” and other terms refer to the temptation to use flawed methods in statistical analysis.</li><li id="footnote4">We believe that these types of factors are only obvious in hindsight because the signals are often unobserved until we know to look for them (Kahneman &amp; Klein, 2009).</li><li id="footnote5">One reason among many why this mental picture is oversimplified is that it implicitly takes business conditions and the world at large to be static—the company “state vector” that maximizes the objective function today is the same as what maximizes the objective function tomorrow.  In other words, it ignores that, in reality, the hill is changing shape under our feet as we try to climb it. Still, it’s a useful toy model.</li><li id="footnote6">Finding a new market (jumping to a new “hill” in the “Mount Revenue” metaphor), as recommended in the next section, is one way to continue improving business metrics even as your company matures.</li><li id="footnote7">VCs are able to learn about the outcomes of the startups even without experimentation. This is because the outcomes are far more readily apparent than that of business ideas. It&#8217;s difficult to cajole results to show a successful outcome when the company is out of business.</li></ol>



<p></p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/the-sobering-truth-about-the-impact-of-your-business-ideas/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>2021 Data/AI Salary Survey</title>
		<link>https://www.oreilly.com/radar/2021-data-ai-salary-survey/</link>
				<comments>https://www.oreilly.com/radar/2021-data-ai-salary-survey/#respond</comments>
				<pubDate>Wed, 15 Sep 2021 11:32:26 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides]]></dc:creator>
				<category><![CDATA[AI & ML]]></category>
		<category><![CDATA[Data]]></category>
		<category><![CDATA[Research]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=13950</guid>
				<description><![CDATA[In June 2021, we asked the recipients of our&#160;Data &#38; AI Newsletter&#160;to respond to a survey about compensation. The results gave us insight into what our subscribers are paid, where they’re located, what industries they work for, what their concerns are, and what sorts of career development opportunities they’re pursuing. While it’s sadly premature to [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>In June 2021, we asked the recipients of our&nbsp;<em>Data &amp; AI Newsletter</em>&nbsp;to respond to a survey about compensation. The results gave us insight into what our subscribers are paid, where they’re located, what industries they work for, what their concerns are, and what sorts of career development opportunities they’re pursuing.</p>



<p>While it’s sadly premature to say that the survey took place at the end of the COVID-19 pandemic (though we can all hope), it took place at a time when restrictions were loosening: we were starting to go out in public, have parties, and in some cases even attend in-person conferences. The results then provide a place to start thinking about what effect the pandemic had on employment. There was a lot of uncertainty about stability, particularly at smaller companies: Would the company’s business model continue to be effective? Would your job still be there in a year? At the same time, employees were reluctant to look for new jobs, especially if they would require relocating—at least according to the rumor mill. Were those concerns reflected in new patterns for employment?</p>



<h2>Executive Summary</h2>



<ul><li>The average salary for data and AI professionals who responded to the survey was $146,000.</li><li>The average change in compensation over the last three years was $9,252. This corresponds to an annual increase of 2.25%. However, 8% of the correspondents reported decreased compensation, and 18% reported no change.</li><li>We don’t see evidence of a “great resignation.” 22% of respondents said they intended to change jobs, roughly what we would have expected. Respondents seemed concerned about job security, probably because of the pandemic’s effect on the economy.</li><li>Average compensation was highest in California ($176,000), followed by Eastern Seaboard states like New York and&nbsp;Massachusetts.</li><li>Compensation for women was significantly lower than for men (84%). Salaries were lower regardless of education or job title. Women were more likely than men to have advanced degrees, particularly PhDs.</li><li>Many respondents acquired certifications. Cloud certifications, specifically in AWS and Microsoft Azure, were most strongly associated with salary increases.</li><li>Most respondents participated in training of some form. Learning new skills and improving old ones were the most common reasons for training, though hireability and job security were also factors. Company-provided training opportunities were most strongly associated with pay increases.</li></ul>



<h2>Demographics</h2>



<p>The survey was publicized through&nbsp;<a href="https://www.oreilly.com/emails/newsletters/">O’Reilly’s&nbsp;<em>Data &amp; AI Newsletter</em></a>&nbsp;and was limited to respondents in the United States and the United Kingdom. There were 3,136 valid responses, 2,778 from the US and 284 from the UK. This report focuses on the respondents from the US, with only limited attention paid to those from the UK. A small number of respondents (74) identified as residents of the US or UK, but their IP addresses indicated that they were located elsewhere. We didn&#8217;t use the data from these respondents; in practice, discarding this data had no effect on the results.</p>



<p>Of the 2,778 US respondents, 2,225 (81%) identified as men, and 383 (14%) identified as women (as identified by their preferred pronouns). 113 (4%) identified as “other,” and 14 (0.5%) used “they.”</p>



<p>The results are biased by the survey’s recipients (subscribers to O’Reilly’s&nbsp;<em>Data &amp; AI Newsletter</em>). Our audience is particularly strong in the software (20% of respondents), computer hardware (4%), and computer security (2%) industries—over 25% of the total. Our&nbsp;audience&nbsp;is also strong in the states where these industries are&nbsp;concentrated: 42% of the US respondents lived in California (20%), New York (9%), Massachusetts (6%), and Texas (7%), though these states only make up 27% of the US population.</p>



<h2>Compensation Basics</h2>



<p>The average annual salary for employees who worked in data or AI was $146,000. Most salaries were between $100,000 and $150,000 yearly (34%); the next most common salary tier was from $150,000 to $200,000 (26%). Compensation depended strongly on location, with average salaries highest in California ($176,000).</p>



<p>The average salary change over the past three years was $9,252, which is 2.25% per year (assuming a final salary equal to the average). A small number of respondents (8%) reported salary decreases, and 18% reported no change. Economic uncertainty caused by the pandemic may be responsible for the declines in compensation. 19% reported increases of $5,000 to $10,000 over that period; 14% reported increases of over $25,000. A&nbsp;<a href="https://spectrum.ieee.org/view-from-the-valley/at-work/tech-careers/us-tech-salaries-climb-says-2021-report">study by the IEEE</a>&nbsp;suggests that the average salary for technical employees increased 3.6% per year, higher than our respondents indicated.</p>



<p>39% of respondents reported promotions in the past three years, and 37% reported changing employers during that period. 22% reported that they were considering changing jobs because their salaries hadn’t increased during the past year. Is this a sign of what some have called a “great resignation”? Common wisdom has it that technical employees change jobs every three to four years.&nbsp;<a href="https://www.linkedin.com/pulse/how-often-should-you-change-jobs-perminus-wainaina/">LinkedIn</a>&nbsp;and&nbsp;<a href="https://www.indeed.com/career-advice/career-development/how-often-should-you-change-job">Indeed</a>&nbsp;both recommend staying for at least three years, though they observe that younger employees change jobs more often. LinkedIn elsewhere states that the&nbsp;<a href="https://www.linkedin.com/business/learning/blog/learner-engagement/see-the-industries-with-the-highest-turnover-and-why-it-s-so-hi">annual turnover rate</a>&nbsp;for technology employees is 13.2%—which suggests that employees stay at their jobs for roughly seven and a half years. If that’s correct, the 37% that changed jobs over three years seems about right, and the 22% who said they “intend to leave their job due to a lack of compensation increase” doesn’t seem overly high. Keep in mind that intent to change and actual change are not the same—and that there are many reasons to change jobs aside from salary, including flexibility around working hours and working from home.</p>



<p>64% of the respondents took part in training or obtained certifications in the past year, and 31% reported spending over 100 hours in training programs, ranging from formal graduate degrees to reading blog posts. As we’ll see later, cloud certifications (specifically in AWS and Microsoft Azure) were the most popular and appeared to have the largest effect on salaries.</p>



<p>The reasons respondents gave for participating in training were surprisingly consistent. The vast majority reported that they wanted to learn new skills (91%) or improve existing skills (84%). Data and AI professionals are clearly interested in learning—and that learning is self-motivated, not imposed by management. Relatively few (22%) said that training was required by their job, and even fewer participated in training because they were concerned about losing their&nbsp;job (9%).</p>



<p>However, there were other motives at work. 56% of our respondents said that they wanted to increase their “job security,” which is at odds with the low number who were concerned about losing their job. And 73% reported that they engaged in training or obtained certifications to increase their “hireability,” which may suggest more concern about job stability than our respondents would admit. The pandemic was a threat to many businesses, and employees were justifiably concerned that their job could vanish after a bad pandemic-influenced quarter. A desire for increased hireability may also indicate that we’ll see more people looking to change jobs in the near future.</p>



<p>Finally, 61% of the respondents said that they participated in training or earned certifications because they wanted a salary increase or a promotion (“increase in job title/responsibilities”). It isn’t surprising that employees see training as a route to promotion—especially as companies that want to hire in fields like data science, machine learning, and AI contend with a&nbsp;<a href="https://www.oreilly.com/radar/ai-adoption-in-the-enterprise-2021/">shortage of qualified employees</a>. Given the difficulty of hiring expertise from outside, we expect an increasing number of companies to grow their own ML and AI talent internally using training programs.</p>



<h2>Salaries by Gender</h2>



<p>To nobody’s surprise, our survey showed that data science and AI professionals are mostly male. The number of respondents tells the story by itself: only 14% identified as women, which is lower than we’d have guessed, though it’s roughly consistent with our conference attendance (back when we had live conferences) and roughly equivalent to other technical fields. A small number (5%) reported their preferred pronoun as “they” or Other, but this sample was too small to draw any significant comparisons about compensation.</p>



<p>Women’s salaries were sharply lower than men’s salaries, averaging $126,000 annually, or 84% of the average salary for men ($150,000). That differential held regardless of education, as&nbsp;Figure 1&nbsp;shows: the average salary for a woman with a doctorate or master’s degree was 82% of the salary for a man with an equivalent degree. The difference wasn’t quite as high for people with bachelor’s degrees or who were still students, but it was still significant: women with bachelor’s degrees or who were students earned 86% or 87% of the average salary for men. The difference in salaries was greatest between people who were self-taught: in that case, women’s salaries were 72% of men’s. An associate’s degree was the only degree for which women’s salaries were higher than men’s.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/01-1048x531.jpg" alt="" class="wp-image-13955" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/01-1048x531.jpg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/01-300x152.jpg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/01-768x389.jpg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/01-1536x779.jpg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/01-2048x1038.jpg 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Figure 1. Women’s and men’s salaries by degree</em></figcaption></figure>



<p>Despite the salary differential, a higher percentage of women had advanced degrees than men: 16% of women had a doctorate, as opposed to 13% of men. And 47% of women had a master’s degree, as opposed to 46% of men. (If those percentages seem high, keep in mind that many professionals in data science and AI are escapees from academia.)</p>



<p>Women’s salaries also lagged men’s salaries when we compared women and men with similar job titles (see&nbsp;Figure 2). At the executive level, the average salary for women was $163,000 versus $205,000 for men (a 20% difference). At the director level, the difference was much smaller—$180,000 for women versus $184,000 for men—and women’s salaries were actually higher than those at the executive level. It’s easy to hypothesize about this difference, but we’re at a loss to explain it. For managers, women’s salaries were $143,000 versus $154,000 for men (a 7% difference).</p>



<p>Career advancement is also an issue: 18% of the women who participated in the survey were executives or directors, compared with 23% of the men.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/02-1048x488.jpg" alt="" class="wp-image-13956" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/02-1048x488.jpg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/02-300x140.jpg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/02-768x358.jpg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/02-1536x715.jpg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/02-2048x954.jpg 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Figure 2. Women’s and men’s salaries by job title</em></figcaption></figure>



<p>Before moving on from our consideration of the effect of gender on salary, let’s take a brief look at how salaries changed over the past three years. As&nbsp;Figure 3&nbsp;shows, the percentage of men and women respondents who saw no change was virtually identical (18%). But more women than men saw their salaries decrease (10% versus 7%). Correspondingly, more men saw their salaries increase. Women were also more likely to have a smaller increase: 24% of women had an increase of under $5,000 versus 17% of men. At the high end of the salary spectrum, the difference between men and women was smaller, though still not zero: 19% of men saw their salaries increase by over $20,000, but only 18% of women did. So the most significant differences were in the midrange. One anomaly sticks out: a slightly higher percentage of women than men received salary increases in the $15,000 to $20,000 range (8% versus 6%).</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/03-1048x618.jpg" alt="" class="wp-image-13957" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/03-1048x618.jpg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/03-300x177.jpg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/03-768x453.jpg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/03-1536x906.jpg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/03-2048x1207.jpg 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Figure 3. Change in salary for women and men over three years</em></figcaption></figure>



<h2>Salaries by Programming Language</h2>



<p>When we looked at the most popular programming languages for data and AI practitioners, we didn’t see any surprises: Python was dominant (61%), followed by SQL (54%), JavaScript (32%), HTML (29%), Bash (29%), Java (24%), and R (20%). C++, C#, and C were further back in the list (12%, 12%, and 11%, respectively).</p>



<p>Discussing the connection between programming languages and salary is tricky because respondents were allowed to check multiple languages, and most did. But when we looked at the languages associated with the highest salaries, we got a significantly different list. The most widely used and popular languages, like Python ($150,000), SQL ($144,000), Java ($155,000), and JavaScript ($146,000), were solidly in the middle of the salary range. The outliers were Rust, which had the highest average salary (over $180,000), Go ($179,000), and Scala ($178,000). Other less common languages associated with high salaries were Erlang, Julia, Swift, and F#. Web languages (HTML, PHP, and CSS) were at the bottom (all around $135,000). See&nbsp;Figure 4&nbsp;for the full list.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/04-774x1048.jpg" alt="" class="wp-image-13958" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/04-774x1048.jpg 774w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/04-222x300.jpg 222w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/04-768x1040.jpg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/04-1134x1536.jpg 1134w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/04-1512x2048.jpg 1512w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/04-scaled.jpg 1890w" sizes="(max-width: 774px) 100vw, 774px" /><figcaption><em>Figure 4. Salary vs. programming language</em></figcaption></figure>



<p>How do we explain this? It’s difficult to say that data and AI developers who use Rust command a higher salary, since most respondents checked several languages. But we believe that this data shows something significant. The supply of talent for newer languages like Rust and Go is relatively small. While there may not be a huge demand for data scientists who use these languages (yet), there’s clearly some demand—and with experienced Go and Rust programmers in short supply, they command a higher salary. Perhaps it is even simpler: regardless of the language someone will use at work, employers interpret knowledge of Rust and Go as a sign of competence and willingness to learn, which increases candidates’ value. A similar argument can be made for Scala, which is the native language for the widely used Spark platform. Languages like Python and SQL are table stakes: an applicant who can’t use them could easily be penalized, but competence doesn’t confer any special distinction.</p>



<p>One surprise is that 10% of the respondents said that they didn’t use any programming languages. We’re not sure what that means. It’s possible they worked entirely in Excel, which should be considered a programming language but often isn’t. It’s also possible that they were managers or executives who no longer did any programming.</p>



<h2>Salaries by Tool and Platform</h2>



<p>We also asked respondents what tools they used for statistics and machine learning and what platforms they used for data analytics and data management. We observed some of the same patterns that we saw with programming languages. And the same caution applies: respondents were allowed to select multiple answers to our questions about the tools and platforms that they use. (However, multiple answers weren’t as frequent as for programming languages.) In addition, if you’re familiar with tools and platforms for machine learning and statistics, you know that the boundary between them is fuzzy. Is Spark a tool or a platform? We considered it a platform, though two Spark libraries are in the list of tools. What about Kafka? A platform, clearly, but a platform for building data pipelines that’s qualitatively different from a platform like Ray, Spark, or Hadoop.</p>



<p>Just as with programming languages, we found that the most widely used tools and platforms were associated with midrange salaries; older tools, even if they’re still widely used, were associated with lower salaries; and some of the tools and platforms with the fewest users corresponded to the highest salaries. (See&nbsp;Figure 5&nbsp;for the full list.)</p>



<p>The most common responses to the question about tools for machine learning or statistics were “I don’t use any tools” (40%) or Excel (31%). Ignoring the question of how one does machine learning or statistics without tools, we’ll only note that those who didn’t use tools had an average salary of $143,000, and Excel users had an average salary of $138,000—both below average. Stata ($120,000) was also at the bottom of the list; it’s an older package with relatively few users and is clearly falling out of favor.</p>



<p>The popular machine learning packages PyTorch (19% of users, $166,000 average salary), TensorFlow (20%, $164,000), and scikit-learn (27%, $157,000) occupied the middle ground. Those salaries were above the average for all respondents, which was pulled down by the large numbers who didn’t use tools or only used Excel. The highest salaries were associated with H2O (3%, $183,000), KNIME (2%, $180,000), Spark NLP (5%, $179,000), and Spark MLlib (8%, $175,000). It’s hard to trust conclusions based on 2% or 3% of the respondents, but it appears that salaries are higher for people who work with tools that have a lot of “buzz” but aren’t yet widely used. Employers pay a premium for specialized expertise.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/05-1048x934.jpg" alt="" class="wp-image-13959" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/05-1048x934.jpg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/05-300x267.jpg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/05-768x684.jpg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/05-1536x1369.jpg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/05-2048x1825.jpg 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Figure 5. Average salary by tools for statistics or machine learning</em></figcaption></figure>



<p>We see almost exactly the same thing when we look at data frameworks (Figure 6). Again, the most common response was from people who didn’t use a framework; that group also received the lowest salaries (30% of users, $133,000 average salary).</p>



<p>In 2021, Hadoop often seems like legacy software, but 15% of the respondents were working on the Hadoop platform, with an average salary of $166,000. That was above the average salary for all users and at the low end of the midrange for salaries sorted by platform.</p>



<p>The highest salaries were associated with Clicktale (now&nbsp;ContentSquare), a cloud-based analytics system for researching customer experience: only 0.2% of respondents use it, but they have an average salary of $225,000. Other frameworks associated with high salaries were Tecton (the commercial version of Michelangelo, at $218,000), Ray ($191,000), and Amundsen ($189,000). These frameworks had relatively few users—the most widely used in this group was Amundsen with 0.8% of respondents (and again, we caution against reading too much into results based on so few respondents). All of these platforms are relatively new, frequently discussed in the tech press and social media, and appear to be growing healthily. Kafka, Spark, Google BigQuery, and Dask were in the middle, with a lot of users (15%, 19%, 8%, and 5%) and above-average salaries ($179,000, $172,000, $170,000, and $170,000). Again, the most popular platforms occupied the middle of the range; experience with less frequently used and growing platforms commanded a premium.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/06-1048x782.jpg" alt="" class="wp-image-13960" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/06-1048x782.jpg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/06-300x224.jpg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/06-768x573.jpg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/06-1536x1146.jpg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/06-2048x1528.jpg 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Figure 6. Average salary by data framework or platform</em></figcaption></figure>



<h2>Salaries by Industry</h2>



<p>The greatest number of respondents worked in the software industry (20% of the total), followed by consulting (11%) and healthcare, banking, and education (each at 8%). Relatively few respondents listed themselves as consultants (also 2%), though consultancy tends to be cyclic, depending on current thinking on outsourcing, tax law, and other factors. The average income for consultants was $150,000, which is only slightly higher than the average for all respondents ($146,000). That may indicate that we’re currently in some kind of an equilibrium between consultants and in-house talent.</p>



<p>While data analysis has become essential to every kind of business and AI is finding many applications outside of computing, salaries were highest in the computer industry itself, as&nbsp;Figure 7&nbsp;makes clear. For our purposes, the “computer industry” was divided into four segments: computer hardware, cloud services and hosting, security, and software. Average salaries in these industries ranged from $171,000 (for computer hardware) to $164,000 (for software). Salaries for the advertising industry (including social media) were surprisingly low, only $150,000.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/07-1048x842.jpg" alt="" class="wp-image-13961" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/07-1048x842.jpg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/07-300x241.jpg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/07-768x617.jpg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/07-1536x1234.jpg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/07-2048x1645.jpg 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Figure 7. Average salary by industry</em></figcaption></figure>



<p>Education and nonprofit organizations (including trade associations) were at the bottom end of the scale, with compensation just above $100,000 ($106,000 and $103,000, respectively). Salaries for technical workers in government were slightly higher ($124,000).</p>



<h2>Salaries by State</h2>



<p>When looking at data and AI practitioners geographically, there weren’t any big surprises. The states with the most respondents were California, New York, Texas, and Massachusetts. California accounted for 19% of the total, with over double the number of respondents from New York (8%). To understand how these four states dominate, remember that they make up 42% of our respondents but only 27% of the United States’ population.</p>



<p>Salaries in California were the highest, averaging&nbsp;$176,000. The Eastern Seaboard did well, with an average salary of $157,000 in Massachusetts (second highest). New York, Delaware, New Jersey, Maryland, and Washington, DC, all reported average salaries in the neighborhood of $150,000 (as did North Dakota, with five respondents). The average salary reported for Texas was $148,000, which is slightly above the national average but nevertheless seems on the low side for a state with a significant technology&nbsp;industry.</p>



<p>Salaries in the Pacific Northwest were not as high as we expected. Washington just barely made it into the top 10 in terms of the number of respondents, and average salaries in Washington and Oregon were $138,000 and $133,000, respectively. (See&nbsp;Figure 8&nbsp;for the full list.)</p>



<p>The highest-paying jobs, with salaries over $300,000, were concentrated in California (5% of the state’s respondents) and Massachusetts (4%). There were a few interesting outliers: North Dakota and Nevada both had very few respondents, but each had one respondent making over $300,000. In Nevada, we’re guessing that’s someone who works for the casino industry—after all, the origins of probability and statistics are tied to gambling. Most states had no respondents with compensation over $300,000.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/08-661x1048.jpg" alt="" class="wp-image-13962" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/08-661x1048.jpg 661w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/08-189x300.jpg 189w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/08-768x1218.jpg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/08-968x1536.jpg 968w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/08-1291x2048.jpg 1291w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/08-scaled.jpg 1614w" sizes="(max-width: 661px) 100vw, 661px" /><figcaption><em>Figure 8. Average salary by state</em></figcaption></figure>



<p>The lowest salaries were, for the most part, from states with the fewest respondents. We’re reluctant to say more than that. These states typically had under 10 respondents, which means that averaging salaries is extremely noisy. For example, Alaska only had two respondents and an average salary of $75,000; Mississippi and Louisiana each only had five respondents, and Rhode Island only had three. In any of these states, one or two additional respondents at the executive level would have a huge effect on the states average. Furthermore, the averages in those states are so low that all (or almost all) respondents must be students, interns, or in entry-level positions. So we don’t think we can make any statement stronger than “the high paying jobs are where you’d expect them to be.”</p>



<h2>Job Change by Salary</h2>



<p>Despite the differences between states, we found that the desire to change jobs based on lack of compensation didn’t depend significantly on geography. There were outliers at both extremes, but they were all in states where the number of respondents was small and one or two people looking to change jobs would make a significant difference. It’s not terribly interesting to say that 24% of respondents from California intend to change jobs (only 2% above the national average); after all, you’d expect California to dominate. There may be a small signal from states like New York, with 232 respondents, of whom 27% intend to change jobs, or from a state like Virginia, with 137 respondents, of whom only 19% were thinking of changing. But again, these numbers aren’t much different from the total percentage of possible job changers.</p>



<p>If intent to change jobs due to compensation isn’t dependent on location, then what does it depend on? Salary. It’s not at all surprising that respondents with the lowest salaries (under $50,000/year) are highly motivated to change jobs (29%); this group is composed largely of students, interns, and others who are starting their careers. The group that showed the second highest desire to change jobs, however, had the highest salaries: over $400,000/year (27%). It’s an interesting pairing: those with the highest and lowest salaries were most intent on getting a salary increase.</p>



<p>26% of those with annual salaries between $50,000 and $100,000 indicated that they intend to change jobs because of compensation. For the remainder of the respondents (those with salaries between $100,000 and $400,000), the percentage who intend to change jobs was 22% or lower.</p>



<h2>Salaries by Certification</h2>



<p>Over a third of the respondents (37%) replied that they hadn’t obtained any certifications in the past year. The next biggest group replied “other” (14%), meaning that they had obtained certifications in the past year but not one of the certifications we listed. We allowed them to write in their own responses, and they shared 352 unique answers, ranging from vendor-specific certifications (e.g., DataRobot) to university degrees (e.g., University of Texas) to well-established certifications in any number of fields (e.g., Certified Information Systems Security Professional a.k.a. CISSP). While there were certainly cases where respondents used different words to describe the same thing, the amount of unique write-in responses reflects the great number of certifications available.</p>



<p>Cloud certifications were by far the most popular. The top certification was for AWS (3.9% obtained AWS Certified Solutions Architect-Associate), followed by Microsoft Azure (3.8% had AZ-900: Microsoft Azure Fundamentals), then two more AWS certifications and CompTIA’s Security+ certification (1% each). Keep in mind that 1% only represents 27 respondents, and all the other certifications had even fewer respondents.</p>



<p>As&nbsp;Figure 9&nbsp;shows, the highest salaries were associated with AWS certifications, the Microsoft AZ-104 (Azure Administrator Associate) certification, and the CISSP security certification. The average salary for people listing these certifications was higher than the average salary for US respondents as a whole. And the average salary for respondents who wrote in a certification was slightly above the average for those who didn’t earn any certifications ($149,000 versus $143,000).</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/09-1048x580.jpg" alt="" class="wp-image-13963" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/09-1048x580.jpg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/09-300x166.jpg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/09-768x425.jpg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/09-1536x850.jpg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/09-2048x1133.jpg 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Figure 9. Average salary by certification earned</em></figcaption></figure>



<p>Certifications were also associated with salary increases (Figure 10). Again AWS and Microsoft Azure dominate, with Microsoft’s AZ-104 leading the way, followed by three AWS certifications. And on the whole, respondents with certifications appear to have received larger salary increases than those who didn’t earn any technical certifications.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/10-1048x599.jpg" alt="" class="wp-image-13964" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/10-1048x599.jpg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/10-300x171.jpg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/10-768x439.jpg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/10-1536x877.jpg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/10-2048x1170.jpg 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Figure 10. Average salary change by certification</em></figcaption></figure>



<p>Google Cloud is an obvious omission from this story. While Google is the third-most-important cloud provider, only 26 respondents (roughly 1%) claimed any Google certification, all under the “Other” category.</p>



<p>Among our respondents, security certifications were relatively uncommon and didn’t appear to be associated with significantly higher salaries or salary increases. Cisco’s CCNP was associated with higher salary increases; respondents who earned the CompTIA Security+ or CISSP certifications received smaller increases. Does this reflect that management undervalues security training? If this hypothesis is correct, undervaluing security is clearly a significant mistake, given the ongoing importance of security and the possibility of new attacks against AI and other data-driven systems.</p>



<p>Cloud certifications clearly had the greatest effect on salary increases. With very few exceptions, any certification was better than no certification: respondents who wrote in a certification under “Other” averaged a $9,600 salary increase over the last few years, as opposed to $8,900 for respondents who didn’t obtain a certification and $9,300 for all respondents regardless of certification.</p>



<h2>Training</h2>



<p>Participating in training resulted in salary increases—but only for those who spent more than 100 hours in a training program. As&nbsp;Figure 11 shows, those respondents had an average salary increase of $11,000. This was also the largest group of respondents (19%). Respondents who only reported undertaking 1–19 hours of training (8%) saw lower salary increases, with an average of $7,100. It’s interesting that those who participated in 1–19 hours of training saw smaller increases than those who didn’t participate in training at all. It doesn’t make sense to speculate about this difference, but the data does make one thing clear: if you engage in training, be serious about it.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/11-1048x468.jpg" alt="" class="wp-image-13965" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/11-1048x468.jpg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/11-300x134.jpg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/11-768x343.jpg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/11-1536x687.jpg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/11-2048x915.jpg 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Figure 11. Average salary change vs. hours of training</em></figcaption></figure>



<p>We also asked what types of training respondents engaged in: whether it was company provided (for which there were three alternatives), a certification program, a conference, or some other kind of training (detailed in&nbsp;Figure 12). Respondents who took advantage of company-provided opportunities had the highest average salaries ($156,000, $150,000, and $149,000). Those who obtained certifications were next ($148,000). The results are similar if we look at salary increases over the past three years: Those who participated in various forms of company-offered training received increases between $11,000 and $10,000. Salary increases for respondents who obtained a certification were in the same range ($11,000).</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/12-1048x557.jpg" alt="" class="wp-image-13966" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/12-1048x557.jpg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/12-300x160.jpg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/12-768x408.jpg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/12-1536x817.jpg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/12-2048x1089.jpg 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Figure 12. Average salary change vs. type of training</em></figcaption></figure>



<h2>The Last Word</h2>



<p>Data and AI professionals—a rubric under which we include data scientists, data engineers, and specialists in AI and ML—are well-paid, reporting an average salary just under $150,000. However, there were sharp state-by-state differences: salaries were significantly higher in California, though the Northeast (with some exceptions) did well.</p>



<p>There were also significant differences between salaries for men and women. Men’s salaries were higher regardless of job title, regardless of training and regardless of academic degrees—even though women were more likely to have an advanced academic degree (PhD or master’s degree) than were men.</p>



<p>We don’t see evidence of a “great resignation.” Job turnover through the pandemic was roughly what we’d expect (perhaps slightly below normal). Respondents did appear to be concerned about job security, though they didn’t want to admit it explicitly. But with the exception of the least- and most-highly compensated respondents, the intent to change jobs because of salary was surprisingly consistent and nothing to be alarmed at.</p>



<p>Training was important, in part because it was associated with hireability and job security but more because respondents were genuinely interested in learning new skills and improving current ones. Cloud training, particularly in AWS and Microsoft Azure, was the most strongly associated with higher salary increases.</p>



<p>But perhaps we should leave the last word to our respondents. The final question in our survey asked what areas of technology would have the biggest effect on salary and promotions in the coming year. It wasn’t a surprise that most of the respondents said machine learning (63%)—these days, ML is the hottest topic in the data world. It was more of a surprise that “programming languages” was noted by just 34% of respondents. (Only “Other” received fewer responses—see&nbsp;Figure 13&nbsp;for full details.) Our respondents clearly aren’t impressed by programming languages, even though the data suggests that employers are willing to pay a premium for Rust, Go, and Scala.</p>



<p>There’s another signal worth paying attention to if we look beyond the extremes. Data tools, cloud and containers, and automation were nearly tied (46, 47, and 44%). The cloud and containers&nbsp;category&nbsp;includes tools like Docker and Kubernetes, cloud providers like AWS and Microsoft Azure, and disciplines like MLOps. The tools category includes tools for building and maintaining data pipelines, like Kafka. “Automation” can mean a lot of things but in this context probably means automated training and deployment.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/13-1048x808.jpg" alt="" class="wp-image-13967" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/13-1048x808.jpg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/13-300x231.jpg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/13-768x592.jpg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/13-1536x1184.jpg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/13-2048x1579.jpg 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Figure 13. What technologies will have the biggest effect on compensation in the coming year?</em></figcaption></figure>



<p>We’ve argued for some time that&nbsp;<a href="https://www.oreilly.com/radar/ai-meets-operations/">operations</a>—successfully deploying and managing applications in production—is the biggest issue facing ML practitioners in the coming years. If you want to stay on top of what’s happening in data, and if you want to maximize your job security, hireability, and salary, don’t just learn how to build AI models; learn how to deploy applications that live in the cloud.</p>



<p>In the classic movie&nbsp;<em>The Graduate</em>, one character famously says, “There’s a great future in plastics. Think about it.” In 2021, and without being anywhere near as repulsive, we’d say, “There’s a great future in the cloud. Think about it.”</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/2021-data-ai-salary-survey/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>AI Adoption in the Enterprise 2021</title>
		<link>https://www.oreilly.com/radar/ai-adoption-in-the-enterprise-2021/</link>
				<comments>https://www.oreilly.com/radar/ai-adoption-in-the-enterprise-2021/#respond</comments>
				<pubDate>Mon, 19 Apr 2021 12:20:38 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides]]></dc:creator>
				<category><![CDATA[AI & ML]]></category>
		<category><![CDATA[Research]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=13720</guid>
				<description><![CDATA[During the first weeks of February, we asked recipients of our Data and AI Newsletters to participate in a survey on AI adoption in the enterprise. We were interested in answering two questions. First, we wanted to understand how the use of AI grew in the past year. We were also interested in the practice [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>During the first weeks of February, we asked recipients of our <em>Data </em>and<em> AI Newsletters</em> to participate in a survey on AI adoption in the enterprise. We were interested in answering two questions. First, we wanted to understand how the use of AI grew in the past year. We were also interested in the practice of AI: how developers work, what techniques and tools they use, what their concerns are, and what development practices are in place.</p>



<p>The most striking result is the sheer number of respondents. In our 2020 survey, which reached the same audience, we had 1,239 responses. This year, we had a total of 5,154. After eliminating 1,580 respondents who didn’t complete the survey, we’re left with 3,574 responses—almost three times as many as last year. It’s possible that pandemic-induced boredom led more people to respond, but we doubt it. Whether they’re putting products into production or just kicking the tires, more people are using AI than ever before.</p>



<hr class="wp-block-separator" />



<p class="has-text-align-center"><strong>Executive Summary</strong></p>



<ul><li>We had almost three times as many responses as last year, with similar efforts at promotion. More people are working with AI.</li><li>In the past, company culture has been the most significant barrier to AI adoption. While it’s still an issue, culture has dropped to fourth place.</li><li>This year, the most significant barrier to AI adoption is the lack of skilled people and the difficulty of hiring. That shortage has been predicted for several years; we’re finally seeing it.</li><li>The second-most significant barrier was the availability of quality data. That realization is a sign that the field is growing up. </li><li>The percentage of respondents reporting “mature” practices has been roughly the same for the last few years. That isn’t surprising, given the increase in the number of respondents: we suspect many organizations are just beginning their AI projects. </li><li>The retail industry sector has the highest percentage of mature practices; education has the lowest. But education also had the highest percentage of respondents who were “considering” AI. </li><li>Relatively few respondents are using version control for data and models. Tools for versioning data and models are still immature, but they’re critical for making AI results reproducible and reliable.</li></ul>



<hr class="wp-block-separator" />



<h2>Respondents</h2>



<p>Of the 3,574 respondents who completed this year’s survey, 3,099 were working with AI in some way: considering it, evaluating it, or putting products into production. Of these respondents, it’s not a surprise that the largest number are based in the United States (39%) and that roughly half were from North America (47%). India had the second-most respondents (7%), while Asia (including India) had 16% of the total. Australia and New Zealand accounted for 3% of the total, giving the Asia-Pacific (APAC) region 19%. A little over a quarter (26%) of respondents were from Europe, led by Germany (4%). 7% of the respondents were from South America, and 2% were from Africa. Except for Antarctica, there were no continents with zero respondents, and a total of 111 countries were represented. These results that interest and use of AI is worldwide and growing.</p>



<p>This year’s results match last year’s data well. But it’s equally important to notice what the data doesn’t say. Only 0.2% of the respondents said they were from China. That clearly doesn’t reflect reality; China is a leader in AI and probably has more AI developers than any other nation, including the US. Likewise, 1% of the respondents were from Russia. Purely as a guess, we suspect that the number of AI developers in Russia is slightly smaller than the number in the US. These anomalies say much more about who the survey reached (subscribers to O’Reilly’s newsletters) than they say about the actual number of AI developers in Russia and China.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0101-1048x993.png" alt="" class="wp-image-13721" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0101-1048x993.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0101-300x284.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0101-768x728.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0101-1536x1456.png 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0101-2048x1941.png 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 1. Respondents working with AI by country (top 12)</figcaption></figure>



<p>The respondents represented a diverse range of industries. Not surprisingly, computers, electronics, and technology topped the charts, with 17% of the respondents. Financial services (15%), healthcare (9%), and education (8%) are the industries making the next-most significant use of AI. We see relatively little use of AI in the pharmaceutical and chemical industries (2%), though we expect that to change sharply given the role of AI in developing the COVID-19 vaccine. Likewise, we see few respondents from the automotive industry (2%), though we know that AI is key to new products such as autonomous vehicles.</p>



<p>3% of the respondents were from the energy industry, and another 1% from public utilities (which includes part of the energy sector). That’s a respectable number by itself, but we have to ask: Will AI play a role in rebuilding our frail and outdated energy infrastructure, as events of the last few years—not just the Texas freeze or the California fires—have demonstrated? We expect that it will, though it’s fair to ask whether AI systems trained on normative data will be robust in the face of “black swan” events. What will an AI system do when faced with a rare situation, one that isn’t well-represented in its training data? That, after all, is the problem facing the developers of autonomous vehicles. Driving a car safely is easy when the other traffic and pedestrians all play by the rules. It’s only difficult when something unexpected happens. The same is true of the electrical grid.</p>



<p>We also expect AI to reshape agriculture (1% of respondents). As with energy, AI-driven changes won’t come quickly. However, we’ve seen a steady stream of AI projects in agriculture, with goals ranging from <a href="https://oreil.ly/3jALP">detecting crop disease</a> to <a href="https://oreil.ly/UPOgM">killing moths with small drones</a>.</p>



<p>Finally, 8% of respondents said that their industry was “Other,” and 14% were grouped into “All Others.” “All Others” combines 12 industries that the survey listed as possible responses (including automotive, pharmaceutical and chemical, and agriculture) but that didn’t have enough responses to show in the chart. “Other” is the wild card, comprising industries we didn’t list as options. “Other” appears in the fourth position, just behind healthcare. Unfortunately, we don’t know which industries are represented by that category—but it shows that the spread of AI has indeed become broad!</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0102-1048x767.png" alt="" class="wp-image-13723" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0102-1048x767.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0102-300x220.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0102-768x562.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0102-1536x1124.png 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0102-2048x1499.png 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 2. Industries using AI</figcaption></figure>



<h2>Maturity</h2>



<p>Roughly one quarter of the respondents described their use of AI as “mature” (26%), meaning that they had revenue-bearing AI products in production. This is almost exactly in line with the results from 2020, where 25% of the respondents reported that they had products in production (“Mature” wasn’t a possible response in the 2020 survey.)</p>



<p>This year, 35% of our respondents were “evaluating” AI (trials and proof-of-concept projects), also roughly the same as last year (33%). 13% of the respondents weren’t making use of AI or considering using it; this is down from last year’s number (15%), but again, it’s not significantly different.</p>



<p>What do we make of the respondents who are “considering” AI but haven’t yet started any projects (26%)? That’s not an option last year’s respondents had. We suspect that last year respondents who were considering AI said they were either “evaluating” or “not using” it.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/AI_Adoption_Data_Viz_AI-practice-maturity-1048x872.png" alt="" class="wp-image-13744" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/AI_Adoption_Data_Viz_AI-practice-maturity-1048x872.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/AI_Adoption_Data_Viz_AI-practice-maturity-300x250.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/AI_Adoption_Data_Viz_AI-practice-maturity-768x639.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/AI_Adoption_Data_Viz_AI-practice-maturity-1536x1278.png 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/AI_Adoption_Data_Viz_AI-practice-maturity-2048x1703.png 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 3. AI practice maturity</figcaption></figure>



<p>Looking at the problems respondents faced in AI adoption provides another way to gauge the overall maturity of AI as a field. Last year, the major bottleneck holding back adoption was company culture (22%), followed by the difficulty of identifying appropriate use cases (20%). This year, cultural problems are in fourth place (14%) and finding appropriate use cases is in third (17%). That’s a very significant change, particularly for corporate culture. Companies have accepted AI to a much greater degree, although finding appropriate problems to solve still remains a challenge.</p>



<p>The biggest problems in this year’s survey are lack of skilled people and difficulty in hiring (19%) and data quality (18%). It’s no surprise that the demand for AI expertise has exceeded the supply, but it’s important to realize that it’s now become the biggest bar to wider adoption. The biggest skills gaps were ML modelers and data scientists (52%), understanding business use cases (49%), and data engineering (42%). The need for people managing and maintaining computing infrastructure was comparatively low (24%), hinting that companies are solving their infrastructure requirements in the cloud.</p>



<p>It’s gratifying to note that organizations starting to realize the importance of data quality (18%). We’ve known about “garbage in, garbage out” for a long time; that goes double for AI. Bad data yields bad results at scale.</p>



<p>Hyperparameter tuning (2%) wasn’t considered a problem. It’s at the bottom of the list—where, we hope, it belongs. That may reflect the success of automated tools for building models (AutoML, although as we’ll see later, most respondents aren’t using them). It’s more concerning that workflow reproducibility (3%) is in second-to-last place. This makes sense, given that we don’t see heavy usage of tools for model and data versioning. We’ll look at this later, but being able to reproduce experimental results is critical to any science, and it’s a well-known problem in AI.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0104-1048x767.png" alt="" class="wp-image-13725" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0104-1048x767.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0104-300x219.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0104-768x562.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0104-1536x1124.png 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0104-2048x1498.png 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 4. Bottlenecks to AI adoption</figcaption></figure>



<h2>Maturity by Continent</h2>



<p>When looking at the geographic distribution of respondents with mature practices, we found almost no difference between North America (27%), Asia (27%), and Europe (28%). In contrast, in our 2018 report, Asia was behind in mature practices, though it had a markedly higher number of respondents in the “early adopter” or “exploring” stages. Asia has clearly caught up. There’s no significant difference between these three continents in our 2021 data.</p>



<p>We found a smaller percentage of respondents with mature practices and a higher percentage of respondents who were “considering” AI in South America (20%), Oceania (Australia and New Zealand, 18%), and Africa (17%). Don’t underestimate AI’s future impact on any of these continents.</p>



<p>Finally, the percentage of respondents “evaluating” AI was almost the same on each continent, varying only from 31% (South America) to 36% (Oceania).</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0105-986x1048.png" alt="" class="wp-image-13726" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0105-986x1048.png 986w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0105-282x300.png 282w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0105-768x816.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0105-1445x1536.png 1445w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0105-1927x2048.png 1927w" sizes="(max-width: 986px) 100vw, 986px" /><figcaption>Figure 5. Maturity by continent</figcaption></figure>



<h2>Maturity by Industry</h2>



<p>While AI maturity doesn’t depend strongly on geography, we see a different picture if we look at maturity by industry.</p>



<p>Looking at the top eight industries, financial services (38%), telecommunications (37%), and retail (40%) had the greatest percentage of respondents reporting mature practices. And while it had by far the greatest number of respondents, computers, electronics, and technology was in fourth place, with 35% of respondents reporting mature practices. Education (10%) and government (16%) were the laggards. Healthcare and life sciences, at 28%, were in the middle, as were manufacturing (25%), defense (26%), and media (29%).</p>



<p>On the other hand, if we look at industries that are considering AI, we find that education is the leader (48%). Respondents working in government and manufacturing seem to be somewhat further along, with 49% and 47% evaluating AI, meaning that they have pilot or proof-of-concept projects in progress.</p>



<p>This may just be a trick of the numbers: every group adds up to 100%, so if there are fewer “mature” practices in one group, the percentage of “evaluating” and “considering” practices has to be higher. But there’s also a real signal: respondents in these industries may not consider their practices “mature,” but each of these industry sectors had over 100 respondents, and education had almost 250. Manufacturing needs to automate many processes (from assembly to inspection and more); government has been as challenged as any industry by the global pandemic, and has always needed ways to “do more with less”; and education has been experimenting with technology for a number of years now. There is a real desire to do more with AI in these fields. It’s worth pointing out that educational and governmental applications of AI frequently raise ethical questions—and one of the most important issues for the next few years will be seeing how these organizations respond to ethical problems.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0106-1048x1009.png" alt="" class="wp-image-13727" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0106-1048x1009.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0106-300x289.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0106-768x739.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0106-1536x1479.png 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0106-2048x1972.png 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 6. Maturity by industry (percent)</figcaption></figure>



<h2>The Practice of AI</h2>



<p>Now that we’ve discussed where mature practices are found, both geographically and by industry, let’s see what a mature practice looks like. What do these organizations have in common? How are they different from organizations that are evaluating or considering AI?</p>



<h3>Techniques</h3>



<p>First, 82% of the respondents are using supervised learning, and 67% are using deep learning. Deep learning is a set of algorithms that are common to almost all AI approaches, so this overlap isn’t surprising. (Participants could provide multiple answers.) 58% claimed to be using unsupervised learning.</p>



<p>After unsupervised learning, there was a significant drop-off. Human-in-the-loop, knowledge graphs, reinforcement learning, simulation, and planning and reasoning all saw usage below 40%. Surprisingly, natural language processing wasn’t in the picture at all. (A very small number of respondents wrote in “natural language processing” as a response, but they were only a small percentage of the total.) This is significant and definitely worth watching over the next few months. In the last few years, there have been many breakthroughs in NLP and NLU (natural language understanding): everyone in the industry has read about GPT-3, and many vendors are betting heavily on using AI to automate customer service call centers and similar applications. This survey suggests that those applications still haven’t moved into practice.</p>



<p>We asked a similar question to respondents who were considering or evaluating the use of AI (60% of the total). While the percentages were lower, the technologies appeared in the same order, with very few differences. This indicates that respondents who are still evaluating AI are experimenting with fewer technologies than respondents with mature practices. That suggests (reasonably enough) that respondents are choosing to “start simple” and limit the techniques that they experiment with.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0107-1048x823.png" alt="" class="wp-image-13728" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0107-1048x823.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0107-300x236.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0107-768x603.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0107-1536x1206.png 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0107-2048x1609.png 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 7. AI technologies used in mature practices</figcaption></figure>



<h3>Data</h3>



<p>We also asked what kinds of data our “mature” respondents are using. Most (83%) are using structured data (logfiles, time series data, geospatial data). 71% are using text data—that isn’t consistent with the number of respondents who reported using NLP, unless “text” is being used generically to include any data that can be represented as text (e.g., form data). 52% of the respondents reported using images and video. That seems low relative to the amount of research we read about AI and computer vision. Perhaps it’s not surprising though: there’s no reason for business use cases to be in sync with academic research. We’d expect most business applications to involve structured data, form data, or text data of some kind. Relatively few respondents (23%) are working with audio, which remains very challenging.</p>



<p>Again, we asked a similar question to respondents who were evaluating or considering AI, and again, we received similar results, though the percentage of respondents for any given answer was somewhat smaller (4–5%).</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0108-1048x503.png" alt="" class="wp-image-13730" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0108-1048x503.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0108-300x144.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0108-768x368.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0108-1536x737.png 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0108-2048x983.png 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 8. Data types used in mature practices</figcaption></figure>



<h3>Risk</h3>



<p>When we asked respondents with mature practices what risks they checked for, 71% said “unexpected outcomes or predictions.” Interpretability, model degradation over time, privacy, and fairness also ranked high (over 50%), though it’s disappointing that only 52% of the respondents selected this option. Security is also a concern, at 42%. AI raises important new security issues, including the possibility of poisoned data sources and reverse engineering models to extract private information.</p>



<p>It’s hard to interpret these results without knowing exactly what applications are being developed. Privacy, security, fairness, and safety are important concerns for every application of AI, but it’s also important to realize that not all applications are the same. A farming application that <a href="https://oreil.ly/jj0Lz">detects crop disease</a> doesn’t have the same kind of risks as an application that’s approving or denying loans. Safety is a much bigger concern for autonomous vehicles than for personalized shopping bots. However, do we really believe that these risks don’t need to be addressed for nearly half of all projects?</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0109-1048x825.png" alt="" class="wp-image-13731" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0109-1048x825.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0109-300x236.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0109-768x605.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0109-1536x1210.png 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0109-2048x1613.png 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 9. Risks checked for during development</figcaption></figure>



<h3>Tools</h3>



<p>Respondents with mature practices clearly had their favorite tools: scikit-learn, TensorFlow, PyTorch, and Keras each scored over 45%, with scikit-learn and TensorFlow the leaders (both with 65%). A second group of tools, including Amazon’s SageMaker (25%), Microsoft’s Azure ML Studio (21%), and Google’s Cloud ML Engine (18%), clustered around 20%, along with Spark NLP and spaCy.</p>



<p>When asked which tools they planned to incorporate over the coming 12 months, roughly half of the respondents answered model monitoring (57%) and model visualization (49%). Models become stale for many reasons, not the least of which is changes in human behavior, changes for which the model itself may be responsible. The ability to monitor a model’s performance and detect when it has become “stale” will be increasingly important as businesses grow more reliant on AI and in turn demand that AI projects demonstrate their value.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0110-913x1048.png" alt="" class="wp-image-13732" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0110-913x1048.png 913w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0110-261x300.png 261w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0110-768x882.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0110-1338x1536.png 1338w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0110-1784x2048.png 1784w" sizes="(max-width: 913px) 100vw, 913px" /><figcaption>Figure 10. Tools used by mature practices</figcaption></figure>



<p>Responses from those who were evaluating or considering AI were similar, but with some interesting differences: scikit-learn moved from first place to third (48%). The second group was led by products from cloud vendors that incorporate AutoML: Microsoft Azure ML Studio (29%), Google Cloud ML Engine (25%), and Amazon SageMaker (23%). These products were significantly more popular than they were among “mature” users. The difference isn’t huge, but it is striking. At risk of over-overinterpreting, users who are newer to AI are more inclined to use vendor-specific packages, more inclined to use AutoML in one of its incarnations, and somewhat more inclined to go with Microsoft or Google rather than Amazon. It’s also possible that scikit-learn has less brand recognition among those who are relatively new to AI compared to packages from organizations like Google or Facebook.</p>



<p>When asked specifically about AutoML products, 51% of “mature” respondents said they weren’t using AutoML at all. 22% use Amazon SageMaker; 16% use Microsoft Azure AutoML; 14% use Google Cloud AutoML; and other tools were all under 10%. Among users who are evaluating or considering AI, only 40% said they weren’t using AutoML at all—and the Google, Microsoft, and Amazon packages were all but tied (27–28%). AutoML isn’t yet a big part of the picture, but it appears to be gaining traction among users who are still considering or experimenting with AI. And it’s possible that we’ll see increased use of AutoML tools among mature users, of whom 45% indicated that they would be incorporating tools for automated model search and hyperparameter tuning (in a word, AutoML) in the coming yet.</p>



<h3>Deployment and Monitoring</h3>



<p>An AI project means nothing if it can’t be deployed; even projects that are only intended for internal use need some kind of deployment. Our survey showed that AI deployment is still largely unknown territory, dominated by homegrown ad hoc processes. The three most significant tools for deploying AI all had roughly 20% adoption: MLflow (22%), TensorFlow Extended, a.k.a. TFX (20%), and Kubeflow (18%). Three products from smaller startups—<a href="https://www.dominodatalab.com/">Domino</a>, <a href="https://www.seldon.io/">Seldon</a>, and <a href="https://www.cortex.dev/">Cortex</a>—had roughly 4% adoption. But the most frequent answer to this question was “none of the above” (46%). Since this question was only asked of respondents with “mature” AI practices (i.e., respondents who have AI products in production), we can only assume that they’ve built their own tools and pipelines for deployment and monitoring. Given the many forms that an AI project can take, and that AI deployment is still something of a dark art, it isn’t surprising that AI developers and operations teams are only starting to adopt third-party tools for deployment.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0111-1048x684.png" alt="" class="wp-image-13733" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0111-1048x684.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0111-300x196.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0111-768x502.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0111-1536x1003.png 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0111-2048x1338.png 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 11. Automated tools used in mature practices for deployment<br> and monitoring</figcaption></figure>



<h3>Versioning</h3>



<p>Source control has long been a standard practice in software development. There are many well-known tools used to build source code repositories.</p>



<p>We’re confident that AI projects use source code repositories such as Git or GitHub; that’s a standard practice for all software developers. However, AI brings with it a different set of problems. In AI systems, the training data is as important as, if not more important than, the source code. So is the model built from the training data: the model reflects the training data and hyperparameters, in addition to the source code itself, and may be the result of hundreds of experiments.</p>



<p>Our survey shows that AI developers are only starting to use tools for data and model versioning. For data versioning, 35% of the respondents are using homegrown tools, while 46% responded “none of the above,” which we take to mean they’re using nothing more than a database. 9% are using <a href="https://dvc.org/">DVC</a>, 8% are using tools from <a href="https://wandb.ai/site">Weights &amp; Biases</a>, and 5% are using <a href="https://www.pachyderm.com/">Pachyderm</a>.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0112-1048x575.png" alt="" class="wp-image-13734" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0112-1048x575.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0112-300x165.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0112-768x422.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0112-1536x843.png 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0112-2048x1124.png 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 12. Automated tools used for data versioning</figcaption></figure>



<p>Tools for model and experiment tracking were used more frequently, although the results are fundamentally the same. 29% are using homegrown tools, while 34% said “none of the above.” The leading tools were MLflow (27%) and Kubeflow (18%), with Weights &amp; Biases at 8%.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0113-1048x826.png" alt="" class="wp-image-13735" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0113-1048x826.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0113-300x237.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0113-768x606.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0113-1536x1211.png 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0113-2048x1615.png 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 13. Automated tools used for model and experiment tracking</figcaption></figure>



<p>Respondents who are considering or evaluating AI are even less likely to use data versioning tools: 59% said “none of the above,” while only 26% are using homegrown tools. Weights &amp; Biases was the most popular third-party solution (12%). When asked about model and experiment tracking, 44% said “none of the above,” while 21% are using homegrown tools. It’s interesting, though, that in this group, MLflow (25%) and Kubeflow (21%) ranked above homegrown tools.</p>



<p>Although the tools available for versioning models and data are still rudimentary, it’s disturbing that so many practices, including those that have AI products in production, aren’t using them. You can’t reproduce results if you can’t reproduce the data and the models that generated the results. We’ve said that a quarter of respondents considered their AI practice mature—but it’s unclear what maturity means if it doesn’t include reproducibility.</p>



<h2>The Bottom Line</h2>



<p>In the past two years, the audience for AI has grown, but it hasn’t changed much: Roughly the same percentage of respondents consider themselves to be part of a “mature” practice; the same industries are represented, and at roughly the same levels; and the geographical distribution of our respondents has changed little.</p>



<p>We don’t know whether to be gratified or discouraged that only 50% of the respondents listed privacy or ethics as a risk they were concerned about. Without data from prior years, it’s hard to tell whether this is an improvement or a step backward. But it’s difficult to believe that there are so many AI applications for which privacy, ethics, and security aren’t significant risks.</p>



<p>Tool usage didn’t present any big surprises: the field is dominated by scikit-learn, TensorFlow, PyTorch, and Keras, though there’s a healthy ecosystem of open source, commercially licensed, and cloud native tools. AutoML has yet to make big inroads, but respondents representing less mature practices seem to be leaning toward automated tools and are less likely to use scikit-learn.</p>



<p>The number of respondents who aren’t addressing data or model versioning was an unwelcome surprise. These practices should be foundational: central to developing AI products that have verifiable, repeatable results. While we acknowledge that versioning tools appropriate to AI applications are still in their early stages, the number of participants who checked “none of the above” was revealing—particularly since “the above” included homegrown tools. You can’t have reproducible results if you don’t have reproducible data and models. Period.</p>



<p>In the past year, AI in the enterprise has grown; the sheer number of respondents will tell you that. But has it matured? Many new teams are entering the field, while the percentage of respondents who have deployed applications has remained roughly constant. In many respects, this indicates success: 25% of a bigger number is more than 25% of a smaller number. But is application deployment the right metric for maturity? Enterprise AI won’t really have matured until development and operations groups can engage in practices like continuous deployment, until results are repeatable (at least in a statistical sense), and until ethics, safety, privacy, and security are primary rather than secondary concerns. Mature AI? Yes, enterprise AI has been maturing. But it’s time to set the bar for maturity higher.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/ai-adoption-in-the-enterprise-2021/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>InfoTribes, Reality Brokers</title>
		<link>https://www.oreilly.com/radar/infotribes-reality-brokers/</link>
				<comments>https://www.oreilly.com/radar/infotribes-reality-brokers/#respond</comments>
				<pubDate>Tue, 23 Mar 2021 14:40:55 +0000</pubDate>
		<dc:creator><![CDATA[Hugo Bowne-Anderson]]></dc:creator>
				<category><![CDATA[AI & ML]]></category>
		<category><![CDATA[Research]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=13687</guid>
				<description><![CDATA[It seems harder than ever to agree with others on basic facts, let alone to develop shared values and goals: we even claim to live in a post-truth era1. With anti-vaxxers, QAnon, Bernie Bros, flat earthers, the intellectual dark web, and disagreement worldwide as to the seriousness of COVID-19 and the effectiveness of masks, have [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>It seems harder than ever to agree with others on basic facts, let alone to develop shared values and goals: we even claim to live in a post-truth era<sup>1</sup>. With anti-vaxxers, QAnon, Bernie Bros, flat earthers, the intellectual dark web, and disagreement worldwide as to the seriousness of COVID-19 and the effectiveness of masks, have we lost our shared reality? For every piece of information X somewhere, you can likely find “not X” elsewhere. There is a growing disbelief and distrust in basic science and government. All too often, conversations on social media descend rapidly to questions such as &#8220;<a href="https://twitter.com/januszczak/status/1285266873483919365">What planet are you from?</a>&#8221; </p>



<h2>Reality Decentralized</h2>



<p>What has happened? Reality has once again become decentralized. Before the advent of broadcast media and mass culture, individuals&#8217; <a href="https://en.wikipedia.org/wiki/Mental_model">mental models</a> of the world were generated locally, along with their sense of reality and what they considered ground truth. With broadcast media and the culture industries came the ability to forge top-down, national identities that could be pushed into the living rooms of families at prime time, completing the project of the press and newspapers in nation-forming<sup>2</sup>. The creation of the TV dinner was perhaps one of the most effective tools in carving out a sense of shared reality at a national level (did the TV dinner mean fewer people said Grace?).</p>



<p>The rise of the Internet, Search, social media, apps, and platforms has resulted in an information landscape that bypasses the centralized knowledge/reality-generation machine of broadcast media. It is, however, driven by the incentives (both visible and hidden) of significant power structures, such as Big Tech companies. With the degradation of top-down knowledge, we&#8217;ve seen the return of locally-generated shared realities, where local now refers to proximity in cyberspace. Content creators and content consumers are connected, share information, and develop mental models of the world, along with shared or distinct realities, based on the information they consume. They form communities and shared realities accordingly and all these interactions are mediated by the incentive systems of the platforms they connect on.</p>



<p>As a result, the number of possible realities has proliferated and the ability to find people to share any given reality with has increased. This InfoLandscape we all increasingly occupy is both novel and shifting rapidly. In it, we are currently finding people we can share some semblance of ground truth with: we&#8217;re forming our own InfoTribes, and shared reality is splintering around the globe.</p>



<p>To understand this paradigm shift, we need to comprehend:</p>



<ul><li>the initial vision behind the internet and the InfoLandscapes that have emerged,</li><li>how we are forming InfoTribes and how reality is splintering, </li><li>that large-scale shared reality has merely occupied a blip in human history, ushered in by the advent of broadcast media, and</li><li>who we look to for information and knowledge in an InfoLandscape that we haven&#8217;t evolved to comprehend.</li></ul>



<h2>The InfoLandscapes</h2>



<blockquote class="wp-block-quote"><p>&#8220;Cyberspace. A consensual hallucination experienced daily by billions of legitimate operators, in every nation, by children being taught mathematical concepts&#8230; A graphic representation of data abstracted from the banks of every computer in the human system. Unthinkable complexity. Lines of light ranged in the nonspace of the mind, clusters, and constellations of data. Like city lights, receding.&#8221;</p><cite>— <em>Neuromancer, </em>William Gibson (1984)</cite></blockquote>



<p>There are several ways to frame the origin story of the internet. One is how it gave rise to new forms of information flow: the vision of a novel space in which anybody could publish anything and everyone could find it. Much of the philosophy of early internet pioneers was couched in terms of the potential to &#8220;flatten organizations, globalize society, decentralize control, and help harmonize people&#8221; (<a href="https://web.media.mit.edu/~nicholas/Wired/WIRED3-02.html">Nicholas Negraponte, MIT</a>)<sup>3</sup>.</p>



<p>As John Perry Barlow (of Grateful Dead fame) wrote in <em>A Declaration of the Independence of Cyberspace </em>(1996):</p>



<blockquote class="wp-block-quote"><p>We are creating a world that all may enter without privilege or prejudice accorded by race, economic power, military force, or station of birth. We are creating a world where anyone, anywhere may express his or her beliefs, no matter how singular, without fear of being coerced into silence or conformity. Your legal concepts of property, expression, identity, movement, and context do not apply to us. They are all based on matter, and there is no matter here.</p></blockquote>



<p>This may have been the world we wanted but not the one we got. We are veering closer to an online and app-mediated environment similar to Deleuze&#8217;s <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwir47Gz-LjuAhU0zTgGHff9CPkQFjAIegQIAxAC&amp;url=https%3A%2F%2Fwww.jstor.org%2Fstable%2F778828&amp;usg=AOvVaw0SN92kTpl6SwpB2o1OM11f"><em>Societies of Control</em></a>, in which we are increasingly treated as our data and what Deleuze calls “dividuals”: collections of behavior and characteristics, associated with online interactions, passwords, spending, clicks, cursor movements, and personal algorithms, that can be passed into statistical and predictive models and guided and incentivized to behave and spend in particular ways. Put simply, we are reduced to the inputs of an algorithm. On top of this, pre-existing societal biases are being reinforced and promulgated at previously unheard of scales as <a href="https://www.oreilly.com/radar/when-models-are-everywhere/">we increasingly integrate machine learning models into our daily lives</a>.</p>



<p>Prescient visions of society along these lines were provided by William Gibson and Neal Stephenson&#8217;s 1992 <em>Snow Crash</em>: societies increasingly interacting in virtual reality environments and computational spaces, in which the landscapes were defined by information flows<sup>4</sup>. Not only this, but both authors envisioned such spaces being turned into marketplaces and segmented and demarcated by large corporations, only a stone&#8217;s throw from where we find ourselves today. How did we get here?</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_168604806-1048x604.jpeg" alt="" class="wp-image-13688" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_168604806-1048x604.jpeg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_168604806-300x173.jpeg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_168604806-768x443.jpeg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_168604806-1536x885.jpeg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_168604806-2048x1180.jpeg 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /></figure>



<h3>Information Creation</h3>



<p>In the early days of the internet, you needed to be a coder to create a website. The ability to publish material was relegated to the technical. It was only in walled gardens such as CompuServe and AOL or after the introduction of tools like Blogger that regular punters were able to create their own websites with relative ease. The participatory culture and user-generated content of Web 2.0 opened up the creative space, allowing anyone and everyone to create content, as well as respond to, rate, and review it. Over the last decade, two new dynamics have drastically increased the amount of information creation, and, therefore, the &#8220;raw material&#8221; with which the landscape can be molded:</p>



<ol><li>Smartphones with high-resolution video cameras and</li><li>The transformation of the attention economy by &#8220;social media&#8221; platforms,  which incentivize individuals to digitize more of their experiences and broadcast as much as possible.</li></ol>



<p>And it isn&#8217;t only the generation of novel content or the speed at which information travels. It is also the vast archives of human information and knowledge that are being unearthed, digitized, and made available online. This is the space of content creation.</p>



<h3>Information Retrieval</h3>



<p>The other necessary side of information flow is discoverability, how it is organized, and where it’s surfaced. When so much of the world’s information is available, what is the method for retrieval? Previously the realm of chat rooms and bulletin boards, this question eventually gave rise to the creation of search engines, social media platforms, streaming sites, apps, and platforms.</p>



<p>Platforms that automate the organizing and surfacing of online content are necessary, given the amount of content currently out there and how much is being generated daily. And they also require interrogating, as we humans base our mental models of how the world works on the information we receive, as we do our senses of reality, the way we make decisions, and the communities we form. Platforms such as Facebook have erected walled gardens in our new InfoLandscape and locked many of us into them, as predicted by both Gibson and Stephenson. Do we want such corporatized and closed structures in our networked commons?</p>



<h2>InfoTribes, Shared Reality</h2>



<p><em>A by-product of algorithmic polarization and fragmentation has been the formation of more groups that agree within their own groups and disagree far more between groups, not only on what they value but on ground truth, about reality.</em></p>



<p>Online spaces are novel forms of community: people who haven&#8217;t met and may never meet in real life interacting in cyberspace. As scholars such as danah boyd have <a href="https://www.danah.org/papers/TakenOutOfContext.html">made clear</a>, &#8220;social network sites like MySpace and Facebook are <em>networked publics</em>, just like parks and other outdoor spaces can be understood as publics.&#8221;</p>



<p>One key characteristic of any community is a sense of <em>shared reality</em>, something agreed upon. Communities are based around a sense of shared reality, shared values, and/or shared goals. Historically, communities have required geographical proximity to coalesce, whereas online communities have been able to form outside the constraints of <a href="https://www.urbandictionary.com/define.php?term=meatspace">meatspace</a>. Let’s not make the mistake of assuming online community formation doesn’t have constraints. The constraints are perhaps more hidden, but they exist: they’re both technological and the result of how the InfoLandscapes have been carved out by the platforms, along with their technological and economic incentives<sup>5</sup>. Landscapes and communities have co-evolved, although, for most of history, on different timescales: mountain ranges can separate parts of a community and, conversely, we build tunnels through mountains; rivers connect communities, cities, and commerce, and humans alter the nature of rivers (an extreme example being <a href="https://en.wikipedia.org/wiki/Chicago_River#Reversing_the_flow">the reversal of the Chicago River</a>!).</p>



<p>The past two decades have seen the formation of several new, rapidly and constantly shifting landscapes that we all increasingly interact with, along with the formation of new information communities, driven and consolidated by the emergent phenomena of filter bubbles and echo chambers, among many others, themselves driven by the platforms’ drive for engagement. What the constituents of each of these communities share are mental models of how the world works, senses of reality, that are, for the most part, reinforced by the algorithms that surface content, either by 1) showing content you agree with to promote engagement or 2) showing content you totally disagree with to the same end. Just as the newspaper page has historically been a mish-mash collection of movie ads, obituaries, and opinions stitched together in a way that made the most business and economic sense for any given publisher, your Facebook feed is driven by a collection of algorithms that, in the end, are optimizing for growth and revenue<sup>6</sup>. These incentives define the InfoLandscape and determine the constraints under which communities form. It just so happens that dividing people increases engagement and makes economic sense. As Karen Hao <a href="https://www.technologyreview.com/2021/03/11/1020600/facebook-responsible-ai-misinformation/">wrote recently</a> in the MIT Technology Review, framing it as a result of &#8220;Zuckerberg’s relentless desire for growth,&#8221; which is directly correlated with economic incentives:</p>



<blockquote class="wp-block-quote"><p>The algorithms that underpin Facebook’s business weren’t created to filter out what was false or inflammatory; they were designed to make people share and engage with as much content as possible by showing them things they were most likely to be outraged or titillated by.</p></blockquote>



<p>The consequence? As groups of people turn inward, agreeing more amongst their in-group, and disagreeing more fervently with those outside of it, the common ground in between, the shared reality, which is where perhaps the truth lies, is slowly lost. Put another way, a by-product of algorithmic polarization and fragmentation has been the formation of more groups that agree within their own groups and disagree far more with other groups, not only on what they value but on ground truth, about reality. </p>



<p>We’ve witnessed the genesis of information tribes or <em>InfoTribes</em> and, as these new ideological territories are carved up, those who occupy InfoLandscapes hold that ground as a part of an InfoTribe<sup>7</sup>. Viewed in this way, the online flame wars we’ve become all too accustomed to form part of the initial staking out of territory in these new InfoLandscapes. Anthropologists have long talked about tribes as being formed around symbols of group membership, symbols that unite a people, like totem animals, flags, or&#8230; online content.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_123489033-1048x667.jpeg" alt="" class="wp-image-13689" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_123489033-1048x667.jpeg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_123489033-300x191.jpeg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_123489033-768x489.jpeg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_123489033-1536x978.jpeg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_123489033-2048x1303.jpeg 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /></figure>



<h2>Reality Brokers, Reality Splintering</h2>



<p><em>The platforms that “decide” what we see and when we see it are reality brokers in a serious sense: they guide how individuals construct their sense of the world, their own identities, what they consider ground truth, and the communities they become a part of.</em></p>



<p>Arguably, many people aren’t particularly interested in the ground truth per se, they’re interested in narratives that support their pre-existing mental models of the world, narratives that help them sleep at night. This is something that 45 brilliantly, and perhaps unwittingly, played into and made starkly apparent, by continually sowing seeds of confusion, gaslighting the global community, and questioning the reality of anything that didn’t serve his own purposes.</p>



<p>This trend isn’t confined to the US. The rise of populism more generally in the West can be seen as the result of diverging senses of reality, the first slice splitting people across ideological and party lines. Why are these divergences in a sense of shared reality becoming so exacerbated and apparent now? The unparalleled velocity at which we receive information is one reason, particularly as we likely haven’t evolved to even begin to process the vast amounts we consume. But it isn&#8217;t only the speed and amount, it&#8217;s the structure. The current media landscape is highly non-linear, as opposed to print and television. Our sense-making and reality-forming faculties are overwhelmed daily by the fractal-like nature of (social) media platforms and environments that are full of overlapping phenomena and patterns that occur at many different frequencies<sup>8</sup>. Moreover, the information we’re served is generally driven by opaque and obscure economic incentives of platforms, which are protected by even more obscure legislation in the form of <a href="https://www.theverge.com/21273768/section-230-explained-internet-speech-law-definition-guide-free-moderation">Section</a> <a href="https://www.amazon.com/exec/obidos/ASIN/1501714414/">230</a> in the US (there are other incentives at play, themselves rarely surfaced, in the name of “trade secrets”). </p>



<p>But let&#8217;s be careful here: it isn’t tech all the way down. We’re also deep in a several decades-long erosion of institutional knowledge, a mistrust in both science and government being the two most obvious. Neoliberalism has carved out the middle class while the fruits of top-down knowledge have left so many people unserved and behind. On top of this, ignorance has been actively cultivated and produced. Look no further than the recent manufacturing of ignorance from the top down with the goals of chaos creation, sowing the seeds of doubt, and delegitimizing the scientific method and data reporting (the study of culturally induced ignorance is known as <em>agnotology </em>and Proctor and Scheibinger&#8217;s book <a href="https://www.sup.org/books/title/?id=11232"><em>Agnotology: The Making and Unmaking of Ignorance</em></a> is canonical). On top of this, we&#8217;ve seen the impact of bad actors and foreign influence (not mutually exclusive) on the dismantling of shared reality, such as Russian interference around the 2016 US election.</p>



<p>This has left reality up for grabs and, in an InfoLandscape exacerbated by a global pandemic, those who control and guide the flow of information also control the building of InfoTribes, along with their shared realities. Viewed from another perspective, the internet is a space in which information is created and consumed, a many-sided marketplace of supply-and-demand in which the dominant currency is information, albeit driven by a shadow market of data, marketing collateral, clicks, cash, and crypto. The platforms that “decide” what we see and when we see it are <em>reality brokers </em>in a serious sense: they guide how individuals construct their sense of the world, their own identities, what they consider ground truth, and the communities they become a part of. In some cases, these reality brokers may be doing it completely by accident. They don&#8217;t necessarily care about the ground truth, just about engagement, attention, and profit: the breakdown of shared reality as collateral damage of a globalized, industrial-scale incentive system. In this framework, the rise of conspiracy theories is an artefact of this process: the reality brokered and formed, whether it be a flat earth or a cabal of Satan-worshipping pedophiles plotting against 45, is a direct result of the bottom-up sense-making of top-down <em>reality splintering</em>, the dissolution of ground truth and the implosion of a more general shared reality. Web 2.0 has had a serious part to play in this reality splintering but the current retreat away into higher signal and private platforms such as newsletters, Slack, Discord, WhatsApp, and Signal groups could be more harmful, in many ways.</p>



<p>Shared reality is breaking down. But was it even real in the first place?</p>



<h2>Shared Reality as Historical Quirk</h2>



<p>Being born after World War Two could lead one to believe that shared reality is foundational for the functioning of the world and that it’s something that always existed. But there’s an argument that shared reality, on national levels, was really ushered in by the advent of broadcast media, first the radio, which was in over 50% of US households by the mid-1930s, and then the television, nuclear suburban families, and TV dinners. The hegemonic consolidation of the American dream was directly related to the projection of ABC, CBS, and NBC into each and every household.  When cable opened up TV to more than three major networks, <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1460-2466.2005.tb02677.x">we began to witness the fragmentation and polarization of broadcast media into more camps</a>, including those split along party lines, modern exemplars being Fox News and CNN.  It is key to recognize that there were distinct and differing realities in this period, split along national lines (USA and Soviet Russia), ideological lines (pro- and anti-Vietnam), and scientific lines (the impact of smoking and asbestos). Even then, it was a large number of people with a small number of shared realities.</p>



<p>The spread of national identity via broadcast media didn&#8217;t come out of the blue. It was a natural continuation of similar impacts of &#8220;The Printed Word,&#8221; which Marshall McLuhan refers to as an &#8220;Architect of Nationalism&#8221; in <a href="https://mitpress.mit.edu/books/understanding-media"><em>Understanding Media</em></a>:</p>



<blockquote class="wp-block-quote"><p>Socially, the typographic extension of man brought in nationalism, industrialism, mass markets, and universal literacy and education. For print presented an image of repeatable precision that inspired totally new forms of extending social energies.</p></blockquote>



<p>Note that the shared realities generated in the US in the 20th century weren&#8217;t only done so by national and governmental interests, but also by commercial and corporate interests: mass culture, the culture industries, culture at scale as a function of the rise of the corporation. There were strong incentives for commercial interests to create shared realities at scale across the nation because it&#8217;s easier to market and sell consumer goods, for example, to a homogeneous mass: one size fits all, one shape fits all. This was achieved through the convergence of mass media, modern marketing, and PR tactics.</p>



<p>Look no further than Edward Bernays, a double nephew of Freud who was referred to in his obituary as &#8220;the Father of Public Relations.&#8221; Bernays famously &#8220;<a href="https://www.npr.org/templates/story/story.php?storyId=4612464">used his Uncle Sigmund Freud&#8217;s ideas to help convince the public, among other things, that bacon and eggs was the true all-American breakfast</a>.&#8221; In the abstract of his 1928 paper &#8220;<a href="https://web.archive.org/web/20170407053758/http://w.truty.org/PDFs/Media/BERNAYS-ManipulatingPublicOpinion.pdf">Manipulating Public Opinion: The Why and the How</a>,&#8221; Bernays wrote:</p>



<blockquote class="wp-block-quote"><p>If the general principles of swaying public opinion are understood, a technique can be developed which, with the correct appraisal of the specific problem and the specific audience, can and has been used effectively in such widely different situations as changing the attitudes of whites toward Negroes in America, changing the buying habits of American women from felt hats to velvet, silk, and straw hats, changing the impression which the American electorate has of its President, introducing new musical instruments, and a variety of others.</p></blockquote>



<p>The Century of Marketing began, in some ways, with psychoanalytical tools, marketing as a mode of reality generation, societal homogenization, and behavioral modification. A paradigm of this is how <a href="https://www.theatlantic.com/international/archive/2015/02/how-an-ad-campaign-invented-the-diamond-engagement-ring/385376/">DeBeers convinced the West to adopt diamonds as the necessary gem for engagement rings</a>. A horrifying and still relevant example is <a href="https://www.newyorker.com/magazine/2017/10/30/the-family-that-built-an-empire-of-pain">Purdue Pharma and the Sackler dynasty&#8217;s marketing of OxyContin</a>.</p>



<p>The channels used by marketers were all of the culture industries, including broadcast media, a theme most evident in the work of <a href="https://en.wikipedia.org/wiki/Frankfurt_School">the Frankfurt School</a>, notably in that of Theodor Adorno and Max Horkheimer. Look no further than Adorno&#8217;s 1954 essay &#8220;<a href="https://users.clas.ufl.edu/burt/I%27mnotcrazy!/AdornoHowtoLookatTelevision.pdf">How to Look at Television</a>&#8220;:</p>



<blockquote class="wp-block-quote"><p>The old cultured elite does not exist any more; the modern intelligentsia only partially corresponds to it. At the same time, huge strata of the population formerly unacquainted with art have become cultural &#8220;consumers.&#8221;</p></blockquote>



<p>Although it was all the culture industries of the 20th century that worked to homogenize society at the behest of corporate interests, television was the one that we brought into our living rooms and that we eventually watched with family over dinner. Top-down reality-generation was centralized and projected into nuclear suburban homes.</p>



<p>Fast forward to today, the post-broadcast era, in which information travels close to the speed of light, in the form of lasers along fiber-optic cables and it’s both multi-platformed and personalized and everyone is a potential creator: reality, <em>once again</em>, is decentralized. In this frame, the age of shared reality was the anomaly, the exception rather than the rule. It’s perhaps ironic that one of the final throes of the age of shared reality was the advent of reality TV, a hyper-simulation of reality filtered through broadcast media. So now, in a fractured and fractal InfoLandscape, who do we look to in our efforts to establish some semblance of ground truth?</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_84736851-1048x704.jpeg" alt="" class="wp-image-13690" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_84736851-1048x704.jpeg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_84736851-300x202.jpeg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_84736851-768x516.jpeg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_84736851-1536x1032.jpeg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_84736851-2048x1376.jpeg 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /></figure>



<h2>Verified Checkmarks and Village Elders</h2>



<p><em>If our online communities are our InfoTribes, then the people we look to for ground truth are our village elders, those who tell stories around the campfire. </em></p>



<p>When COVID-19 hit, we were all scrambling around for information about reality in order to make decisions, and not only were the stakes a matter of life and death but, for every piece of information somewhere, you could find the opposite somewhere else. The majority of information, for many, came through social media feeds. Even when the source was broadcast media, a lot of the time it would be surfaced in a social media feed. Who did I pay attention to? Who did I believe? How about you? For better or for worse, I looked to my local (in an online sense) community, those whom I considered closest to me in terms of shared values and shared reality. On top of this, I looked to those respected in my communities. On Twitter, for example, I paid attention to <a href="https://twitter.com/EpiEllie">Dr Eleanor Murray</a> and <a href="https://twitter.com/NAChristakis">Professor Nicholas Christakis</a>, among many others. And why? They’re both leaders in their fields with track records of deep expertise, for one. But they also have a lot of Twitter followers and have the coveted blue verified checkmarks: in an InfoLandscape of such increasing velocity, we use rules of thumbs and heuristics around what to believe and what to not, including the validity and verifiability of the content creator, signaled by the number of followers, who the followers are (do I follow any of them? And what do I think of them?), and whether or not the platform has verified them.</p>



<p>If our online communities are our InfoTribes, then the people we look to for ground truth are our village elders, those who tell stories around the campfire. In the way they have insight into the nature of reality, we look to them as our illiterate ancestors looked to those who could read or as Pre-Reformation Christians looked to the Priests who could read Biblical Latin. With the emergence of these decentralized and fractured realities, we are seeing hand-in-hand those who rise up to define the realities of each InfoTribe. It’s no wonder the term <em>Thought Leader</em> rose to prominence as this landscape clarified itself. We are also arguably in the midst of a paradigm shift from content being the main object of verification online to content creators themselves being those verified. As Robyn Caplan points out astutely in <em><a href="https://slate.com/technology/2020/12/pornhub-verified-users-twitter.html">Pornhub Is Just the Latest Example of the Move Toward a Verified Internet</a></em>:</p>



<blockquote class="wp-block-quote"><p>It is often said that pornography drives innovation in technology, so perhaps that’s why many outlets have framed Pornhub’s verification move as “unprecedented.” However, what is happening on Pornhub is part of a broader shift online: Many, even most, platforms are using “verification” as a way to distinguish between sources, often framing these efforts within concerns about safety or trustworthiness.</p></blockquote>



<p>But mainstream journalists are more likely to be verified than independent journalists, men more likely than women, and, as Caplan points out “there is a dearth of publicly available information about the demographics of verification in general—for instance, whether BIPOC users are verified at the same rates as white users.” And it is key to note that many platforms are increasingly verifying and surfacing content created by “platform partners,“ an approach also driven by business incentives. Who decides who we listen to? And, as <a href="https://www.theguardian.com/books/2019/oct/04/shoshana-zuboff-surveillance-capitalism-assault-human-automomy-digital-privacy">Shoshana Zuboff</a> continually asks, <em>Who decides who decides?</em></p>



<p>This isn&#8217;t likely to get better anytime soon, with the retreat to private and higher signal communication channels, the next generation of personalized products, the advent of deep fakes, the increasing amount of information we&#8217;ll be getting from voice assistants over the coming 5-10 years, the proportion of information consumed via ephemeral voice-only apps such as Clubhouse, and the possibility of augmented reality playing an increasing role in our daily lives.</p>



<p>So what to do? Perhaps instead of trying to convince people of what we believe to be true, we need to stop asking &#8220;What planet are you from?&#8221; and start looking for shared foundations in our conversations, a sense of shared reality. We also have a public awareness crisis on our hands as the old methods of media literacy and education have stopped working. We need to construct new methods for people to build awareness, educate, and create the ability to dissent. Public education will need to bring to light the true contours of the emergent InfoLandscapes, some key aspects of which I have attempted to highlight in this essay. It will also likely include developing awareness of all our information platforms as multi-sided marketplaces, a growing compendium of all the informational dark patterns at play, the development of informational diets and new ways to count InfoCalories, and bringing antitrust suits against the largest reality brokers. Watch these spaces.</p>



<hr class="wp-block-separator" />



<p><em>Many thanks to Angela Bowne, Anthony Gee, Katharine Jarmul, Jamie Joyce, Mike Loukides, Emanuel Moss, and Peter Wang for their valuable and critical feedback on drafts of this essay along the way.</em></p>



<hr class="wp-block-separator" />



<h3>Footnotes</h3>



<p>1. A term first coined in 1990 by the playwright Steve Teisch and that was the Oxford Dictionaries 2016 Word of the Year (source: <a href="https://www.thenation.com/article/archive/post-truth-and-its-consequences-what-a-25-year-old-essay-tells-us-about-the-current-moment">Post-Truth and Its Consequences: What a 25-Year-Old Essay Tells Us About the Current Moment)</a><br>2. See Benedict Anderson&#8217;s <a href="https://www.versobooks.com/books/2259-imagined-communities"><em>Imagined Communities</em></a> for more about the making of nations through shared reading of print media and newspapers.<br>3. I discovered this reference in Fred Turner&#8217;s startling book&nbsp;<a rel="noreferrer noopener" href="https://press.uchicago.edu/ucp/books/book/chicago/F/bo3773600.html" target="_blank"><em>From Counterculture to Cyberculture</em></a>, which traces the countercultural roots of the internet to movements such as the New Communalists, leading many tech pioneers to have a vision of the web as &#8220;a collaborative and digital utopia modeled on the communal ideals&#8221; and &#8220;reimagined computers as tools for personal [and societal] liberation.&#8221;<br>4. There is a growing movement recognizing the importance of information flows in society. See, for example, <a href="https://courses.openmined.org/">OpenMined&#8217;s free online courses</a> which are framed around the theme that <a href="https://blog.openmined.org/society-runs-on-information-flows/">&#8220;Society runs on information flows.&#8221;</a><br>5. Think Twitter, for example, which builds communities by surfacing specific tweets for specific groups of people, a surfacing that’s driven by economic incentives, among others; although do note that TweetDeck, owned by Twitter, does not show ads, surface tweets, or recommend follows: perhaps the demographic that mostly uses TweetDeck doesn&#8217;t click on ads?<br>6. Having said this, there are some ethical constraints in the physical publishing business, for example, you can&#8217;t run an ad for a product across from an article or review of the product; there are also forms of transparency and accountability in physical publishing: we can all see what any given broadsheet publishes, discuss it, and interrogate it collectively.<br>7. Related concepts are the <a href="https://en.wikipedia.org/wiki/Tribe_(internet)">digital tribe</a>, a group of people who share common interests online, and the <a href="https://medium.com/s/world-wide-wtf/memetic-tribes-and-culture-war-2-0-14705c43f6bb">memetic tribe</a>, &#8220;a group of agents with a meme complex, or memeplex, that directly or indirectly seeks to impose its distinct map of reality—along with its moral imperatives—on others.&#8221;<br>8. Is it a coincidence that we&#8217;re also currently seeing the rise of non-linear note-taking, knowledge base, and networked thought tools, such as <a href="https://roamresearch.com/">Roam Research</a> and <a href="https://obsidian.md/">Obsidian</a>?</p>



<p></p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/infotribes-reality-brokers/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>The Next Generation of AI</title>
		<link>https://www.oreilly.com/radar/the-next-generation-of-ai/</link>
				<comments>https://www.oreilly.com/radar/the-next-generation-of-ai/#respond</comments>
				<pubDate>Tue, 09 Mar 2021 13:46:41 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides]]></dc:creator>
				<category><![CDATA[AI & ML]]></category>
		<category><![CDATA[Research]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=13676</guid>
				<description><![CDATA[Programs like AlphaZero and GPT-3 are massive accomplishments: they represent years of sustained work solving a difficult problem. But these problems are squarely within the domain of traditional AI. Playing Chess and Go or building ever-better language models have been AI projects for decades. The following projects have a different flavor: In February, PLOS Genetics [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>Programs like <a href="https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go">AlphaZero</a> and <a href="https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/">GPT-3</a> are massive accomplishments: they represent years of sustained work solving a difficult problem. But these problems are squarely within the domain of traditional AI. Playing Chess and Go or building ever-better language models have been AI projects for decades. The following projects have a different flavor:</p>



<ul><li>In February, PLOS Genetics published an article by researchers who are using GANs (Generative Adversarial Networks) to <a href="https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1009303">create artificial human genomes</a>.</li></ul>



<ul><li>Another group of researchers published an article about using NLP (natural language processing) to analyze viral genomes and, specifically, to <a href="https://www.technologyreview.com/2021/01/14/1016162/ai-language-nlp-coronavirus-hiv-flu-mutations-antinbodies-immune-vaccines/">predict the behavior of mutations</a>. They were able to distinguish between errors in &#8220;syntax&#8221; (which make the gene non-viable), and changes in semantics (which result in a viable virus that functions differently).</li></ul>



<ul><li>Yet another group of researchers modelled a small portion of a fruit fly&#8217;s brain (the part used for smell), and were able to train that to <a href="https://www.discovermagazine.com/the-sciences/fruit-fly-brain-network-hacked-for-language-processing">create a model for natural language processing</a>. This new model appears to be orders of magnitude more efficient than state-of-the-art models like GPT-3.</li></ul>



<p>The common thread through these advances is applying work in one field to another area that’s apparently unrelated—not sustained research at cracking a core AI problem. Using NLP to analyze mutations? That&#8217;s brilliant—and it&#8217;s one of those brilliant things that sounds so obvious once you think about it. And it&#8217;s an area where NLP may have a real significant advantage because it <em>doesn&#8217;t</em> actually understand language, any more than humans understand DNA.</p>



<p>The ability to create artificial human genomes is important in the short term because the human genome data available to researchers is limited by privacy laws. Synthetic genomes aren&#8217;t subject to privacy laws, because they don&#8217;t belong to any person. Data limitations aren’t a new problem; AI researchers frequently face the problem of finding sufficient data to train a model. So they have developed a lot of techniques for generating &#8220;synthetic&#8221; data: for example, cropping, rotating, or distorting pictures to get more data for image recognition. Once you’ve realized that it’s possible to create synthetic data, the jump to creating synthetic genomes isn’t far-fetched; you just have to make the connection. Asking where it might lead in the long term is even more important.</p>



<p>It&#8217;s not hard to come up with more examples of surprising work that comes from bringing techniques from one field into another. <a href="https://openai.com/blog/dall-e/">DALL-E</a> (which combines NLP with image analysis to create a new image from a description) is another example. So is <a href="https://dl.acm.org/doi/10.1145/3432202">ShadowSense</a>, which uses image analysis to let robots determine when they are touched.</p>



<p>These results suggest that we&#8217;re at the start of something new. The world isn&#8217;t a better place because computers can play Go; but it may become a better place if we can understand how our genomes work. Using adversarial techniques outside of game play or NLP techniques outside of language will inevitably lead to solving the problems we <em>actually</em> need to solve. </p>



<p>Unfortunately, that&#8217;s really only half the story. While we may be on the edge of making great advances in applications, we aren&#8217;t making the same advances in fairness and justice. Here are some key indicators:</p>



<ul><li>Attempts to train models to predict the pain that Black patients will suffer as a result of medical procedures have largely failed. Recently, research discovered that the models were more successful if they got their training data by <a href="https://www.technologyreview.com/2021/01/22/1016577/ai-fairer-healthcare-patient-outcomes/">actually listening to Black patients</a>, rather than just using records from their doctors.</li></ul>



<ul><li>A study by MIT discovered that training predictive crime models on crime reports rather than arrests doesn&#8217;t make them less racist. </li></ul>



<p>Fortunately, the doctors modeling medical pain decided to listen to their Black patients; unfortunately, that kind of listening is still rare. Listening to Black patients shouldn&#8217;t be a breakthrough akin to using NLP to analyze DNA. Why weren’t we listening to the patients in the first place? And why are the patients’ assessments of their pain so different from the doctors’?&nbsp; This is clearly progress, but more than that, it’s a sign of how much progress has yet to be made in treating minorities fairly.</p>



<p>And I&#8217;m afraid that MIT has only discovered that there aren&#8217;t any historical data sources about crime that aren&#8217;t biased, something we already knew. If you look at so-called &#8220;white collar&#8221; crime, Midtown Manhattan is the most dangerous neighborhood in New York. But that&#8217;s not where the police are spending their time.&nbsp; The only somewhat tongue-in-cheek <a href="https://whitecollar.thenewinquiry.com/static/whitepaper.pdf">paper</a> accompanying the map of <a href="https://whitecollar.thenewinquiry.com/">White Collar Crime Risk Zones</a> suggests that their next step will be using “facial features to quantify the ‘criminality’ of the individual.”&nbsp; That would clearly be a joke if such techniques weren’t already <a href="https://www.bbc.com/news/technology-53165286">under development</a>, and not just in China.</p>



<p>It looks like we&#8217;re at the cusp of some breakthroughs in AI—not new algorithms or approaches, but new ways to use the algorithms we already have. But the more things change, the more they stay the same. Our ability to think about our responsibilities of ethics and justice—and, more specifically, to put&nbsp; in place mechanisms to redress harms caused by unfair decisions–are slow to catch up.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/the-next-generation-of-ai/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>Where Programming, Ops, AI, and the Cloud are Headed in 2021</title>
		<link>https://www.oreilly.com/radar/where-programming-ops-ai-and-the-cloud-are-headed-in-2021/</link>
				<comments>https://www.oreilly.com/radar/where-programming-ops-ai-and-the-cloud-are-headed-in-2021/#respond</comments>
				<pubDate>Mon, 25 Jan 2021 12:03:14 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides]]></dc:creator>
				<category><![CDATA[AI & ML]]></category>
		<category><![CDATA[Operations]]></category>
		<category><![CDATA[Web Programming]]></category>
		<category><![CDATA[Research]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=13616</guid>
				<description><![CDATA[In this report, we look at the data generated by the O’Reilly online learning platform to discern trends in the technology industry—trends technology leaders need to follow. But what are “trends”? All too often, trends degenerate into horse races over languages and platforms. Look at all the angst heating up social media when TIOBE or [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>In this report, we look at the data generated by the <a href="https://learning.oreilly.com/home/">O’Reilly online learning platform</a> to discern trends in the technology industry—trends technology leaders need to follow. </p>



<p>But what are “trends”? All too often, trends degenerate into horse races over languages and platforms. Look at all the angst heating up social media when TIOBE or RedMonk releases their reports on language rankings. Those reports are valuable, but their value isn’t in knowing what languages are popular in any given month. And that’s what I’d like to get to here: the real trends that aren’t reflected (or at best, are indirectly reflected) by the horse races. Sometimes they’re only apparent if you look carefully at the data; sometimes it’s just a matter of keeping your ear to the ground.</p>



<p>In either case, there’s a difference between “trends” and “trendy.” Trendy, fashionable things are often a flash in the pan, forgotten or regretted a year or two later (like <a href="https://en.wikipedia.org/wiki/Pet_Rock">Pet Rocks</a> or <a href="https://en.wikipedia.org/wiki/Chia_Pet">Chia Pets</a>). Real trends unfold on much longer time scales and may take several steps backward during the process: civil rights, for example. Something is happening and, over the long arc of history, it’s not going to stop. In our industry, cloud computing might be a good example.</p>



<h3>Methodology</h3>



<p>This study is based on title usage on O’Reilly online learning. The data includes all usage of our platform, not just content that O’Reilly has published, and certainly not just books. We’ve explored usage across all publishing partners and learning modes, from live training courses and online events to interactive functionality provided by Katacoda and Jupyter notebooks. We’ve included search data in the graphs, although we have avoided using search data in our analysis. Search data is distorted by how quickly customers find what they want: if they don’t succeed, they may try a similar search with many of the same terms. (But don’t even think of searching for R or C!) Usage data shows what content our members actually use, though we admit it has its own problems: usage is biased by the content that’s available, and there’s no data for topics that are so new that content hasn’t been developed.</p>



<p>We haven’t combined data from multiple terms. Because we’re doing simple pattern matching against titles, usage for “AWS security” is a subset of the usage for “security.” We made a (very) few exceptions, usually when there are two different ways to search for the same concept. For example, we combined “SRE” with “site reliability engineering,” and “object oriented” with “object-oriented.”</p>



<p>The results are, of course, biased by the makeup of the user population of O’Reilly online learning itself. Our members are a mix of individuals (professionals, students, hobbyists) and corporate users (employees of a company with a corporate account). We suspect that the latter group is somewhat more conservative than the former. In practice, this means that we may have less meaningful data on the latest JavaScript frameworks or the newest programming languages. New frameworks appear every day (literally), and our corporate clients won’t suddenly tell their staff to reimplement the ecommerce site just because last year’s hot framework is no longer fashionable.</p>



<p>Usage and query data for each group are normalized to the highest value in each group. Practically, this means that you can compare topics within a group, but you can’t compare the groups with each other. Year-over-year (YOY) growth compares January through September 2020 with the same months of 2019. Small fluctuations (under 5% or so) are likely to be noise rather than a sign of a real trend.</p>



<p>Enough preliminaries. Let’s look at the data, starting at the highest level: O’Reilly online learning itself.</p>



<h3>O’Reilly Online Learning</h3>



<p>Usage of O’Reilly online learning grew steadily in 2020, with 24% growth since 2019. That may not be surprising, given the COVID-19 pandemic and the resulting changes in the technology industry. Companies that once resisted working from home were suddenly shutting down their offices and asking their staff to work remotely. Many have said that remote work will remain an option indefinitely. COVID had a significant effect on training: in-person training (whether on- or off-site) was no longer an option, so organizations of all sizes increased their participation in live online training, which grew by 96%. More traditional modes also saw increases: usage of books increased by 11%, while videos were up 24%. We also added two new learning modes, Katacoda scenarios and Jupyter notebooks, during the year; we don’t yet have enough data to see how they’re trending. </p>



<p>It’s important to place our growth data in this context. We frequently say that 10% growth in a topic is “healthy,” and we’ll stand by that, but remember that O’Reilly online learning itself showed 24% growth. So while a technology whose usage is growing 10% annually is healthy, it’s not keeping up with the platform.</p>



<p>As travel ground to a halt, so did traditional in-person conferences. We closed our conference business in March, replacing it with live virtual Superstreams. While we can’t compare in-person conference data with virtual event data, we can make a few observations. The most successful superstream series focused on <a href="https://learning.oreilly.com/live-training/courses/software-architecture-superstream-series/0636920444961/">software architecture</a> and <a href="https://learning.oreilly.com/live-training/courses/oreilly-infrastructure-ops-superstream-series/0636920410027/">infrastructure and operations</a>. Why? The in-person O’Reilly Software Architecture Conference was small but growing. But when the pandemic hit, companies found out that they really were online businesses—and if they weren’t, they had to become online to survive. Even small restaurants and farm markets were adding online ordering features to their websites. Suddenly, the ability to design, build, and operate applications at scale wasn’t optional; it was necessary for survival. </p>



<h3><strong>Programming Languages</strong></h3>



<p>Although we’re not fans of the language horse race, programming languages are as good a place as any to start. Figure 1 shows usage, year-over-year growth in usage, and the number of search queries for several popular languages. The top languages for O’Reilly online learning are Python (up 27%), Java (down 3%), C++ (up 10%), C (up 12%), and JavaScript (up 40%). Looking at 2020 usage rather than year-over-year changes, it’s surprising to see JavaScript so far behind Python and Java. (JavaScript usage is 20% of Python’s, and 33% of Java’s.) </p>



<p>Past the top five languages, we see healthy growth in Go (16%) and Rust (94%). Although we believe that Rust’s popularity will continue to grow, don’t get too excited; it’s easy to grow 94% when you’re starting from a small base. Go has clearly established itself, particularly as a language for concurrent programming, and Rust is likely to establish itself for “system programming”: building new operating systems and tooling for cloud operations. Julia, a language designed for mathematical computation, is an interesting wild card. It’s slightly down over the past year, but we’re optimistic about its long term chances.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-1-1048x748.png" alt="" class="wp-image-13620" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-1-1048x748.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-1-300x214.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-1-768x548.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-1.png 1388w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><br><em>Figure 1. Programming languages</em></figcaption></figure>



<p>We shouldn’t separate usage of titles specifically aimed at learning a programming language from titles applying the language or using frameworks based on it. After all, many Java developers use Spring, and searching for “Java” misses content only has the word “Spring” in the title. The same is true for JavaScript, with the React, Angular, and Node.js frameworks. With Python, the most heavily used libraries are PyTorch and scikit-learn. Figure 2 shows what happens when you add the use of content about Python, Java, and JavaScript to the most important frameworks for those languages.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-2-1048x758.png" alt="" class="wp-image-13621" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-2-1048x758.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-2-300x217.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-2-768x555.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-2.png 1384w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><br><em>Figure 2. Programming languages and frameworks combined</em></figcaption></figure>



<p>It probably isn’t a surprise that the results are similar, but there are some key differences. Adding usage and search query data for Spring (up 7%) reverses Java’s apparent decline (net-zero growth). Zero growth isn’t inappropriate for an established enterprise language, particularly one owned by a company that has mired the language in controversy. Looking further at JavaScript, if you add in usage for the most popular frameworks (React, Angular, and Node.js), JavaScript usage on O’Reilly online learning rises to 50% of Python’s, only slightly behind Java and its frameworks. However, Python, when added to the heavily used frameworks PyTorch and scikit-learn, remains the clear leader.</p>



<p>It’s important to understand what we’ve done though. We’re trying to build a more comprehensive picture of language use that includes the use of various frameworks. We’re not pretending the frameworks themselves are comparable—Spring is primarily for backend and middleware development (though it includes a web framework); React and Angular are for frontend development; and scikit-learn and PyTorch are machine learning libraries. And although it’s widely used, we didn’t assign TensorFlow to any language; it has bindings for Python, Java, C++, and JavaScript, and it’s not clear which language predominates. (Google Trends suggests C++.) We also ignored thousands (literally) of minor platforms, frameworks, and libraries for all these languages; once you get past the top few, you’re into the noise.</p>



<p>We aren’t advocating for Python, Java, or any other language. None of these top languages are going away, though their stock may rise or fall as fashions change and the software industry evolves. We’re just saying that when you make comparisons, you have to be careful about exactly what you’re comparing. The horse race? That’s just what it is. Fun to watch, and have a mint julep when it’s over, but don’t bet your savings (or your job) on it.</p>



<p>If the horse race isn’t significant, just what <em>are </em>the important trends for programming languages? We see several factors changing pro‐ gramming in significant ways:</p>



<ul><li><em>Multiparadigm languages</em><br>Since last year, O’Reilly online learning has seen a 14% increase in the use of content on functional programming. However, Haskell and Erlang, the classic functional languages, aren’t where the action is; neither shows significant usage, and both are headed down (roughly 20% decline year over year). Object oriented programming is up even more than functional programming: 29% growth since last year. This suggests that the real story is the integration of functional features into procedural and object-oriented languages. Starting with Python 3.0 in 2008 and continuing with Java 8 in 2014, programming languages have added higher-order functions (lambdas) and other “functional” features. Several popular languages (including JavaScript and Go) have had functional features from the beginning. This trend started over 20 years ago (with the Standard Template Library for C++), and we expect it to continue.<br></li><li><em>Concurrent programming</em><br>Platform data for concurrency shows an 8% year-over-year increase. This isn’t a large number, but don’t miss the story because the numbers are small. Java was the first widely used language to support concurrency as part of the language. In the mid-’90s, thread support was a luxury; Moore’s law had plenty of room to grow. That’s no longer the case, and support for concurrency, like support for functional programming, has become table stakes. Go, Rust, and most other modern languages have built-in support for concurrency. Concurrency has always been one of Python’s weaknesses.<br></li><li><em>Dynamic versus static typing </em><br>This is another important paradigmatic axis. The distinction between languages with dynamic typing (like Ruby and JavaScript) and statically typed languages (like Java and Go) is arguably more important than the distinction between functional and object-oriented languages. Not long ago, the idea of adding static typing to dynamic languages would have started a brawl. No longer. Combining paradigms to form a hybrid is taking a hold here too. Python 3.5 added type hinting, and more recent versions have added additional static typing features. TypeScript, which adds static typing to JavaScript, is coming into its own (12% year-over-year increase).<br></li><li><em>Low-code and no-code computing</em><br>It’s hard for a learning platform to gather data about a trend that minimizes the need to learn, but low-code is real and is bound to have an effect. Spreadsheets were the forerunner of low-code computing. When VisiCalc was first released in 1979, it enabled millions to do significant and important computation without learning a programming language. Democratization is an important trend in many areas of technology; it would be surprising if programming were any different. </li></ul>



<p>What’s important isn’t the horse race so much as the features that languages are acquiring, and why. Given that we’ve run to the end of <a href="https://en.wikipedia.org/wiki/Moore%27s_law">Moore’s law</a>, concurrency will be central to the future of programming. We can’t just get faster processors. We’ll be working with microservices and serverless/functions-as-a-service in the cloud for a long time–and these are inherently concurrent systems. Functional programming doesn’t solve the problem of concurrency—but the discipline of immutability certainly helps avoid pitfalls. (And who doesn’t love first-class functions?) As software projects inevitably become larger and more complex, it makes eminent sense for languages to extend themselves by mixing in functional features. We need programmers who are thinking about how to use functional and object-oriented features together; what practices and patterns make sense when building enterprise-scale concurrent software?</p>



<p>Low-code and no-code programming will inevitably change the nature of programming and programming languages:</p>



<ul><li>There will be new languages, new libraries, and new tools to support no- or low-code programmers. They’ll be very simple. (Horrors, will they look like BASIC? Please no.) Whatever form they take, it will take programmers to build and maintain them.<br></li><li>We’ll certainly see sophisticated computer-aided coding as an aid to experienced programmers. Whether that means &#8220;<a href="https://www.oreilly.com/radar/pair-programming-with-ai/">pair programming with a machine</a>&#8221; or algorithms that can write <a href="https://www.oreilly.com/radar/automated-coding-and-the-future-of-programming/">simple programs on their own</a> remains to be seen. These tools won’t eliminate programmers; they’ll make programmers more productive.</li></ul>



<p>There will be a predictable <a href="https://www.wired.com/story/databases-coding-real-programming-myth/">backlash against letting the great unwashed</a> into the programmers’ domain. Ignore it. Low-code is part of a democratization movement that puts the power of computing into more peoples’ hands, and that’s almost always a good thing. Programmers who realize what this movement means won’t be put out of jobs by nonprogrammers. They’ll be the ones becoming more productive and writing the tools that others will use.</p>



<p>Whether you’re a technology leader or a new programmer, pay attention to these slow, long-term trends. They’re the ones that will change the face of our industry. </p>



<h3><strong>Operations or DevOps or SRE</strong></h3>



<p>The science (or art) of IT operations has changed radically in the last decade. There’s been a lot of discussion about operations culture (the movement frequently known as DevOps), continuous integration and deployment (CI/CD), and site reliability engineering (SRE). Cloud computing has replaced data centers, colocation facilities, and in-house machine rooms. Containers allow much closer integration between developers and operations and do a lot to standardize deployment.</p>



<p>Operations isn’t going away; there’s no such thing as <a href="http://radar.oreilly.com/2012/06/what-is-devops.html">NoOps</a>. Technologies like Function as a Service (a.k.a. FaaS, a.k.a. serverless, a.k.a. AWS Lambda) only change the nature of the beast. The number of people needed to manage an infrastructure of a given size has shrunk, but the infrastructures we’re building have expanded, sometimes by orders of magnitude. It’s easy to round up tens of thousands of nodes to train or deploy a complex AI application. Even if those machines are all in Amazon’s giant data centers and managed in bulk using highly automated tools, operations staff still need to keep systems running smoothly, monitoring, troubleshooting, and ensuring that you’re not <a href="https://thenewstack.io/is-cloud-waste-inevitable-as-companies-move-to-the-cloud/">paying for resources you don’t need</a>. Serverless and other cloud technologies allow the same operations team to manage much larger infrastructures; they don’t make operations go away.</p>



<p>The terminology used to describe this job fluctuates, but we don’t see any real changes. The term “DevOps” has fallen on hard times. Usage of DevOps-titled content in O’Reilly online learning has dropped by 17% in the past year, while SRE (including “site reliability engineering”) has climbed by 37%, and the term “operations” is up 25%. While SRE and DevOps are distinct concepts, for many customers SRE is DevOps at Google scale–and who doesn’t want that kind of growth? Both SRE and DevOps emphasize similar practices: version control (62% growth for GitHub, and 48% for Git), testing (high usage, though no year-over-year growth), continuous deployment (down 20%), monitoring (up 9%), and observability (up 128%). <a href="https://en.wikipedia.org/wiki/Terraform_(software)">Terraform</a>, HashiCorp’s open source tool for automating the configuration of cloud infrastructure, also shows strong (53%) growth.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-3-1048x788.png" alt="" class="wp-image-13622" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-3-1048x788.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-3-300x225.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-3-768x577.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-3.png 1385w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><br><em>Figure 3. Operations, DevOps, and SRE </em></figcaption></figure>



<p>It’s more interesting to look at the story the data tells about the tools. Docker is close to flat (5% decline year over year), but usage of content about containers skyrocketed by 99%. So yes, containerization is clearly a big deal. Docker itself may have stalled—we’ll know more next year—but Kubernetes’s dominance as the tool for container orchestration keeps containers central. Docker was the enabling technology, but Kubernetes made it possible to deploy containers at scale.</p>



<p>Kubernetes itself is the other superstar, with 47% growth, along with the highest usage (and the most search queries) in this group. Kubernetes isn’t just an orchestration tool; it’s the <a href="https://www.itproportal.com/features/kubernetes-as-a-cloud-native-operating-system/">cloud’s operating system</a> (or, as Kelsey Hightower has <a href="https://twitter.com/kelseyhightower/status/775487754868133888">said</a>, “Kubernetes will be the Linux of distributed systems”). But the data doesn’t show the number of conversations we’ve had with people who think that Kubernetes is just “too complex.” We see three possible solutions:</p>



<ul><li>A “simplified” version of Kubernetes that isn’t as flexible, but trades off a lot of the complexity. <a href="https://k3s.io/">K3s</a> is a possible step in this direction. The question is, What’s the trade-off? Here’s my version of the <a href="https://en.wikipedia.org/wiki/Pareto_principle">Pareto principle</a>, also known as the 80/20 rule. Given any system (like Kubernetes), it’s usually possible to build something simpler by keeping the most widely used 80% of the features and cutting the other 20%. And some applications will fit within the 80% of the features that were kept. But most applications (maybe 80% of them?) will require at least one of the features that were sacrificed to make the system simpler.<br></li><li>An entirely new approach, some tool that isn’t yet on the horizon. We have no idea what that tool is. In Yeats’s words, “What rough beast&#8230;slouches towards Bethlehem to be born”? <br></li><li>An integrated solution from a cloud vendor (for example, Microsoft’s open source <a href="https://thenewstack.io/the-dapr-distributed-runtime-nears-production-readiness/">Dapr distributed runtime</a>). I don’t mean cloud vendors that provide Kubernetes as a service; we already have those. What if the cloud vendors integrate Kubernetes’s functionality into their stack in such a way that that functionality disappears into some kind of management console? Then the question becomes, What features do you lose, and do you need them? And what kind of vendor lock-in games do you want to play? </li></ul>



<p>The rich ecosystem of tools surrounding Kubernetes (Istio, Helm, and others) shows how valuable it is. But where do we go from here? Even if Kubernetes is the right tool to manage the complexity of modern applications that run in the cloud, the desire for simpler solutions will eventually lead to higher-level abstractions. Will they be adequate?</p>



<p><a href="https://thenewstack.io/observability-a-3-year-retrospective/">Observability</a> saw the greatest growth in the past year (128%), while monitoring is only up 9%. While observability is a richer, more powerful capability than monitoring—observability is the ability to find the information you need to analyze or debug software, while monitoring requires predicting in advance what data will be useful—we suspect that this shift is largely cosmetic. “Observability” risks becoming the new name for monitoring. And that’s <a href="https://www.honeycomb.io/blog/observability-whats-in-a-name/">unfortunate</a>. If you think observability is merely a more fashionable term for monitoring, you’re missing its value. Complex systems running in the cloud will need true observability to be manageable.</p>



<p>Infrastructure is code, and we’ve seen plenty of tools for automating configuration. But Chef and Puppet, two leaders in this movement, are both significantly down (49% and 40% respectively), as is Salt. Ansible is the only tool from this group that’s up (34%). Two trends are responsible for this. Ansible appears to have supplanted Chef and Puppet, possibly because Ansible is multilingual, while Chef and Puppet are tied to Ruby. Second, Docker and Kubernetes have changed the configuration game. Our data shows that Chef and Puppet peaked in 2017, when Kubernetes started an almost exponential growth spurt, as Figure 4 shows. (Each curve is normalized separately to 1; we wanted to emphasize the inflection points rather than compare usage.) Containerized deployment appears to minimize the problem of reproducible configuration, since a container is a complete software package. You have a container; you can deploy it many times, getting the same result each time. In reality, it’s never that simple, but it certainly looks that simple–and that apparent simplicity reduces the need for tools like Chef and Puppet.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-4-1048x782.png" alt="" class="wp-image-13623" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-4-1048x782.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-4-300x224.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-4-768x573.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-4.png 1392w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><br><em>Figure 4. Docker and Kubernetes versus Chef and Puppet</em></figcaption></figure>



<p>The biggest challenge facing operations teams in the coming year, and the biggest challenge facing data engineers, will be learning how to deploy AI systems effectively. In the past decade, a lot of ideas and technologies have come out of the DevOps movement: the source repository as the single source of truth, rapid automated deployment, constant testing, and more. They’ve been very effective, but AI breaks the assumptions that lie behind them, and deployment is frequently the greatest barrier to AI success.</p>



<p>AI breaks these assumptions because data is more important than code. We don’t yet have adequate tools for versioning data (though <a href="https://dvc.org/">DVC</a> is a start). Models are neither code nor data, and we don’t have adequate tools for versioning models either (though tools like <a href="https://mlflow.org/">MLflow</a> are a start). Frequent deployment assumes that the software can be built relatively quickly, but training a model can take days. It’s been suggested that model training doesn’t need to be part of the build process, but that’s really the most important part of the application. Testing is critical to continuous deployment, but the behavior of AI systems is probabilistic, not deterministic, so it’s harder to say that this test or that test failed. It’s particularly difficult if testing includes issues like fairness and bias.</p>



<p>Although there is a nascent <a href="https://en.wikipedia.org/wiki/MLOps">MLOps</a> movement, our data doesn’t show that people are using (or searching for) content in these areas in significant numbers. Usage is easily explainable; in many of these areas, content doesn’t exist yet. But users will search for content whether or not it exists, so the small number of searches shows that most of our users aren’t yet aware of the problem. Operations staff too frequently assume that an AI system is just another application—but they’re wrong. And AI developers too frequently assume that an operations team will be able to deploy their software, and they’ll be able to move on to the next project—but they’re also wrong. This situation is a train wreck in slow motion, and the big question is whether we can stop the trains before they crash. These problems will be solved eventually, with a new generation of tools—indeed, those tools are already being built—but we’re not there yet.</p>



<h3><strong>AI, Machine Learning, and Data</strong></h3>



<p>Healthy growth in artificial intelligence has continued: machine learning is up 14%, while AI is up 64%; data science is up 16%, and statistics is up 47%. While AI and machine learning are distinct concepts, there’s enough confusion about definitions that they’re frequently used interchangeably. We informally define machine learning as “the part of AI that works”; AI itself is more research oriented and aspirational. If you accept that definition, it’s not surprising that content about machine learning has seen the heaviest usage: it’s about taking research out of the lab and putting it into practice. It’s also not surprising that we see solid growth for AI, because that’s where bleeding-edge engineers are looking for new ideas to turn into machine learning.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-5-1048x760.png" alt="" class="wp-image-13624" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-5-1048x760.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-5-300x217.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-5-768x557.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-5.png 1374w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><br><em>Figure 5. Artificial intelligence, machine learning, and data</em></figcaption></figure>



<p>Have the skepticism, fear, and criticism surrounding AI taken a toll, or are “reports of AI’s death greatly exaggerated”? We don’t see that in our data, though there are certainly some metrics to say that <a href="https://science.sciencemag.org/content/368/6494/927">artificial intelligence has stalled</a>. Many projects never make it to production, and while the last year has seen amazing progress in natural language processing (up 21%), such as OpenAI’s GPT-3, we’re seeing fewer spectacular results like winning Go games. It’s possible that AI (along with machine learning, data, big data, and all their fellow travelers) is descending into the trough of the hype cycle. We don’t think so, but we’re prepared to be wrong. As <a href="https://www.linkedin.com/in/benlorica/">Ben Lorica</a> has said (in conversation), many years of work will be needed to bring current research into commercial products.</p>



<p>It’s certainly true that there’s been a (deserved) backlash over heavy handed use of AI. A backlash is only to be expected when deep learning applications are used to justify <a href="https://www.npr.org/2020/06/24/882683463/the-computer-got-it-wrong-how-facial-recognition-led-to-a-false-arrest-in-michig">arresting the wrong people</a>, and when some police departments are comfortable using software with a <a href="https://www.theverge.com/2018/7/5/17535814/uk-face-recognition-police-london-accuracy-completely-comfortable">98% false positive rate</a>. A backlash is only to be expected when software systems designed to maximize “engagement” end up spreading misinformation and conspiracy theories. A backlash is only to be expected when software developers don’t take into account issues of power and abuse. And a backlash is only to be expected when too many executives see AI as a “magic sauce” that will turn their organization around without pain or, frankly, a whole lot of work.</p>



<p>But we don’t think those issues, as important as they are, say a lot about the future of AI. The future of AI is less about breathtaking breakthroughs and creepy face or voice recognition than it is about small, mundane applications. Think quality control in a factory; think intelligent search <a href="https://learning.oreilly.com/answers/search/">on O’Reilly online learning</a>; think <a href="https://www.dpreview.com/news/5756257699/nvidia-research-develops-a-neural-network-to-replace-traditional-video-compression">optimizing data compression</a>; think <a href="https://www.technologyreview.com/2020/10/16/1010617/ai-image-recognition-construction-computer-vision-costs-delays/">tracking progress on a construction site</a>. I’ve seen too many articles saying that AI hasn’t helped in the struggle against COVID, as if someone was going to click a button on their MacBook and a superdrug was going to pop out of a USB-C port. (And AI has played a huge role in <a href="https://spectrum.ieee.org/artificial-intelligence/medical-ai/what-ai-can-and-cant-do-in-the-race-for-a-coronavirus-vaccine">COVID vaccine development</a>.) AI is playing an important supporting role—and that’s exactly the role we should expect. It’s enabling researchers to navigate tens of thousands of research papers and reports, design drugs and engineer genes that might work, and <a href="https://theconversation.com/teaching-computers-to-read-health-records-is-helping-fight-covid-19-heres-how-147385">analyze millions of health records</a>. Without automating these tasks, getting to the end of the pandemic will be impossible.</p>



<p>So here’s the future we see for AI and machine learning:</p>



<ul><li>Natural language has been (and will continue to be) a big deal. GPT-3 has changed the world. We’ll see AI being used to create “fake news,” and we’ll find that AI gives us the best tools for detecting what’s fake and what isn’t.<br></li><li>Many companies are placing significant bets on using AI to automate customer service. We’ve made great strides in our ability to synthesize speech, generate realistic answers, and search for solutions.<br></li><li>We’ll see lots of tiny, embedded AI systems in everything from medical sensors to appliances to factory floors. Anyone interested in the future of technology should watch <a href="https://learning.oreilly.com/library/view/tinyml/9781492052036/">Pete Warden’s work on TinyML</a> very carefully.<br></li><li>We still haven’t faced squarely the issue of user interfaces for collaboration between humans and AI. We don’t want AI oracles that just replace human errors with machine-generated errors at scale; we want the ability to collaborate with AI to produce results better than either humans or machines could alone. Researchers are <a href="http://jessylin.com/2020/06/08/rethinking-human-ai-interaction/">starting to catch on</a>.</li></ul>



<p>TensorFlow is the leader among machine learning platforms; it gets the most searches, while usage has stabilized at 6% growth. Content about scikit-learn, Python’s machine learning library, is used almost as heavily, with 11% year-over-year growth. PyTorch is in third place (yes, this is a horse race), but usage of PyTorch content has gone up 159% year over year. That increase is no doubt influenced by the popularity of Jeremy Howard’s <a href="https://course.fast.ai/">Practical Deep Learning for Coders</a> course and the PyTorch-based fastai library (no data for 2019). It also appears that PyTorch is more popular among researchers, while TensorFlow remains dominant in production. But as Jeremy’s students move into industry, and as researchers migrate toward production positions, we expect to see the balance between PyTorch and TensorFlow shift. </p>



<p><a href="https://kafka.apache.org/">Kafka</a> is a crucial tool for building data pipelines; it’s stable, with 6% growth and usage similar to Spark. Pulsar, Kafka’s “next generation” competition, isn’t yet on the map.</p>



<p>Tools for automating AI and machine learning development (IBM’s <a href="https://www.ibm.com/cloud/watson-studio/autoai">AutoAI</a>, Google’s <a href="https://cloud.google.com/automl">Cloud AutoML</a>, Microsoft’s <a href="https://www.microsoft.com/en-us/research/project/automl/">AutoML</a>, and Amazon’s <a href="https://aws.amazon.com/sagemaker/">SageMaker</a>) have gotten a lot of press attention in the past year, but we don’t see any signs that they’re making a significant dent in the market. That content usage is nonexistent isn’t a surprise; O’Reilly members can’t use content that doesn’t exist. But our members aren’t searching for these topics either. It may be that AutoAI is relatively new or that users don’t think they need to search for supplementary training material.</p>



<p>What about data science? The report <em>What Is Data Science </em>is a decade old, but surprisingly for a 10-year-old paper, views are up 142% over 2019. The tooling has changed though. Hadoop was at the center of the data science world a decade ago. It’s still around, but now it’s a legacy system, with a 23% decline since 2019. Spark is now the dominant data platform, and it’s certainly the tool engineers want to learn about: usage of Spark content is about three times that of Hadoop. But even Spark is down 11% since last year. <a href="https://ray.io/">Ray</a>, a newcomer that promises to make it easier to build distributed applications, doesn’t yet show usage to match Spark (or even Hadoop), but it does show 189% growth. And there are other tools on the horizon: <a href="https://dask.org/">Dask</a> has seen nearly 400% growth.</p>



<p>It’s been exciting to watch the discussion of data ethics and activism in the past year. Broader societal movements (such as #BlackLivesMatter), along with increased industry awareness of diversity and inclusion, have made it more difficult to ignore issues like fairness, power, and transparency. What’s sad is that our data shows little evidence that this is more than a discussion. Usage of general content (not specific to AI and ML) about diversity and inclusion is up significantly (87%), but the absolute numbers are still small. Topics like ethics, fairness, transparency, and explainability don’t make a dent in our data. That may be because few books have been published and few training courses have been offered—but that’s a problem in itself.</p>



<h3><strong>Web Development</strong></h3>



<p>Since the invention of HTML in the early 1990s, the first web servers, and the first browsers, the web has exploded (or degenerated) into a proliferation of platforms. Those platforms make web development infinitely more flexible: They make it possible to support a host of devices and screen sizes. They make it possible to build sophisticated applications that run in the browser. And with every new year, “desktop” applications look more old-fashioned.</p>



<p>So what does the world of web frameworks look like? React leads in usage of content and also shows significant growth (34% year over year). Despite rumors that Angular is fading, it’s the #2 platform, with 10% growth. And usage of content about the server-side platform Node.js is just behind Angular, with 15% growth. None of this is surprising.</p>



<p>It’s more surprising that Ruby on Rails shows extremely strong growth (77% year over year) after several years of moderate, stable performance. Likewise, Django (which appeared at roughly the same time as Rails) shows both heavy usage and 63% growth. You might wonder whether this growth holds for all older platforms; it doesn’t. Usage of content about PHP is relatively low and declining (8% drop), even though it’s still used by <a href="https://w3techs.com/technologies/details/pl-php">almost 80%</a> of all websites. (It will be interesting to see how <a href="https://www.php.net/archive/2020.php">PHP 8</a> changes the picture.) And while jQuery shows healthy 18% growth, usage of jQuery content was lower than any other platform we looked at. (Keep in mind, though, that there are literally thousands of web platforms. A complete study would be either heroic or foolish. Or both.)</p>



<p><a href="https://vuejs.org/">Vue</a> and <a href="https://palletsprojects.com/p/flask/">Flask</a> make surprisingly weak showings: for both platforms, content usage is about one-eighth of React’s. Usage of Vue-related content declined 13% in the past year, while Flask grew 10%. Neither is challenging the dominant players. It’s tempting to think of Flask and Vue as “new” platforms, but they were released in 2010 and 2014, respectively; they’ve had time to establish themselves. Two of the most promising new platforms, <a href="https://svelte.dev/">Svelte</a> and <a href="https://nextjs.org/">Next.js</a>, don’t yet produce enough data to chart—possibly because there isn’t yet much content to use. Likewise, <a href="https://webassembly.org/">WebAssembly</a> (Wasm) doesn’t show up. (It’s also too new, with little content or training material available.) But WebAssembly represents a major rethinking of web programming and bears watching closely. Could WebAssembly turn JavaScript’s dominance of web development on its head? We suspect that nothing will happen quickly. Enterprise customers will be reluctant to bear the cost of moving from an older framework like PHP to a more fashionable JavaScript framework. It costs little to stick with an old stalwart.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-6-1048x761.png" alt="" class="wp-image-13625" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-6-1048x761.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-6-300x218.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-6-768x558.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-6.png 1368w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><br><em>Figure 6. Web development</em></figcaption></figure>



<p>The foundational technologies HTML, CSS, and JavaScript are all showing healthy growth in usage (22%, 46%, and 40%, respectively), though they’re behind the leading frameworks. We’ve already noted that JavaScript is one of the top programming languages—and the modern web platforms are nothing if not the apotheosis of JavaScript. We find that chilling. The original vision for the World Wide Web was radically empowering and democratizing. You didn’t need to be a techno-geek; you didn’t even need to program—you could just click “view source” in the browser and copy bits you liked from other sites. Twenty-five years later, that’s no longer true: you can still “view source,” but all you’ll see is a lot of incomprehensible JavaScript. Ironically, just as other technologies are democratizing, web development is increasingly the domain of programmers. Will that trend be reversed by a new generation of platforms, or by a reformulation of the web itself? We shall see.</p>



<h3><strong>Clouds of All Kinds</strong></h3>



<p>It’s no surprise that the cloud is growing rapidly. Usage of content about the cloud is up 41% since last year. Usage of cloud titles that don’t mention a specific vendor (e.g., Amazon Web Services, Microsoft Azure, or Google Cloud) grew at an even faster rate (46%). Our customers don’t see the cloud through the lens of any single platform. We’re only at the beginning of cloud adoption; while <a href="https://www.oreilly.com/radar/cloud-adoption-in-2020/">most companies</a> are using cloud services in some form, and many have moved significant business-critical applications and datasets to the cloud, we have a long way to go. If there’s one technology trend you need to be on top of, this is it.</p>



<p>The horse race between the leading cloud vendors, AWS, Azure, and Google Cloud, doesn’t present any surprises. Amazon is winning, even ahead of the generic “cloud”—but Microsoft and Google are catching up, and Amazon’s growth has stalled (only 5%). Use of content about Azure shows 136% growth—more than any of the competitors—while Google Cloud’s 84% growth is hardly shabby. When you dominate a market the way AWS dominates the cloud, there’s nowhere to go but down. But with the growth that Azure and Google Cloud are showing, Amazon’s dominance could be short-lived.</p>



<p>What’s behind this story? Microsoft has done an excellent job of reinventing itself as a cloud company. In the past decade, it’s rethought every aspect of its business: Microsoft has become a leader in open source; it owns GitHub; it owns LinkedIn. It’s hard to think of any corporate transformation so radical. This clearly isn’t the Microsoft that declared Linux a “cancer,” and that Microsoft could never have succeeded with Azure.</p>



<p>Google faces a different set of problems. Twelve years ago, the company arguably delivered serverless with App Engine. It open sourced Kubernetes and bet very heavily on its leadership in AI, with the leading AI platform TensorFlow highly optimized to run on Google hardware. So why is it in third place? Google’s problem hasn’t been its ability to deliver leading-edge technology but rather its ability to reach customers—a problem that Thomas Kurian, Google Cloud’s CEO, is <a href="https://www.crn.com/news/cloud/how-thomas-kurian-s-quite-simple-strategy-is-transforming-google-cloud">attempting to address</a>. Ironically, part of Google’s customer problem is its focus on engineering to the detriment of the customers themselves. Any number of people have told us that they stay away from Google because they’re too likely to say, “Oh, that service you rely on? We’re shutting it down; we have a better solution.” Amazon and Microsoft don’t do that; they understand that a cloud provider has to support legacy software, and that all software is legacy the moment it’s released.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-7-1048x776.png" alt="" class="wp-image-13626" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-7-1048x776.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-7-300x222.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-7-768x569.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-7.png 1376w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><br><em>Figure 7. Cloud usage </em></figcaption></figure>



<p>While our data shows very strong growth (41%) in usage for content about the cloud, it doesn’t show significant usage for terms like “multicloud” and “hybrid cloud” or for specific hybrid cloud products like Google’s <a href="https://cloud.google.com/anthos">Anthos</a> or Microsoft’s <a href="https://azure.microsoft.com/en-us/services/azure-arc/">Azure Arc</a>. These are new products, for which little content exists, so low usage isn’t surprising. But the usage of specific cloud technologies isn’t that important in this context; what’s more important is that usage of all the cloud platforms is growing, particularly content that isn’t tied to any vendor. We also see that our corporate clients are using content that spans all the cloud vendors; it’s difficult to find anyone who’s looking at a single vendor.</p>



<p>Not long ago, we were skeptical about hybrid and multicloud. It’s easy to assume that these concepts are pipe dreams springing from the minds of vendors who are in second, third, fourth, or fifth place: if you can’t win customers from Amazon, at least you can get a slice of their business. That story isn’t compelling—but it’s also the wrong story to tell. Cloud computing is hybrid by nature. Think about how companies “get into the cloud.” It’s often a chaotic grassroots process rather than a carefully planned strategy. An engineer can’t get the resources for some project, so they create an AWS account, billed to the company credit card. Then someone in another group runs into the same problem, but goes with Azure. Next there’s an acquisition, and the new company has built its infrastructure on Google Cloud. And there’s petabytes of data on-premises, and that data is subject to regulatory requirements that make it difficult to move. The result? Companies have hybrid clouds long before anyone at the C-level perceives the need for a coherent cloud strategy. By the time the C suite is building a master plan, there are already mission-critical apps in marketing, sales, and product development. And the one way to fail is to dictate that “we’ve decided to unify on cloud X.”</p>



<p>All the cloud vendors, including Amazon (which until recently<a href="https://www.crn.com.au/news/aws-forbids-partners-even-mentioning-multi-cloud-529598"> didn’t even allow its partners to use the word multicloud</a>), are being drawn to a strategy based not on locking customers into a specific cloud but on facilitating management of a hybrid cloud, and all offer tools to support hybrid cloud development. They know that support for hybrid clouds is key to cloud adoption–and, if there is any lock in, it will be around management. As IBM’s Rob Thomas has frequently said, “<a href="https://learning.oreilly.com/library/view/the-ai-ladder/9781492073420/ch04.html">Cloud is a capability, not a location</a>.”</p>



<p>As expected, we see a lot of interest in microservices, with a 10% year-over-year increase—not large, but still healthy. Serverless (a.k.a. functions as a service) also shows a 10% increase, but with lower usage. That’s important: while it “feels like” serverless adoption has stalled, our data suggests that it’s growing in parallel with microservices.</p>



<h3><strong>Security and Privacy</strong></h3>



<p>Security has always been a problematic discipline: defenders have to get thousands of things right, while an attacker only has to discover one mistake. And that mistake might have been made by a careless user rather than someone on the IT staff. On top of that, companies have often underinvested in security: when the best sign of success is that “nothing bad happened,” it’s very difficult to say whether money was well spent. Was the team successful or just lucky?</p>



<p>Yet the last decade has been full of high-profile break-ins that have cost billions of dollars (including increasingly hefty penalties) and led to the <a href="https://www.csoonline.com/article/3510640/7-security-incidents-that-cost-cisos-their-jobs.html">resignations and firings of C-suite executives</a>. Have companies learned their lessons?</p>



<p>The data doesn’t tell a clear story. While we’ve avoided discussing absolute usage, usage of content about security is very high—higher than for any other topic except for the major programming languages like Java and Python. Perhaps a better comparison would be to compare security with a general topic like programming or cloud. If we take that approach, programming usage is heavier than security, and security is only slightly behind cloud. So the usage of content about security is high, indeed, with year-over-year growth of 35%.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-8-1048x764.png" alt="" class="wp-image-13627" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-8-1048x764.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-8-300x219.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-8-768x560.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/01/76572_ORM_Platform_Analysis_Report_Data_Viz_Figure-8.png 1368w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><br><em>Figure 8. Security and privacy</em></figcaption></figure>



<p>But what content are people using? Certification resources, certainly: CISSP content and training is 66% of general security content, with a slight (2%) decrease since 2019. Usage of content about the CompTIA Security+ certification is about 33% of general security, with a strong 58% increase.</p>



<p>There’s a fair amount of interest in hacking, which shows 16% growth. Interestingly, ethical hacking (a subset of hacking) shows about half as much usage as hacking, with 33% growth. So we’re evenly split between good and bad actors, but the good guys are increasing more rapidly. Penetration testing, which should be considered a kind of ethical hacking, shows a 14% decrease; this shift may only reflect which term is more popular.</p>



<p>Beyond those categories, we get into the long tail: there’s only minimal usage of content about specific topics like phishing and ransomware, though ransomware shows a huge year-over-year increase (155%); that increase no doubt reflects the frequency and severity of ransomware attacks in the past year. There’s also a 130% increase in content about “zero trust,” a technology used to build defensible networks—though again, usage is small.</p>



<p>It’s disappointing that we see so little interest in content about privacy, including content about specific regulatory requirements such as GDPR. We don’t see heavy usage; we don’t see growth; we don’t even see significant numbers of search queries. This doesn’t bode well.</p>



<h3><strong>Not the End of the Story</strong></h3>



<p>We’ve taken a tour through a significant portion of the technology landscape. We’ve reported on the horse races along with the deeper stories underlying those races. Trends aren’t just the latest fashions; they’re also long-term processes. Containerization goes back to <a href="https://www.section.io/engineering-education/history-of-container-technology/">Unix version 7 in 1979</a>; and didn’t Sun Microsystems invent the cloud in the 1990s with its workstations and <a href="https://en.wikipedia.org/wiki/Sun_Ray">Sun Ray</a> terminals? We may talk about “internet time,” but the most important trends span decades, not months or years—and often involve reinventing technology that was useful but forgotten, or technology that surfaced before its time.</p>



<p>With that in mind, let’s take several steps back and think about the big picture. How are we going to harness the computing power needed for AI applications? We’ve talked about concurrency for decades, but it was only an exotic capability important for huge number-crunching tasks. That’s no longer true; we’ve run out of Moore’s law, and concurrency is table stakes. We’ve talked about system administration for decades, and during that time, the ratio of IT staff to computers managed has gone from many-to-one (one mainframe, many operators) to one-to-thousands (monitoring infrastructure in the cloud). As part of that evolution, automation has also gone from an option to a necessity. </p>



<p>We’ve all heard that “everyone should learn to program.” This may be correct&#8230;or maybe not. It doesn’t mean that everyone should be a professional programmer but that everyone should be able to use computers effectively, and that requires programming. Will that be true in the future? No-code and low-code products are reaching the market, allowing users to build everything from business applications to AI prototypes. Again, this trend goes way back: in the late 1950s, the first modern programming languages made programming much easier. And yes, even back then there were those who said “real men use machine language.” (And that sexism was no doubt intentional, since the first generation of programmers included many women.) Will our future bring further democratization? Or a return to a cult of “wizards”? Low-code AI and complex JavaScript web platforms offer conflicting visions of what the future may bring.</p>



<p>Finally, the most important trend may not yet appear in our data at all. Technology has largely gotten a free ride as far as regulation and legislation are concerned. Yes, there are heavily regulated sectors like healthcare and finance, but social media, much of machine learning, and even much of online commerce have only been lightly regulated. That free ride is coming to an end. Between <a href="https://en.wikipedia.org/wiki/General_Data_Protection_Regulation">GDPR</a>, the <a href="https://oag.ca.gov/privacy/ccpa">California Consumer Privacy Act</a> (which will probably be copied by many states), California Propositions <a href="https://voterguide.sos.ca.gov/propositions/22/">22</a> and <a href="https://voterguide.sos.ca.gov/propositions/24/">24</a>, many <a href="https://www.portland.gov/smart-city-pdx/news/2020/9/9/city-council-approves-ordinances-banning-use-face-recognition">city ordinances</a> regarding the use of face recognition, and rethinking the meaning of <a href="https://www.lawfareblog.com/whats-next-section-230-roundup-proposals">Section 230</a> of the Communications Decency Act, laws and regulations will play a big role in shaping technology in the coming years. Some of that regulation was inevitable, but a lot of it is a direct response to an industry that moved too fast and broke too many things. In this light, the lack of interest in privacy and related topics is unhealthy. Twenty years ago, we built a future that we don’t really want to live in. The question facing us now is simple:<br><br>What future will we build? </p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/where-programming-ops-ai-and-the-cloud-are-headed-in-2021/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>Microservices Adoption in 2020</title>
		<link>https://www.oreilly.com/radar/microservices-adoption-in-2020/</link>
				<comments>https://www.oreilly.com/radar/microservices-adoption-in-2020/#respond</comments>
				<pubDate>Wed, 15 Jul 2020 13:33:44 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides and Steve Swoyer]]></dc:creator>
				<category><![CDATA[Next Architecture]]></category>
		<category><![CDATA[Research]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=13077</guid>
				<description><![CDATA[Microservices&#160;seem to be everywhere. Scratch that:&#160;talk&#160;about microservices seems to be everywhere. And that’s the problem. Thinkers as dissimilar as&#160;Plato,&#160;Robert Boyle, and&#160;Keith Richards&#160;tend to agree about one thing: Talk is cheap. So we wanted to determine to what extent, and how, O’Reilly subscribers are empirically&#160;using&#160;microservices. In other words, how long have people been using them? What [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p><a href="http://shop.oreilly.com/product/0636920033158.do">Microservices</a>&nbsp;seem to be everywhere. Scratch that:&nbsp;<em>talk</em>&nbsp;about microservices seems to be everywhere.</p>



<p>And that’s the problem. Thinkers as dissimilar as<a href="https://ndpr.nd.edu/news/plato-on-the-rhetoric-of-philosophers-and-sophists/">&nbsp;Plato</a>,<a href="https://web.stanford.edu/class/history34q/readings/ShapinSchaffer/ShapinSchaffer_Seeing.html">&nbsp;Robert Boyle</a>, and<a href="https://en.wikipedia.org/wiki/Talk_Is_Cheap">&nbsp;Keith Richards</a>&nbsp;tend to agree about one thing: Talk is cheap. So we wanted to determine to what extent, and how, O’Reilly subscribers are empirically&nbsp;<em>using</em>&nbsp;microservices. In other words, how long have people been using them? What are they using them for? Are they having success? If so, what kinds of benefits are they seeing? What can we learn from their failures?</p>



<p>So we did what we usually do: we ran a survey. The survey ran from January 31, 2020 through February 29; we had 1502 respondents from the readers of our mailing lists. Here’s a summary of our key findings:</p>



<p><em>Most adopters are successful with microservices.</em></p>



<blockquote class="wp-block-quote"><p>A minority (under 10%) reports “complete success,” but a clear majority (54%) describes their use as at least “mostly successful” and 92% of the respondents had at least some success.</p></blockquote>



<p><em>Microservices practices are surprisingly mature</em></p>



<blockquote class="wp-block-quote"><p> About 28% of respondents say their organizations have been using microservices for at least three years; more than three-fifths (61%) of the respondents have been using microservices for a year or more.</p></blockquote>



<p><em>Adopters are betting big on microservices</em></p>



<blockquote class="wp-block-quote"><p> Almost one-third (29%) of respondents say their employers are migrating or implementing a majority of their systems (over 50%) using microservices.</p></blockquote>



<p><em>Success with microservices means owning the software lifecycle</em></p>



<blockquote class="wp-block-quote"><p>Most (74%) respondents say their teams own the build-test-deploy-maintain phases of the software lifecycle. Teams that own the lifecycle succeed at a rate 18% higher than those that don’t.</p></blockquote>



<p><em>Success with containers</em></p>



<blockquote class="wp-block-quote"><p>Respondents who used containers to deploy microservices were significantly more likely to report success than those who didn’t. For our audience, use of containers is one of the strongest predictors of success with microservices.</p></blockquote>



<p><em>It’s the culture</em></p>



<blockquote class="wp-block-quote"><p>A plurality of adopters cite cultural or mindset barriers to adoption. Decomposing monolithic applications into microservices is also a major challenge, but complexity may be the biggest challenge of all.</p></blockquote>



<h2><strong>Respondent Demographics</strong></h2>



<p><em>Technical roles dominate, but management roles are represented, too.&nbsp;</em>Software engineers comprise the survey audience’s single largest cluster, over one quarter (27%) of respondents (Figure 1). If you combine the different architectural roles—i.e., software and systems architects, technical leads—architects represent almost 28% of the sample. Adding architects and engineers, we see that roughly 55% of the respondents are directly involved in software development. Respondents in management roles constitute close to 23% of the audience; executive-level positions (vice presidents and CxOs) account for roughly 13%, overall.</p>



<p>That said, the audience for this survey—like those of almost all Radar surveys—is disproportionately technical. Technical roles represented in the “Other” category include IT managers, data engineers, DevOps practitioners, data scientists, systems engineers, and systems administrators. Each of these roles comprised less than 3% of all respondents.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_01-1-1048x699.png" alt="" class="wp-image-13104" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_01-1-1048x699.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_01-1-300x200.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_01-1-768x512.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_01-1.png 1088w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Figure 1: Respondent roles</em></figcaption></figure>



<p><em>Respondents work in more than 25 different vertical industries</em>.The single largest cluster consists of respondents (26%) from the software industry. Combined, technology verticals—software, computers/hardware, and telecommunications—account for about 35% of the audience (Figure 2).</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_02-1-1048x825.png" alt="" class="wp-image-13103" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_02-1-1048x825.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_02-1-300x236.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_02-1-768x605.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_02-1.png 1054w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Figure 2: Respondent industries</em></figcaption></figure>



<p>The second-largest cluster (exclusive of “Other”) was finance, at close to 12%, followed by consulting/professional services, at roughly 10%. The “Other” category, at more than 22% of the total, includes respondents from education (3%, combining K12 and higher education), insurance (3%), energy and utilities (2%), media/entertainment (2%), non-profit (2%), consumer products (1%) and other verticals.</p>



<p><em>Microservices aren’t just for the big guys</em>. Large companies are amply represented, and respondents affiliated with organizations of 1,000 or more employees comprise 42% of the audience. Overall, however, small- and medium-sized organizations predominate. The largest single cluster (33% of all respondents) was organizations with under 100 employees.</p>



<p><em>A good global mix</em>. We’re used to seeing a North American-centric tilt in our Radar surveys. This is true, too, of the audience for this survey, nearly half of which (over 49%) works in North America. Europe—inclusive of the United Kingdom—was the next largest region, comprising almost 26% of all respondents. Asia-Pacific accounts for roughly 15% of respondents. In total, just over half of the audience hails from outside of North America. This roughly mirrors usage on the O’Reilly learning platform.</p>



<h2><strong>Microservices Adoption: Explication and Analysis</strong></h2>



<p>Slightly more than one-third of respondents say their organizations have been using microservices for between 1 and 3 years (Figure 3). This is the single largest cluster in our sample. The second-largest cluster, at nearly one-quarter (over 23%) of respondent organizations, does not use microservices at all. Mature adopters, companies that first adopted microservices between 3 and 5 years ago, comprise the third-largest cluster, at 18%. If you factor in the respondents whose organizations first adopted microservices over 5 years ago, this means that more than 28% of respondent organizations have been using microservices for at least three years. A little under one-sixth of companies (15%) are just beginning microservice adoption, however; they’ve been using microservices for less than 1 year.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_03-1.png" alt="" class="wp-image-13105" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_03-1.png 994w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_03-1-300x169.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_03-1-768x433.png 768w" sizes="(max-width: 994px) 100vw, 994px" /><figcaption><em>Figure 3: Duration of use of microservices</em></figcaption></figure>



<p>The pace of adoption, in which a clear majority (61%) of respondents say their organizations have used microservices for between 1 and 5 years, roughly tracks with empirical trends we’ve observed internally (e.g., with respect to keyword search and topic usage activity on the O’Reilly online learning platform) and&nbsp;<a href="https://trends.google.com/trends/explore?date=today%205-y&amp;geo=US&amp;q=%2Fm%2F011spz0k">in other contexts</a>. The share of adoption for the 12-month period between January of 2019 and January of 2020 likewise suggests that microservices remains a topic of interest to O’Reilly users. On the other hand, there’s evidence from other quarters that interest in microservices—<a href="https://trends.google.com/trends/explore?date=2019-01-31%202020-01-31&amp;geo=US&amp;q=%2Fm%2F011spz0k">both as a topic</a>&nbsp;and as a general&nbsp;<a href="https://trends.google.com/trends/explore?date=2019-01-31%202020-01-31&amp;geo=US&amp;q=Microservices">search term</a>—has flattened (and perhaps even cooled) over the 12 months.</p>



<p>So not only are most respondents using microservices, but three-fifths (61%) have been using them for a year or more. But do they describe their experience as successful? What share of the systems they’re deploying or maintaining are built to microservices architecture? What characteristics do successful adopters share? What can we learn from those who say they’ve failed with microservices?</p>



<h2><strong>Critical Factors for Success</strong></h2>



<p>A majority of respondents (55%) say their organization’s use of microservices has been either a “complete success” (close to 9%) or “mostly successful” (Figure 4).</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_04-1.png" alt="" class="wp-image-13106" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_04-1.png 973w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_04-1-300x189.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_04-1-768x483.png 768w" sizes="(max-width: 973px) 100vw, 973px" /><figcaption><em>Figure 4: Success with microservices</em></figcaption></figure>



<p>More than one-third (37%) say they’ve had “some success” with microservices. Approximately 8% say they haven’t been successful at all. However, relationships in the data hint at several critical factors for success. For example, in organizations in which development teams own the entire development cycle (i.e., building, testing, deployment, and maintenance), close to half (almost 49%) also reported being “mostly successful” with microservices—and more than 10% said their microservices development efforts were a “complete success. (Figure 5). The combined tally (close to 59%) is about 18% higher than the baseline tally for the survey audience as a whole.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_05-1.png" alt="" class="wp-image-13107" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_05-1.png 837w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_05-1-300x163.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_05-1-768x417.png 768w" sizes="(max-width: 837px) 100vw, 837px" /><figcaption><em>Figure 5: Do dev teams own/not own the software lifecycle?</em></figcaption></figure>



<p>Do we see meaningful connections between success with microservices and the use, or disuse, of specific technologies? Perhaps; we’ll take a look at that next, specifically with respect to containers, centrally managed databases, and monolithic UIs. The semantics get a bit twisted, but the connections we identify seem more strongly associated with “disuse” than with “use.”</p>



<p>We emphasize the qualifier “seem” because—with respect to the problem of identifying factors for success—we lack detailed data to draw rigorous conclusions. We kept our survey simple; our priority was to develop a general sense of how, why, and in which scenarios people are using microservices. For the most part, our questions didn’t drill down into specifics. We emphasize “seem” for another reason, too. When we asked respondents to cite the biggest challenges to microservices adoption, two problems—<a href="https://en.wikipedia.org/wiki/Decomposition_(computer_science)">decomposition</a>&nbsp;and complexity—stood out for us. Decomposition because it came in at #2, trailing only corporate culture / mindset, which (in almost all Radar surveys) is #1; complexity because two ostensibly different kinds of complexity—“increased complexity” (#4) and the “complexity of managing many services” (#5)—cracked the top 5. Add them together and respondents view complexity in one form or another as the biggest challenge.</p>



<p>The upshot is that complexity in general and (more specifically) the complexity associated with decomposition are the shoals in which most microservices projects seem to run aground. That’s hard to argue with; complexity is rarely (if ever) associated with success. But it’s easy to pretend that complexity is the enemy when it’s really only one variable in a set of tradeoffs.</p>



<p>What does that mean? Although it’s not a question we asked, anecdotally we hear that most microservices projects are replacing large, legacy, monolithic software systems. Those monoliths are themselves very complex, having evolved over decades. The complexity of the monolith that’s being replaced is a “sunk cost” that has only partially been paid; it continues to extract a toll as that software is extended to support new features, changing business models, changing modes of user interaction, and more. Microservices may require paying a complexity cost again, but that’s where the tradeoff comes in: in return for the complexity of re-engineering the system, you get increased flexibility, including a simpler path to adding new features, simpler management, simplified scaling as the organization grows.</p>



<h3>Use of Containers for Microservice Deployment</h3>



<p>We asked respondents what proportion of their microservices they deploy using containers.</p>



<p>The largest single cluster (38%) for this question consists of respondents that do not deploy their microservices in containers. One might reasonably expect that containers would be the most common means of instantiating microservices. This just isn’t the case, however. Our findings indicate that, for our survey audience as a whole, most respondents (58%) deploy microservices using some medium other than containers (Figure 6).</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_06-1.png" alt="" class="wp-image-13108" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_06-1.png 903w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_06-1-300x176.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_06-1-768x451.png 768w" sizes="(max-width: 903px) 100vw, 903px" /><figcaption><em>Figure 6: Proportion of microservices deployed using containers</em></figcaption></figure>



<p>There are valid reasons not to use containers. For some adopters, technical debt (in the form of custom-built, proprietary and monolithic systems, applications, or architectures) is a constraining factor. So it just makes sense to instantiate microservices at the level of the&nbsp;<a href="https://en.wikipedia.org/wiki/Hardware_virtualization">virtual machine</a>&nbsp;(VM), as distinct to that of the container. Or maybe it’s faster and less costly, at least in the short term, to build and instantiate microservices as non-virtualized code running in the context of a conventional operating system, a database, application server, etc..</p>



<p>This doesn’t mean respondents aren’t using containers; most of the survey audience is. It’s just that containers are not yet the most popular means of instantiating microservices, although that could be changing. For example, the second-largest cluster (31%) for this question consists of organizations that deploy between 75% and 100% of their microservices using containers. And 11% use containers to deploy between 50 and 75% of their microservices.</p>



<p>The upshot is that more than two-fifths (42%) of respondent organizations use containers to deploy at least half of their microservices—and that, for the survey audience as a whole, nearly two-thirds (62%) are using containers to deploy at least&nbsp;<em>some</em>&nbsp;of their microservices. So we have a split: on the one hand, most microservices are instantiated using a technique other than containers; on the other hand, most organizations that use microservices also instantiate at least some of them in containersFor example, 10% of respondents say they use containers to deploy between 10-25% of their microservices; a little more than 9% deploy between 25-50% of microservices with containers; and, again, 11% deploy between 50-75% using containers.<sup>1</sup> It seems adopters either go (mostly) all-in on containers, using them for most microservices, or use them sparingly.</p>



<p>There’s a critical, and intriguing, “but” here: a higher than average proportion of respondents who report success with microservices opt to instantiate them using containers; conversely, a much higher proportion of respondents who describe their microservices efforts as “Not successful at all”&nbsp;<em>do not</em>&nbsp;instantiate them in containers. For example, almost half (49%) of respondents who describe their deployments as “a complete success” also instantiate most of their microservices (75-100%) in containers. This is more than 5x the baseline (9%) for this question. Conversely, an overwhelming majority (83%) of respondents who describe their microservices efforts as “Not successful at all” are instantiating them by some means other than containers. (These are respondents who use containers in conjunction with less than 10% of their microservices.) This is about 11x the baseline for this question (Figure 4).</p>



<p>This observation makes intuitive sense. With microservices, instead of deploying one monolithic application, you may need to deploy and manage hundreds or thousands of services. Using containers to standardize deployment, and container orchestration tools to automate ongoing management, greatly simplifies the burden of deployment and management. It’s worth remembering that “containers” get their name by analogy to shipping containers: instead of loading a ship with 10,000 cases of canned soup, 20,000 board-feet of lumber, and a few thousand automobile tires, the shippers would pack their goods in standardized containers that could be stacked up on the ship. This represents a significant reduction in the cost of shipping–or the operational cost of deploying and maintaining software. Containers are a simplifying technology.</p>



<h3>Use of a Central, Managed Database</h3>



<p>When we asked respondents what proportion of their microservices share a central, managed database (Figure 7), we found that not using a centrally managed database with microservices tends to be associated with failure. That said, the question itself (i.e., “What percentage of your microservices share a centrally-managed database?”) doesn’t give us much to go on. We don’t know what respondents considered a centrally managed database; what kind of database they were using (relational or non-relational); or whether transactional integrity was an issue.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_07-1.png" alt="" class="wp-image-13109" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_07-1.png 901w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_07-1-300x180.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_07-1-768x462.png 768w" sizes="(max-width: 901px) 100vw, 901px" /><figcaption><em>Figure 7: Proportion of microservices that use a centrally managed database</em></figcaption></figure>



<p>Granted that there’s a lot we’d like to know that we don’t, let’s think about why this question is important, and dig a bit deeper. The point of microservices is breaking an application into separate, decoupled services. Using the database as the integration point between services is counter to this purpose. The result is a software system that looks like it’s built from microservices, but really isn’t, and which realizes few of the promised advantages because the individual services are still coupled to each other at the database layer.</p>



<p>Are our respondents succeeding with decoupled databases? For our survey audience, it looks like the smaller the proportion of microservices that share access to a central, managed database, the greater the likelihood of failure. Think of this as what&nbsp;<a href="https://en.wikipedia.org/wiki/Survivorship_bias">survivorship bias</a>&nbsp;is most apt to miss: i.e., failure. The lesson in such cases is usually that failure&nbsp;<a href="https://medium.com/@penguinpress/an-excerpt-from-how-not-to-be-wrong-by-jordan-ellenberg-664e708cfc3d">tells a valuable story of its own</a>.</p>



<p>Of the respondents who said that they are “not successful at all,” 71% said that they did not make much use of a centrally managed database (under 10% of their microservices). The problem is that we don’t actually know what using (or not using) a centrally managed database entails because the question itself is imprecise. For example, a recommended practice is to implement a separate namespace for each microservice. This can be accomplished in several ways—including by using a centralized database!<sup>2</sup> But implementing a separate namespace for each service raises a number of hard problems, starting with transactional integrity and data consistency, and (notionally) encompassing security and privacy, too. Working through these issues isn’t easy, even with a centralized database. It’s much harder without one, however.</p>



<p>However, while it’s easy to look at these failures and suppose that using a separate database (or database schema) per service is a bad idea that leads to failure, that’s far from the whole picture. Think back to where we started this section: microservices represents a tradeoff of complexity against flexibility. (And also a tradeoff of complexity now versus complexity accreted over the years in a legacy system–we could perhaps think of paying off technical debt with a big “balloon payment.”) Replacing a legacy monolith with microservices isn’t easy. But working through the current complexity to build a system that’s more likely to serve your future needs is what good engineering is all about. And when you’re building a complex system, you need to know where the pain points are. That’s what this data is really telling us: redesigning databases to eliminate dependencies between services is a significant pain point.</p>



<h3><strong>Monolithic UI</strong></h3>



<p>We also asked users what percentage of the systems deployed as microservices had a monolithic user interface (Figure 8). This question is also problematic. First, what is a monolithic UI? More to the point, what would our respondents assume we mean by a monolithic UI, and what are they doing if they’re not building a monolithic UI? There are several possibilities, none of which is entirely convincing. One recent trend in web development is&nbsp;<a href="https://martinfowler.com/articles/micro-frontends.html">“micro frontends”</a>, but this doesn’t appear to be&nbsp;<a href="https://trends.google.com/trends/explore?geo=US&amp;q=micro%20frontend,microservice">well-known enough</a>&nbsp;to have a significant effect on our data. Respondents are likely to associate a “monolithic UI” with traditional web development (or perhaps even desktop applications or ancient “green screens”), as opposed to the use of components within a framework like React. (Since micro frontends are built with frameworks like React, these may be two names for almost the same thing. Once you’re using a component-based framework, associating components with individual back-end services isn’t a big leap.) In any case, it is likely that a monolithic UI represents a tradeoff for simplicity over flexibility and complexity. A front end built with a modern reactive web framework is almost certainly more flexible–and more complex–than a 1990s-era web form.</p>



<p>Use of a monolithic UI is another possible example of survivorship bias; it’s also another case in which respondents who are successful with microservices are much less interesting than those who aren’t.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_08-1.png" alt="" class="wp-image-13110" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_08-1.png 904w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_08-1-300x177.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_08-1-768x454.png 768w" sizes="(max-width: 904px) 100vw, 904px" /><figcaption><em>Figure 8: Proportion of microservices that use a monolithic UI</em></figcaption></figure>



<p>Take the cluster of respondents who&nbsp;<em>don’t</em>&nbsp;build a monolithic UI. (That is, &lt;10% of their microservices use a monolithic UI.) At 31% of all respondents, this is the largest group. But it also has the highest rate of complete failure: 17% describe their implementations as “Not successful at all.” Respondents in this group were also more likely to describe their projects as a “complete success” (12%); 37% said they were “mostly successful.”</p>



<p>At the other end of this spectrum, among respondents who used a monolithic UI for 50-75% of their systems, only 2% said they were not successful at all. 8% said their projects were a “complete success,” while 52% said their projects were “mostly successful.”</p>



<p>What are we to make of this? It’s hard to say, given that the notion of a monolithic UI (and its opposite) are poorly defined. But this looks similar to what we observed for databases. Reactive web frameworks (like React and Angular), which are used to build many modern web interfaces, are very complex systems in their own right. It’s not surprising that systems that don’t use a monolithic UI have a high failure rate, and that systems that implement a monolithic UI have a fairly high success rate; if simplicity for developers were the only criteria, we’d all go back to green screens.</p>



<p>Rather than just looking at a “complexity tax,” we have to consider what the goal is: building systems that can easily be extended and scaled as conditions change. Is a UI that isn’t monolithic (regardless of the technology) more complex? Probably so. Ideally, moving a monolithic application’s back end to microservices while maintaining the legacy front-end would be minimal work (though “minimal” is always more minimal in theory than in practice). What’s gained in return? Additional flexibility; in this case, we’d be looking for the ability to support different kinds of user interfaces and different kind of interactions. Adding a mobile or voice UI to a legacy front end can easily become a nightmare; but it’s hard to imagine any modern application that doesn’t have a mobile front-end, and in a few years, it will be hard to imagine an application without a voice-driven front end. Re-envisioning the front end in terms of services (however they may be implemented) is building flexibility into the system. What other user interfaces will we need in the next few years? Again, the issue is never simply “complexity”; it’s what you get in return.</p>



<h2><strong>Benefits and Challenges</strong></h2>



<p>Respondents were asked which benefits, if any, they attribute to their successful use of microservices (Figure 9). They were asked to select all applicable benefits.</p>



<p>The largest cluster for this response (at 45%) named “feature flexibility,” followed (at just under 45%) by “responding quickly to changing technology and business requirements.” Just under 44% cited the benefit of “better overall scalability,” followed (43%) by “more frequent code refreshes.” The least-cited benefit (15%) was that of lower development costs; just 20% of respondents say they realized one of the core promises of microservices: improved availability by virtue of multiple, redundant functions (i.e., services).</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_09-1-1048x730.png" alt="" class="wp-image-13111" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_09-1-1048x730.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_09-1-300x209.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_09-1-768x535.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_09-1.png 1223w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Figure 9: Benefits of using microservices</em></figcaption></figure>



<p>Almost all of our Radar surveys have found that respondents cite corporate culture as one of the top two or three impediments to adoption or success. This survey was no exception. Culture was the most oft-cited challenge: almost 40% of respondents cited culture—or, alternatively, the problem of “overcoming existing mindset”—as among the biggest challenges they faced in adopting microservices (Figure 10). This was the largest cluster for this response, followed (37%) by the challenge of decomposing requirements into primitive/granular functions. The challenge of integrating with legacy systems was third, cited by approximately 30% of respondents.</p>



<p>If we combine the two responses that have to do with complexity (“Increased complexity” and “Complexity of managing many services”), we find that complexity in one form or another is the biggest overall challenge, cited by 56% of respondents.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_10-1-768x1048.png" alt="" class="wp-image-13112" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_10-1-768x1048.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_10-1-220x300.png 220w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_10-1-1126x1536.png 1126w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/07/ma20_10-1.png 1253w" sizes="(max-width: 768px) 100vw, 768px" /><figcaption><em>Figure 10: Challenges encountered in adoption of microservices</em></figcaption></figure>



<p>Other results of note: at No. 6, technical debt (cited by 28% of respondents) just missed the top five. It’s the kind of hard problem that cannot be wished, willed, or architected away–indeed, replacing a monolithic system with microservices can be seen as a “lump sum” payment of years of accrued technical debt. The rest of the top 10 reads like a laundry list of hard problems: API quality (28%), talent and skill shortages (28%), training and retraining challenges (also 28%) and monitoring/observability (27%) round out the top 10. The high showing for API quality—i.e., their richness, robustness, transparency, accessibility, stability (over time) and usefulness—shouldn’t be surprising. The relatively low result for security (No. 11, cited by 21% of respondents) is surprising.</p>



<h2><strong>Takeaways</strong></h2>



<p>For the most part, adopters say they’re having success with microservices: more than half (55%) describe their efforts as at least “mostly” successful. Respondents commonly attribute several clear benefits to their use of microservices, including feature flexibility; the ability to respond to changing business requirements; improved scalability; and more frequent code releases. And, at least among adopters, microservices comprise a growing share of production systems: almost half (46%) of respondents say their organizations are currently developing for or migrating one-quarter or more of their production systems to microservices. Nearly one-sixth (15%) say they’re developing or migrating between 75-100% of their systems to a microservices-oriented architecture. For our audience, microservices architecture is not only established but (for a majority of adopters) used to support systems or workloads in production.</p>



<p>There’s evidence here for the microservices skeptic, too. About 8% of would-be adopters describe their experiences with microservices as “not successful at all.” Proponents will claim that 8% is a shockingly low rate of failure for any software development project, particularly given the&nbsp;<a href="https://www.zdnet.com/article/study-68-percent-of-it-projects-fail/">historical failure rates of large IT projects</a>; but skeptics will counter that 8% is still, well, 8%. And even if a majority of respondents say they’ve been “mostly successful” with microservices, a sizable percentage (37%) say they’ve had only “some” success. The upshot is that a little under half (45%) of all users have had bad, middling, or only modestly successful experiences with microservices. A skeptic might also point to selection bias: not only is our audience dominated by people in technical roles, but software developers and architects are overrepresented. A survey like this is bound to attract a disproportionate share of participants with an interest in seeing microservices succeed—or fail. In the same way, the person who volunteers her time to complete our survey is more likely than not to be working with or to have an interest in (for or against) microservices. These are all valid objections.</p>



<p>Our results emphasize the importance of&nbsp;<em>pragmatic</em>&nbsp;microservices development. Microservices can be complex–there’s no point in denying that. And complexity frequently leads to failure–there’s no point in denying that, either. Using containers for deployment is a way of minimizing complexity. We’ve seen that decoupling databases is frequently a pain point, along with redesigning the user interface as a set of components. But again, it’s important to recognize that microservices rarely arise out of nowhere. Many microservice projects are replacing existing systems. And that existing system has its own complexity: complexity that grew over the years, complexity that’s preventing you from achieving your current goals. If you&#8217;ve become accustomed to your legacy systems, you probably don&#8217;t realize how complex they are–but if you’ve made the decision to migrate from a monolith to microservices, the complexity of maintaining your monolith is almost certainly the reason.</p>



<p>Replacing one kind of complexity with another–is that a gain? Absolutely, if it’s done correctly and enables you to achieve your goals. Complexity is an engineering problem, and engineering problems are always about tradeoffs. When you’re building microservices, keep your eye on the goal–whether that’s supporting new features, scaling for more customers, providing a new experience for users, or something else. Don&#8217;t be surprised that microservices bring their own complexity, and don&#8217;t let that daunt you. But don&#8217;t underestimate the challenge, either.</p>



<p><em>Special thanks to Kristen Haring, Phil Harvey, Mark Madsen, Roger Magoulas, Larry “Catfish” Murdock, Sam Newman, and Mac Slocum for—among other invaluable contributions—questions, comments, context, and, above all, constructive criticism.</em></p>



<p></p>



<h3>Footnotes</h3>



<p><sup>1</sup> For example, 10% of respondents say they use containers to deploy between 10-25% of their microservices; a little more than 9% deploy between 25-50% of microservices with containers; and, again, 11% deploy between 50-75% using containers.</p>



<p><sup>2</sup> A development team could use a centrally managed database to create separate namespaces for each service. This practice is notionally consistent with the idea that each microservice should own its data. It also saves developers the headache of coding logic to manage data consistency, transactional durability, and other issues. Or, alternatively, software developers could opt to manage data persistence (along with data consistency and transactional durability) on a per-microservices basis, a practice that is strictly consistent with the idea of data sovereignty, but which radically ups the complexity factor.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/microservices-adoption-in-2020/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>Cloud Adoption in 2020</title>
		<link>https://www.oreilly.com/radar/cloud-adoption-in-2020/</link>
				<comments>https://www.oreilly.com/radar/cloud-adoption-in-2020/#respond</comments>
				<pubDate>Tue, 19 May 2020 18:40:09 +0000</pubDate>
		<dc:creator><![CDATA[Roger Magoulas and Steve Swoyer]]></dc:creator>
				<category><![CDATA[Next Architecture]]></category>
		<category><![CDATA[Research]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=12817</guid>
				<description><![CDATA[We wanted to discover what our readers were doing with cloud, microservices, and other critical infrastructure and operations technologies. So we constructed a survey and ran it earlier this year: from January 9th through January 31st, 2020. All told, we received 1,283 responses. A lot happened between January and the first week of March, when [&#8230;]]]></description>
								<content:encoded><![CDATA[
<div class="wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-block-group"><div class="wp-block-group__inner-container">
<p class="has-text-color has-background has-small-font-size has-very-dark-gray-color has-very-light-gray-background-color">To continue learning and to get ahead with your career, check out O&#8217;Reilly Learning with a free trial.  Live online training, videos, books, certification prep, and more, from O&#8217;Reilly and our partner publishers.</p>



<div class="wp-block-button"><a class="wp-block-button__link has-text-color has-vivid-red-color has-background has-very-light-gray-background-color" href="https://www.oreilly.com/">O&#8217;Reilly Learning &gt;</a></div>
<br />
</div></div>
</div></div>



<br />
<br />



<p>We wanted to discover what our readers were doing with cloud, microservices, and other critical infrastructure and operations technologies. So we constructed a survey and ran it earlier this year: from January 9th through January 31st, 2020. All told, we received 1,283 responses.</p>



<p>A lot happened between January and the first week of March, when we got around to analyzing our survey data. It seemed clear to us that the world we’d captured in our survey was going to change (if it hadn’t already)—that some trends would accelerate, that some would decelerate, and that things would never be quite the same. It seems to us that the results of our survey offer a point-in-time snapshot of the latest trends in cloud, microservices, distributed application development, and other emergent areas. Not only do they capture where organizations are, but, more important, they illuminate how they will evolve. We will spend months or even years trying to determine the extent to which we must recalibrate our best-laid plans and assumptions. And as we do so, we will look to surveys like this one as lodestars.</p>



<p>Without further ado, here are the key results:</p>



<p>• At first glance, cloud usage seems overwhelming. More than 88% percent of respondents use cloud in one form or another. Most respondent organizations also expect to grow their usage over the next 12 months.<br><br>• A surprising number of respondents—about 25%—said that their companies plan to move <em>all</em> of their applications to a cloud context in the next year. This includes 17% of respondents from large organizations (over 10,000 employees) that have already moved 100% of their applications to the cloud.<br><br>• Public cloud dominates, but most organizations use a mix of cloud options; almost half (49%) continue to run applications in traditional, on-premises contexts.<br><br>• More than half of respondents use multiple cloud services.<br><br>• AWS is far and away the cloud leader, followed by Azure (at more than half of share) and Google Cloud. But most Azure and GCP users also use AWS; the reverse isn’t necessarily true.<br><br>• More than half of respondent organizations use microservices.<br><br>• More than <em>one-third</em> have adopted site reliability engineering<br> (SRE); slightly less have developed production AI services. For<br> this audience, SRE’s future is brighter than AI’s, however.</p>



<h2>Respondent Demographics</h2>



<p>Software engineers represent the largest cohort, comprising almost 20% of all respondents (see <a data-type="xref" href="#fig_1">Figure 1</a>). Technical leads and architects (about 11%) are next, followed by software and systems architects (9+%). For the sample as a whole, most respondents (approximately 60%) occupy technical positions. However, a notable minority—some 15%—occupy C-level or executive roles. And about 10% work in technical management positions. That results in roughly 25% share for managers and executives. The “Other” category had a good mix of technical positions (e.g., network engineer, at &gt;2%) and management positions (IT manager, at close to 3%; operations manager at &gt;1%).</p>



<figure class="wp-block-image size-large" id="fig_1"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0101-1048x504.png" alt="" class="wp-image-12818" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0101-1048x504.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0101-300x144.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0101-768x370.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0101.png 1436w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 1. Role of survey respondents</figcaption></figure>



<p>A significant minority of respondents (22%) have worked in their roles for more than 10 years; the largest single bloc—almost 34% of all respondents—for between one to three years (<a data-type="xref" href="#fig_2">Figure 2</a>). There is atypical longevity in the survey audience: almost 55% have worked in their roles for at least four years, and a surprising number (almost 13%), for more than 12 years. It is a more experienced group than we’re used to seeing in our Radar surveys. The maturity of this audience could be a reflection of the maturity of the topic. Still, a solid third of respondents have between one to three years of experience.</p>



<figure class="wp-block-image size-large" id="fig_2"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0102-1048x634.png" alt="" class="wp-image-12819" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0102-1048x634.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0102-300x181.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0102-768x464.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0102.png 1085w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 2. Length of time in current role</figcaption></figure>



<p>Almost one-quarter (23%) of respondents work in the software<br> industry (<a data-type="xref" href="#fig_3">Figure 3</a>). Finance and banking (&gt;11%) is the second-largest vertical, followed closely by consulting and professional services (also &gt;11%).</p>



<figure class="wp-block-image size-large" id="fig_3"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0103-1048x732.png" alt="" class="wp-image-12822" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0103-1048x732.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0103-300x210.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0103-768x536.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0103.png 1423w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 3. Industry of survey respondents</figcaption></figure>



<p>We’re used to seeing these three verticals dominate representation in our Radar surveys. At 23%, however, the software vertical is significantly overrepresented, at least relative to prior surveys. Consulting and professional services, by contrast, could be slightly underrepresented. This may be a source of bias. We imagine that companies in the software industry are more likely to be early (or mid-stage) adopters of technologies like cloud computing.</p>



<p>There’s a good mix between large and small firms. About half of all respondents work in organizations with fewer than 1,000 employees. More than a quarter work with very large organizations—i.e., 10,000 or more employees. And about 28% work for small outfits of between one and 100 people.</p>



<p>About two-thirds of respondents work in North America. The next largest region, Asia, is home to about 15% of all respondents. Europe, which in most Radar surveys constitutes the second-largest respondent bloc, was third, accounting for just 11% of all participants. This survey’s disproportionate tilt toward North America is unusual. In our <a href="https://oreil.ly/F33Ir">AI adoption in the enterprise survey</a>, for example, we had close to a 50/50 split between North America and the rest of the world.<sup>1</sup> Regional representation in Radar surveys typically tracks with usage on the O’Reilly learning platform. North American users account for about half of activity on the O’Reilly platform. That isn’t the case here. Again, this is a source of bias: companies in some European countries are much more hesitant about moving workloads to the cloud.</p>



<h2>Almost Completely Cloud-y</h2>



<p>Slightly more than 88% of respondent organizations use cloud computing. Just 10% of respondents say they don’t use cloud computing <em>at all</em>, however. If this seems anomalously high, it shouldn’t.</p>



<p>A strict definition of “cloud” must also include software-as-a-service (SaaS) and platform-as-a-service (PaaS) offerings of all kinds—including email (Google G Suite email; Microsoft Exchange Online), office productivity suites (Google Docs and Sheets; Microsoft Office 365), and similar offerings. Designing a survey inevitably entails making a spate of methodological trade-offs. We <em>could</em> have specified a narrow definition of cloud—inclusive of the SaaS, PaaS, and infrastructure-as-a-service (IaaS) cloud; exclusive of cloud-based email, office productivity, etc.—but the fact remains that a proportion of enterprises either outsource their email hosting to Google, Microsoft, and other providers <em>or</em> subscribe to cloud office productivity services that (in most cases) bundle email hosting, too. These services are also designed to function as gateway drugs to cloud services: e.g., Microsoft integrates its on- and off-premises Excel client experience with its PowerBI cloud analytics service, as well as with its ecosystem of Azure-based advanced analytics and machine learning (ML) services.</p>



<p>This brings up another, related issue: how much visibility do survey respondents actually have with respect to how and where cloud gets used in their organizations? It’s likely that most respondents lack complete visibility into and across their organizations; in a large enterprise, for example, few if any people have this kind of panoptic view. When we took all of these considerations into account, a more-inclusive frame for cloud adoption made the most sense to us. It encompasses private clouds, the IaaS cloud—also host to virtual private clouds (VPC)—and the PaaS and SaaS clouds. It is less concerned with formal definitions<sup>2</sup> and captures the point-in-time totality of cloud adoption.</p>



<p>Among non-adopters, culture seems to be the biggest impediment to cloud adoption: just under 5% of non-adopters cited an “organizational preference to keep data on premises” (<a data-type="xref" href="#fig_4">Figure 4</a>). More than 2% cited regulatory concerns as a bulwark to adoption, while a still larger proportion—close to 3%—cited risk, especially with respect to migrating on-premises workloads, services, or data to cloud. Oddly, about 3% of non-adopters cited cost as a primary reason <em>not</em> to move workloads to cloud; cost-efficiency is usually touted as one of cloud’s most attractive features.</p>



<figure class="wp-block-image size-large" id="fig_4"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0104-1048x669.png" alt="" class="wp-image-12834" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0104-1048x669.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0104-300x192.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0104-768x491.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0104.png 1179w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 4. Reasons why organizations have not adopted cloud computing (represents respondents who answered no to the question “Does your organization use cloud computing?”)</figcaption></figure>



<p>Also of interest: close to 2% of non-adopters cited the prospect of<br> vendor lock-in as a rationale for not using cloud. All told, 22% of<br> respondents selected at least three issues; 46% chose at least two issues. Again, non-adopters comprise just 10% of the survey audience.</p>



<h2>Cloud Usage Waxing, not Waning</h2>



<p>Most (90%+) respondent organizations expect to increase their usage of cloud-based infrastructure. This result aligns very closely with the proportion of respondents (88%+) who have already adopted cloud. The upshot is that the overwhelming majority of adopters plan to grow, rather than reduce, their cloud usage share. Oddly, most growth seems to be happening at the extremes: almost one quarter of respondent organizations expect to move all of their applications to the cloud in the next 12 months (<a data-type="xref" href="#fig_5">Figure 5</a>). This was the second biggest cluster, overall. The largest cluster—at just under 34%—consists of respondents who expect to move one-quarter of their applications to the cloud in the next 12 months.</p>



<figure class="wp-block-image size-large" id="fig_5"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0105.png" alt="" class="wp-image-12843" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0105.png 909w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0105-300x173.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0105-768x443.png 768w" sizes="(max-width: 909px) 100vw, 909px" /><figcaption>Figure 5. Share of applications respondents expect their organizations to migrate to the cloud</figcaption></figure>



<p>About 45% of respondent organizations expect to move three-quarters or more of their applications to cloud during this same period; 67% expect to shift half or more of their applications during that same period. Zoom out to 36 months, and close to 40% of all respondents expect that all of their applications will run in a cloud context—and about 63% anticipate running at least three-quarters of their applications in the cloud.<sup>3</sup> </p>



<p>Taken at face value, these results suggest almost irresistible momentum in favor of cloud. Keep in mind, however, that usage share is based <em>on the applications respondents know of</em>, and that few if any respondents have a complete view of deployments across the whole of their organizations. With this caveat in mind, the results nonetheless suggest a wider embrace of cloud infrastructure and support <a rel="noreferrer noopener" aria-label="the idea that most organizations now equate cloud with what’s next for their infrastructure decisions (opens in a new tab)" href="https://www.oreilly.com/radar/what-is-next-architecture/" target="_blank">the idea that most organizations now equate cloud with what’s next for their infrastructure decisions</a>.</p>



<h2>Public Cloud Dominates, but Most Organizations Opt to Mix Things Up</h2>



<p>Among cloud adopters, more than 21% host all of their applications in a cloud context of one kind or another. However, organizations that host one-quarter or fewer of their applications in the cloud comprise the largest single cluster, at 39% of all respondents. As might be expected, small companies and startups are likely to host substantial proportions—in some cases, <em>all</em>—of their applications in the cloud. There were some surprises, however. For example, about 17% of companies with 10,000 or more employees host 100% of their applications in a cloud context of some kind (<a data-type="xref" href="#fig_6">Figure 6</a>). This number balloons to about 37% of companies with between one and 100 employees. Just under 50% of companies with 10,000 or more employees host 25% or fewer of their applications in a cloud context.</p>



<figure class="wp-block-image size-large" id="fig_6"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0106-1048x642.png" alt="" class="wp-image-12850" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0106-1048x642.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0106-300x184.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0106-768x471.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0106.png 1232w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 6. A comparison of respondent organization size and share of applications hosted in the cloud</figcaption></figure>



<p>Public cloud is the most popular overall deployment option, with a usage share greater than 61%. Traditional, on-premises deployment—at just under half (49%) of usage share—is second. Hybrid cloud, which combines public cloud services with on-premises private cloud infrastructure, is third, with approximately 39% usage.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0107-1048x539.png" alt="" class="wp-image-12855" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0107-1048x539.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0107-300x154.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0107-768x395.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0107.png 1172w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 7. Cloud types used by respondents’ organizations (respondents could select all types that apply)</figcaption></figure>



<p>The survey encouraged respondents to make multiple selections from among five cloud deployment options. Nearly one-tenth (9%) selected all five, and almost one-fifth (19%) selected four out of five. Almost two-thirds (64%) selected at least two cloud deployment options. The upshot is that—even though the public cloud is by far the most popular option—most respondent organizations employ a mix of cloud types. Interestingly, multi-cloud, or the use of multiple cloud computing and storage services in a single homogeneous network architecture, had the fewest users (24% of the respondents).</p>



<p>However, more than half of respondents (54%) also use multiple cloud services. The poor showing for multi-cloud might be the difference between tactical/ad hoc and strategic usage. In other words, comparatively few respondent organizations appear to be pursuing <em>dedicated</em> multi-cloud strategies.</p>



<h2>Amazon and AWS Ascendant</h2>



<p>Not surprisingly, Amazon Web Services (AWS) is far out ahead of the rest of the pack: it’s used by more than two-thirds (~67%) of all respondents. However, close to half (~48%) use Microsoft Azure, and close to one-third (~32%) use Google Cloud Platform (GCP). Respondents were encouraged to select multiple cloud service providers; in fact, a slight majority of respondents—54%—use more than a single provider. Among cloud providers, Amazon, Microsoft, and Google dominate their rivals, with Alibaba Cloud, IBM Cloud, and Oracle Cloud garnering just under 12% of share. (The poor showing for Alibaba Cloud could be a function of the larger-thannormal North American bias in the audience, as could the representations of both IBM and Oracle, which are less.)</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0108.png" alt="" class="wp-image-12856" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0108.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0108-300x150.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0108-768x384.png 768w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 8. Public cloud vendors used by respondents’ organizations (respondents could select all that apply)</figcaption></figure>



<p>Among respondents who use only public cloud providers, AWS’ share was even larger: it accounted for 75% of usage, compared with 52% for Azure and 34% for GCP. In fact, AWS is clearly the backstop vendor: not only does it have the highest share among respondent organizations, but—of the 54% who use at least two cloud vendors—almost all of them (93%) list AWS as one of those vendors.</p>



<p>If Microsoft and Google really are coming on strong, they aren’t dislodging Amazon and AWS. If anything, organizations seem to be pursuing multi-cloud strategies—even if they aren’t explicitly “doing” multi-cloud. Among our survey respondents, multi-cloud effectively means AWS + another cloud service.</p>



<h2>Microservices Achieves Critical Mass, SRE Surging</h2>



<p>More than half (52%) of respondent organizations say they use microservices concepts, tools, or methods for software development. Of these, a large minority—just over 28%—have been using microservices for more than three years. This was the second-largest cluster among users of microservices. The largest, at more than 55%, has been using microservices for between one and three years. Just 17% of users are new to microservices, with less than one year of adoption and use.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0109.png" alt="" class="wp-image-12857" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0109.png 990w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0109-300x132.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0109-768x339.png 768w" sizes="(max-width: 990px) 100vw, 990px" /><figcaption>Figure 9. Length of time respondents’ organizations have used microservices</figcaption></figure>



<p>A few caveats are in order. First, our survey didn’t ask respondents if they (or their organizations) have adopted <em>microservices architecture</em>. There’s a world of difference between experimentation and/or ad hoc usage and adoption; we saw this with agile, and—as we note below—we’re likely seeing it with SRE, too. Just because a development team uses the tools, concepts, and methods of microservices architecture doesn’t mean it has <em>adopted</em> microservices architecture. It may be that microservices patterns, as distinct to conventional software development, are well suited for the particular use case, as with video encoding, which entails multiple parallel or concurrent CPU- or GPU-intensive workloads.</p>



<p>Second, there is <a rel="noreferrer noopener" aria-label="some evidence (opens in a new tab)" href="https://trends.google.com/trends/explore?date=today%205-y" target="_blank">some evidence</a> that interest in microservices might be at or close to peaking. There is <a rel="noreferrer noopener" aria-label="also evidence (opens in a new tab)" href="https://arxiv.org/pdf/1903.11665.pdf" target="_blank">also evidence</a> that <a rel="noreferrer noopener" aria-label="decomposition (opens in a new tab)" href="https://martinfowler.com/articles/break-monolith-into-microservices.html" target="_blank">decomposition</a>—at least to the degree of granularity prescribed in microservices architecture—is proving to be more difficult than anticipated. Finally, there’s the <a rel="noreferrer noopener" aria-label="Perrow-ian (opens in a new tab)" href="https://en.wikipedia.org/wiki/Normal_Accidents" target="_blank">Perrow-ian</a> critique of microservices architecture, which argues that its complexity constitutes a kind of <em>de facto</em> tight coupling that makes it impossible to anticipate potential edge cases and eliminate risk.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0110.png" alt="" class="wp-image-12859" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0110.png 873w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0110-300x136.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0110-768x347.png 768w" sizes="(max-width: 873px) 100vw, 873px" /><figcaption>Figure 10. Presence of a Site Reliability Engineering (SRE) team within respondents’ organizations</figcaption></figure>



<p>Almost 35% of respondent organizations have implemented a Site Reliability Engineering (SRE) function. Even though SRE is less well known than microservices, DevOps, and other topics, <a rel="noreferrer noopener" aria-label="it isn’t in any sense new (opens in a new tab)" href="https://trends.google.com/trends/explore?date=2005-01-01%202020-03-22" target="_blank">it isn’t in any sense new</a>. At this point, interest in SRE actually t<a rel="noreferrer noopener" aria-label="racks closely with interest in microservices itself (opens in a new tab)" href="https://trends.google.com/trends/explore?date=today%205-y" target="_blank">racks closely with interest in microservices itself</a>.</p>



<p>Close to half of all organizations (47%) in our survey say they expect to implement an SRE function at some point in the future. Should this pan out, SRE adoption share would be roughly comparable to that of microservices. Is there significant overlap between the two, however? In other words, if an organization adopts microservices-oriented concepts, tools, and methods will it <em>also</em> tend to adopt an SRE function? Or is the growth in SRE related to other factors, such as (for example) declining interest in DevOps itself? In <a rel="noreferrer noopener" aria-label="our analysis of user activity on the O’Reilly learning platform (opens in a new tab)" href="https://www.oreilly.com/radar/oreilly-2020-platform-analysis/" target="_blank">our analysis of user activity on the O’Reilly learning platform</a>, we found that DevOps-related search and usage declined in both 2018 and 2019. We posited that adopters “might be having trouble scaling DevOps” because “developers tend to be less committed to DevOps’ operations component.”</p>



<p>We’d be remiss if we didn’t note that the strong showing for SRE is almost certainly a function of selection bias in our audience—i.e., our respondents are more likely to be using SRE than not. SRE’s performance could also be a function of the same <a rel="noreferrer noopener" aria-label="cargo cult phenomenon (opens in a new tab)" href="https://en.wikipedia.org/wiki/Cargo_cult" target="_blank">cargo cult phenomenon</a> we saw during the agile revolution, when familiarity with the term and uptake of select ideas or methods was conflated with adoption. As for the declining interest in DevOps we recorded in our platform survey, it’s just as possible that this decline—measured in terms of topic usage and search activity on the O’Reilly learning platform—is actually a function of something else: namely, the <em>maturation</em> of the DevOps topic. Clearly, the DevOps practices that took root over the last decade aren’t going anywhere. Instead, it’s likely that IT professionals are exploring and learning about DevOps-adjacent disciplines (such as SRE) that are new to them.</p>



<p>As we noted in our <a href="https://www.oreilly.com/radar/oreilly-2020-platform-analysis/" target="_blank" rel="noreferrer noopener" aria-label="platform analysis (opens in a new tab)">platform analysis</a>, in this and similar cases, it’s helpful to view the problem of user interest through the lens of the so-called <a href="https://en.wikipedia.org/wiki/Overton_window" target="_blank" rel="noreferrer noopener" aria-label="Overton Window (opens in a new tab)">Overton Window</a>, which circumscribes the human cognitive bandwidth that’s available in a certain place at a certain time. Obviously, no combination of issues or trends can exceed more than 100% of available bandwidth. The upshot is that declining interest in a topic doesn’t have to correlate with a decline in use (or usefulness) in practice. Or vice-versa. In the case of decline, a mix of emergent trends might be crowding out a topic. In the case of ascendancy, a trend might be (ephemerally) emergent.</p>



<h2>Serverless Stagnant</h2>



<p>We didn’t attempt to define serverless precisely, but for many people in our audience, serverless means “function-as-a-service” (for example, AWS Lambda). Services like AWS S3 are very much “serverless,” but that’s not common usage. With that in mind, one-third (almost 34%, in fact) of respondent organizations say they’re using serverless computing.</p>



<p>This is roughly on par with the percentage that says they’re using SRE. Unlike with SRE, where almost half (47%) of respondents expect to add an SRE function at some point in the future, fewer (approximately 37%) expect to adopt serverless.<sup>4</sup> By the same margins—i.e., 37% pro-experimentation, 63% anti—fewer respondent organizations have “experimented” with serverless computing, e.g., by evaluating vendors, scoping serverless scenarios, or testing serverless on a limited basis.</p>



<p>What’s interesting is that all three topics—viz., microservices, SRE, and serverless—seem to <a rel="noreferrer noopener" aria-label="track closely with one another (opens in a new tab)" href="https://trends.google.com/trends/explore?date=today%205-y" target="_blank">track closely with one another</a>. Is there a meaningful correlation here, or is this consonance spurious? Clearly, microservices <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://trends.google.com/trends/explore?date=2013-09-01%202020-03-22" target="_blank">are not a new thing</a>—but <a rel="noreferrer noopener" aria-label="neither is SRE (opens in a new tab)" href="https://trends.google.com/trends/explore?date=2013-09-01%202020-03-22" target="_blank">neither is SRE</a>. Is it possible that the complexity of microservice architecture, serverless computing, service mesh architecture, and other next-generation patterns is contributing to (if not driving) interest in SRE? We don’t have the data to begin to answer this question.</p>



<p>But it’s one we’d plan to keep an eye on.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0111-1048x518.png" alt="" class="wp-image-12863" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0111-1048x518.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0111-300x148.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0111-768x379.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0111.png 1138w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 11. When respondents whose organizations do not currently use serverless expect their organizations to adopt serverless</figcaption></figure>



<h2>Critical Skills for Success</h2>



<p>Which skills are most important for migrating or implementing cloud-based infrastructure? Expertise in containers, Kubernetes, and monitoring all scored highly, but the number one skill area was cloud-based security. (The survey design encouraged respondents to select from among multiple listed skills.)</p>



<p>Almost two-thirds of respondents (65%) selected cloud security, with monitoring (58%) a distant number two. General cloud knowledge was third (just over 56%), followed by containers and Kubernetes (just under 56%), respectively. All told, six separate skills polled at 50% or greater; 10 listed skills polled at 45% or greater. Clearly, respondents believe that they—along with other infrastructure and ops practitioners—need to skill up, with emphasis on security. Almost half (48%) of respondents selected six or more listed skills; 85% selected at least three listed skills. And 15% selected all 10 listed skills.</p>



<p>We looked at the intersection of skills to see if respondents had selected specific combinations of skills more frequently than might otherwise be expected. We discovered obvious examples of correlation (i.e., a threshold at least 5% higher than expected) with containers and Kubernetes; containers and microservices; monitoring and observability; and between security and compliance. We found several examples of correlation between cloud-based security and other listed skills, which reinforces the idea that security dominates the thinking of infrastructure and ops practitioners; we found correlations involving security and monitoring; security and performance; and between security and observability.</p>



<p>Finally, respondents selected some skill combinations less frequently than would be expected.<sup>5</sup> Some of these results are baffling, such as the absence of a correlation between microservices and security. Some examples of strong correlation (microservices and Kubernetes; containers and microservices) are consistent with trends we’ve described elsewhere, e.g., <a rel="noreferrer noopener" aria-label="the Next Architecture (opens in a new tab)" href="https://www.oreilly.com/radar/what-is-next-architecture/" target="_blank">the Next Architecture</a>.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0112-1048x691.png" alt="" class="wp-image-12865" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0112-1048x691.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0112-300x198.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0112-768x506.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0112.png 1296w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 12. Skills respondents’ organizations need for better migration and implementation of cloud-based infrastructure</figcaption></figure>



<h2>AI in Production, Poised for Growth</h2>



<p>Almost 36% of respondent organizations have deployed AI services. About 47% expect to deploy AI-based services at some point over the next three years; of these, the largest cohort (almost 20%) expects to do so in the next two years. Still, close to 53% do not anticipate doing anything with AI.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0113-1048x555.png" alt="" class="wp-image-12866" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0113-1048x555.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0113-300x159.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0113-768x406.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/05/ca20_0113.png 1181w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 13. When respondents whose organizations do not currently use AI services expect their organizations to adopt AI services</figcaption></figure>



<p>The discrepancy isn’t surprising. A survey on infrastructure and operations will tend to attract people who are interested in infrastructure and operations. Ditto for ML and AI. Every survey has a self-selection bias.</p>



<p>Still, the result seems anomalous. In the first case, it flies in the face of predominant trends. In <a rel="noreferrer noopener" aria-label="our recent machine learning (ML) and AI adoption survey (opens in a new tab)" href="https://www.oreilly.com/radar/ai-adoption-in-the-enterprise-2020/" target="_blank">our recent machine learning (ML) and AI adoption survey</a>, for example, we found that most organizations—about 53%—are using AI in production today. Even granting that AI is (over-)hyped, we should expect to see a <em>majority</em> result for planned AI adoption, shouldn’t we? In the second case, there are very good reasons why AI should be of interest to IT professionals who work in infrastructure and operations and (more important) the companies that employ them. Take <a rel="noreferrer noopener" aria-label="observability (opens in a new tab)" href="https://learning.oreilly.com/library/view/distributed-systems-observability/9781492033431/ch01.html" target="_blank">observability</a>, for example. It’s an important concept in software architecture, especially in next-generation regimes, such as microservice architecture. Machine learning and similar advanced techniques (e.g., deep learning) will likely play an important role in <em>observing</em> the observable systems that we build, just as AI-directed rules and AI-driven automation will be critical for managing and securing these systems.</p>



<p>How, then, can an organization expect to manage the thousands or tens of thousands of services that comprise an observable system without building AI services? One explanation is that respondents simply lack visibility into this aspect of their organization’s planning. In other words, because AI-related development is owned by one or more different groups—data scientists, ML and AI engineers, Data‐Ops practitioners—many respondents genuinely aren’t aware of what their organizations are doing. An equally likely explanation is that respondents are failing to appreciate what actually constitutes AI. <a rel="noreferrer noopener" aria-label="As we noted in another context (opens in a new tab)" href="https://www.oreilly.com/radar/6-trends-framing-the-state-of-ai-and-ml/" target="_blank">As we noted in another context</a>, “AI” used to be identified with so-called <a rel="noreferrer noopener" aria-label="artificial general intelligence (opens in a new tab)" href="https://en.wikipedia.org/wiki/Artificial_general_intelligence" target="_blank">artificial general intelligence</a>, or AGI. Increasingly, however, we’re seeing it used to describe the application of machine learning to solve problems, increase productivity, accelerate processes, and in many cases deliver wholly new products and services. Almost any consumer-facing site that makes product recommendations is using AI (although possibly in a very simple form.) It’s possible that some proportion of respondents had AGI in mind. Had we asked more specific questions, we likely would have gotten different results.</p>



<h2>Conclusion</h2>



<p>The survey was conceived and conducted in the months prior to the tumult of March and April. It is a product of a pre-pandemic sensibility.</p>



<p>The impact of a pandemic event isn’t just disruptive, it’s transformative: it fundamentally changes the status quo; it compels the revaluation of virtually all assumptions. This invites the obvious question: Were we to conceive this survey today, what would we do differently? Obviously, we’d ask questions that take into account the realities—e.g., an unprecedented emphasis on social isolation; a new (and mostly unprecedented) acceptance of telework, geographical separation, and distance(-ing); a business climate characterized by extreme uncertainty, with most analysts forecasting severe recession, if not possible depression—that serve as backdrop to this, our moment.</p>



<p>The most challenging thing about what’s happening is that it’s <em>very much happening</em>: we’re still coming to terms with it. Changes, compromises, reconfigurations that we never thought possible could become <em>de rigueur</em>. Other major changes will unfold over much larger periods of time.</p>



<p>It’s naïve to think we could anticipate even the broad strokes of these changes, let alone the specificity of their content. And it’s unhelpful—perhaps even dangerous—to overthink things. That said, it is useful to speculate about what could change in the near term, at least now that we have precedent for the unprecedented. It’s possible that the public cloud could become an even more attractive option for companies of all sizes. It’s possible that hybrid clouds combining PaaS or IaaS services with the virtual private cloud—that is, private cloud deployments which live in the public cloud—could see increased uptake, too. It’s also possible that more organizations will pursue multi-cloud as a strategy to hedge against potential disruption.</p>



<p>The hope of reducing costs won’t be the only thing driving new interest in cloud. Almost all enterprises are already dealing with staffing problems on several fronts: <em>first</em>, illness-related staffing shortages; <em>second</em>, staffing shortages that stem from shelter-in-place orders at the municipal, county, or state levels; <em>third</em>, furlough- or layoff-related staffing shortages. In some cases, IT workers have opted to withdraw from the workforce, e.g., to safeguard the health and well-being of their families. The combination of these and other staffing-related issues could compel companies to revisit not only the necessity for on-site/on-premises work, but the responsibility for hiring, recruiting, and managing IT staff to support applications or services that could shift to a third-party provider. Is it more credible for a large cloud service like Amazon, Google, IBM, or Microsoft to argue that its employees are essential than—for example—the IT staff of a major cosmetics retailer? More to the point, are the major cloud providers more likely to keep their datacenters running in the face of quarantines than a business with a private datacenter? The answer to the latter question has to be “yes.”</p>



<p>These are just a few possible changes. Had we the opportunity to redo our survey, we would almost certainly ask questions that drill down deeper into these issues. Nevertheless, we believe the results we capture here have considerable merit: not as quaint relics of a prelapsarian past, but as valid indications of where we were and <em>where we’ll be</em> when things pick back up again. There’s no reason to assume that the underlying trends here will be annulled by the effects of COVID-19. Impacted, yes; annulled, no. The shift to cloud, uptake of microservices, increasing interest in SRE, emphasis on Kubernetes, container virtualization, and other critical skills: each of these trends has staying power, especially to the degree that they’re implicated in or correlated to one another.</p>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<br />
</div></div>



<p>————————————————————————————————</p>



<p><sup>1</sup> Regional representation in Radar surveys typically tracks with usage on the O’Reilly learning platform. North American users account for about half of activity on the O’Reilly platform. That isn’t the case here.</p>



<p><sup>2</sup> For example: What criteria do we use to distinguish between private and public cloud? How do these criteria—and the distinction itself—relate to hybrid cloud? To virtual private cloud? What if an organization hosts its cloud in a colocation facility? Is it public or private? Is tenancy—e.g., a single-tenant cloud hosted in an off-site facility is a private cloud—the most important criterion? If an organization uses a combination of virtualization and automation to host some of its workloads, has it created a private cloud?</p>



<p><sup>3</sup> But <em>which</em> cloud? Or in which cloud context? The essential characteristics of modern software architecture—loose coupling; abstraction, isolation, and atomicity—are eliding the boundaries between what we think of as “cloud” versus “on-premises” contexts. As we wrote in <a href="https://www.oreilly.com/radar/oreilly-2020-platform-analysis/" target="_blank" rel="noopener noreferrer">our analysis of search and usage on the O’Reilly learning platform</a>: “Specific deployment contexts will still matter, of course … but the clear boundaries that used to demarcate the public cloud from the private cloud from conventional on-premises systems will fall away. It’s all cloud-like, irrespective of context.”</p>



<p><sup>4</sup> This looks like another case in which interest in a technology—namely, serverless—also tracks with interest in other, not necessarily related technologies, <a href="https://trends.google.com/trends/explore?date=today%205-y" target="_blank" rel="noopener noreferrer">in this case, microservices and SRE</a>. Even if serverless adoption lags, interest in it seems to wax and wane with (and perhaps benefit from) interest in these other technologies.</p>



<p><sup>5</sup> These include general cloud knowledge + security; general cloud knowledge + performance; microservices + security; compliance + monitoring; compliance + performance. All with a threshold 5% lower than expected.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/cloud-adoption-in-2020/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
	</channel>
</rss>
