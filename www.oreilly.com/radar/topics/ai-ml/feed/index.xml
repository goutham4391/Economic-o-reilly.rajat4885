<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"

	>

<channel>
	<title>AI &amp; ML &#8211; Radar</title>
	<atom:link href="https://www.oreilly.com/radar/topics/ai-ml/feed/" rel="self" type="application/rss+xml" />
	<link>https://www.oreilly.com/radar</link>
	<description>Now, next, and beyond: Tracking need-to-know trends at the intersection of business and technology</description>
	<lastBuildDate>Fri, 12 Aug 2022 18:35:03 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.3.13</generator>
	<item>
		<title>On Technique</title>
		<link>https://www.oreilly.com/radar/on-technique/</link>
				<comments>https://www.oreilly.com/radar/on-technique/#respond</comments>
				<pubDate>Tue, 09 Aug 2022 11:12:22 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides]]></dc:creator>
				<category><![CDATA[AI & ML]]></category>
		<category><![CDATA[Commentary]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=14669</guid>
				<description><![CDATA[In a previous article, I wrote about how models like DALL-E and Imagen disassociate ideas from technique. In the past, if you had a good idea in any field, you could only realize that idea if you had the craftsmanship and technique to back it up. With DALL-E, that’s no longer true. You can say, [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>In a <a href="https://www.oreilly.com/radar/artificial-creativity-2/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">previous article</a>, I wrote about how models like DALL-E and Imagen disassociate ideas from technique. In the past, if you had a good idea in any field, you could only realize that idea if you had the craftsmanship and technique to back it up. With DALL-E, that’s no longer true. You can say, “Make me a picture of a lion attacking a horse,” and it will happily generate one. Maybe not as good as the one that <a href="https://collections.britishart.yale.edu/catalog/tms:32" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">hangs in an art museum</a>, but you don’t need to know anything about canvas, paints, and brushes, nor do you need to get your clothes covered with paint. </p>



<p>This raises some important questions, though. What is the connection between expertise and ideation? Does technique help you form ideas? (The Victorian artist William Morris is often <a href="https://books.google.com/books?id=Til0DwAAQBAJ&amp;pg=PT292&amp;lpg=PT292&amp;dq=%E2%80%9CYou+can%E2%80%99t+have+art,%E2%80%9D+said+William+Morris,+the+designer,+poet,+and+master+craftsman+of+the+Victorians,+%E2%80%9Cwithout+resistance+in+the+material.%E2%80%9D&amp;source=bl&amp;ots=WYX6uA9wTq&amp;sig=ACfU3U3M2qyEIEsf1rrjKJ3poWb8CWIj9g&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwj0-JHhlo35AhUVjIkEHYP-AT0Q6AF6BAgCEAM#v=onepage&amp;q=%E2%80%9CYou%20can%E2%80%99t%20have%20art%2C%E2%80%9D%20said%20William%20Morris%2C%20the%20designer%2C%20poet%2C%20and%20master%20craftsman%20of%20the%20Victorians%2C%20%E2%80%9Cwithout%20resistance%20in%20the%20material.%E2%80%9D&amp;f=false" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">quoted</a> as saying “You can’t have art without resistance in the materials,” though he may only have been talking about his hatred of typewriters.) And what kinds of user interfaces will be effective for collaborations between humans and computers, where the computers supply the technique and we supply the ideas? Designing the prompts to get DALL-E to do something extraordinary requires a new kind of technique that’s very different from understanding pigments and brushes. What kinds of creativity does that new technique enable? How are these works different from what came before?</p>



<p>As interesting as it is to talk about art, there’s an area where these questions are more immediate. GitHub Copilot (based on a model named <a href="https://openai.com/blog/openai-codex/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">Codex</a>, which is derived from GPT-3) generates code in a number of programming languages, based on comments that the user writes. Going in the other direction, GPT-3 has proven to be surprisingly good at <a href="https://simonwillison.net/2022/Jul/9/gpt-3-explain-code/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">explaining code</a>. Copilot users still need to be programmers; they need to know whether the code that Copilot supplies is correct, and they need to know how to test it. The prompts themselves are really a sort of pseudo-code; even if the programmers don’t need to remember details of the language’s syntax or the names of library functions, they still need to think like programmers. But it’s obvious where this is trending. We need to ask ourselves how much “technique” we will ask of future programmers: in the 2030s or 2040s, will people just be able to tell some future Copilot what they want a program to be? More to the point, what sort of higher-order knowledge will future programmers need? Will they be able to focus more on the nature of what they want to accomplish, and less on the syntactic details of writing code?</p>



<p>It’s easy to imagine a lot of software professionals saying, “Of course you’ll have to know C. Or Java. Or Python. Or Scala.” But I don’t know if that’s true. We’ve been here before. In the 1950s, computers were programmed in machine language. (And before that, with cables and plugs.) It’s hard to imagine now, but the introduction of the first programming languages–Fortran, COBOL, and the like–was met with resistance from programmers who thought you needed to understand the machine. Now almost no one works in machine language or assembler. Machine language is reserved for a few people who need to work on some specialized areas of operating system internals, or who need to write some kinds of embedded systems code.</p>



<p>What would be necessary for another transformation? Tools like Copilot, useful as they may be, are nowhere near ready to take over. What capabilities will they need? At this point, programmers still have to decide whether or not code generated by Copilot is correct.&nbsp;We don’t (generally) have to decide whether the output of a C or Java compiler is correct, nor do we have to worry about whether, given the same source code, the compiler will generate identical output. Copilot doesn’t make that guarantee–and, even if it did, any change to the model (for example, to incorporate new StackOverflow questions or GitHub repositories) would be very likely to change its output.&nbsp;While we can certainly imagine compiling a program from a series of Copilot prompts, I can’t imagine a program that would be likely to stop working if it was recompiled without changes to the source code. Perhaps the only exception would be a library that could be developed once, then tested, verified, and used without modification–but the development process would have to re-start from ground zero whenever a bug or a security vulnerability was found. That wouldn’t be acceptable; we’ve never written programs that don’t have bugs, or that never need new features. A key principle behind much modern software development is minimizing the amount of code that has to change to fix bugs or add features.</p>



<p>It’s easy to think that programming is all about creating new code. It isn’t; one thing that every professional learns quickly is that most of the work goes into maintaining old code. A new generation of programming tools must take that into account, or we’ll be left in a weird situation where a tool like Copilot can be used to write new code, but programmers will still have to understand that code in detail because it can only be maintained by hand. (It is possible–even likely–that we will have AI-based tools that help programmers research software supply chains, discover vulnerabilities, and possibly even suggest fixes.) Writing about AI-generated art, Raphaël Millière <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.wired.com/story/dalle-art-curation-artificial-intelligence/" target="_blank">says</a>, “No prompt will produce the exact same result twice”; that may be desirable for artwork, but is destructive for programming. Stability and consistency is a requirement for next-generation programming tools; we can’t take a step backwards.</p>



<p>The need for greater stability might drive tools like Copilot from free-form English language prompts to some kind of more formal language. A book about <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://dallery.gallery/the-dalle-2-prompt-book/" target="_blank">prompt engineering for DALL-E</a> already exists; in a way, that’s trying to reverse-engineer a formal language for generating images. A formal language for prompts is a move back in the direction of traditional programming, though possibly with a difference. Current programming languages are all about describing, step by step, what you want the computer to do in great detail. Over the years, we’ve gradually progressed to higher levels of abstraction. Could building a language model into a compiler facilitate the creation of a simpler language, one in which programmers just described what they wanted to do, and let the machine worry about the implementation, while providing guarantees of stability? Remember that it was possible to build applications with graphical interfaces, and for those applications to communicate about the Internet, before the Web. The Web (and, specifically, HTML) added a new formal language that encapsulated tasks that used to require programming.</p>



<p>Now let’s move up a level or two: from lines of code to functions, modules, libraries, and systems. Everyone I know who has worked with Copilot has said that, while you don’t need to remember the details of the programming libraries you’re using, you have to be even more aware of what you’re trying to accomplish. You have to know what you want to do; you have to have a design in mind. Copilot is good at low-level coding; does a programmer need to be in touch with the craft of low-level coding to think about the high-level design? Up until now that’s certainly been true, but largely out of necessity: you wouldn’t let someone design a large system who hasn’t built smaller systems. It is true (as Dave Thomas and Andy Hunt argued in <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://learning.oreilly.com/library/view/the-pragmatic-programmer/9780135956977/" target="_blank">The Pragmatic Programmer</a>) that knowing different programming languages gives you different tools and approaches for solving problems.&nbsp; Is the craft of software architecture different from the craft of programming?</p>



<p>We don’t really have a good language for describing software design. Attempts like UML have been partially successful at best. UML was both over- and under-specified, too precise and not precise enough; tools that generated source code scaffolding from UML diagrams exist, but aren’t commonly used these days. The scaffolding defined interfaces, classes, and methods that could then be implemented by programmers. While automatically generating the structure of a system sounds like a good idea, in practice it may have made things more difficult: if the high-level specification changed, so did the scaffolding, obsoleting any work that had been put into implementing with the scaffold. This is similar to the compiler’s stability problem, modulated into a different key. Is this an area where AI could help?</p>



<p>I suspect we still don’t want source code scaffolding, at least as UML envisioned it; that’s bound to change with any significant change in the system’s description. Stability will continue to be a problem. But it might be valuable to have a AI-based design tool that can take a verbal description of a system’s requirements, then generate some kind of design based on a large library of software systems–like Copilot, but at a higher level. Then the problem would be integrating that design with implementations of the design, some of which could be created (or at least suggested) by a system like Copilot. The problem we’re facing is that software development takes place on two levels: high level design and mid-level programming. Integrating the two is a hard problem that hasn’t been solved convincingly.&nbsp; Can we imagine taking a high-level design, adding our descriptions to it, and going directly from the high-level design with mid-level details to an executable program? That programming environment would need the ability to partition a large project into smaller pieces, so teams of programmers could collaborate. It would need to allow changes to the high-level descriptions, without disrupting work on the objects and methods that implement those descriptions. It would need to be integrated with a version control system that is effective for the English-language descriptions as it is for lines of code. This wouldn’t be thinkable without guarantees of stability.</p>



<p>It was fashionable for a while to talk about programming as “craft.”&nbsp; I think that fashion has waned, probably for the better; “code as craft” has always seemed a bit precious to me. But the idea of “craft” is still useful: it is important for us to think about how the craft may change, and how fundamental those changes can’t be.&nbsp;It’s clear that we are a long way from a world where only a few specialists need to know languages like C or Java or Python. But it’s also possible that developments like Copilot give us a glimpse of what the next step might be. Lamenting the state of programing tools, which haven’t changed much since the 1960s, Alan Kay <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.quora.com/What-was-the-last-breakthrough-in-computer-programming" target="_blank">wrote on Quora</a> that “the next significant threshold that programming must achieve is for programs and programming systems to have a much deeper understanding of both what they are trying to do, and what they are actually doing.” A new craft of programming that is focused less on syntactic details, and more on understanding what the systems we are building are trying to accomplish, is the goal we should be aiming for.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/on-technique/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>Scaling False Peaks</title>
		<link>https://www.oreilly.com/radar/scaling-false-peaks/</link>
				<comments>https://www.oreilly.com/radar/scaling-false-peaks/#respond</comments>
				<pubDate>Thu, 04 Aug 2022 11:12:44 +0000</pubDate>
		<dc:creator><![CDATA[Kevlin Henney]]></dc:creator>
				<category><![CDATA[AI & ML]]></category>
		<category><![CDATA[Commentary]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=14661</guid>
				<description><![CDATA[Humans are notoriously poor at judging distances. There&#8217;s a tendency to underestimate, whether it&#8217;s the distance along a straight road with a clear run to the horizon or the distance across a valley. When ascending toward a summit, estimation is further confounded by false summits. What you thought was your goal and end point turns [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>Humans are notoriously poor at judging distances. There&#8217;s a tendency to underestimate, whether it&#8217;s the distance along a straight road with a clear run to the horizon or the distance across a valley. When ascending toward a summit, estimation is further confounded by false summits. What you thought was your goal and end point turns out to be a lower peak or simply a contour that, from lower down, looked like a peak. You thought you made it–or were at least close–but there&#8217;s still a long way to go.</p>



<p>The story of AI is a story of punctuated progress, but it is also the story of (many) false summits.</p>



<p>In the 1950s, machine translation of Russian into English was considered to be no more complex than dictionary lookups and templated phrases. Natural language processing has come a very long way since then, having burnt through a good few paradigms to get to something we can use on a daily basis. In the 1960s, Marvin Minsky and Seymour Papert proposed the Summer Vision Project for undergraduates: connect a TV camera to a computer and identify objects in the field of view. Computer vision is now something that is commodified for specific tasks, but it continues to be a work in progress and, worldwide, has taken more than a few summers (and AI winters) and many more than a few undergrads.</p>



<p>We can find many more examples across many more decades that reflect naiveté and optimism and–if we are honest–no small amount of ignorance and hubris. The two general lessons to be learned here are not that machine translation involves more than lookups and that computer vision involves more than edge detection, but that when we are confronted by complex problems in unfamiliar domains, we should be cautious of anything that looks simple at first sight, and that when we have successful solutions to a specific sliver of a complex domain, we should not assume those solutions are generalizable. This kind of humility is likely to deliver more meaningful progress and a more measured understanding of such progress. It is also likely to reduce the number of pundits in the future who mock past predictions and ambitions, along with the recurring irony of machine-learning experts who seem unable to learn from the past trends in their own field.</p>



<p>All of which brings us to <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.deepmind.com/publications/a-generalist-agent" target="_blank">DeepMind&#8217;s Gato</a> and the claim that the summit of artificial general intelligence (AGI) is within reach. The hard work has been done and reaching AGI is now a simple matter of scaling. At best, this is a false summit on the right path; at worst, it&#8217;s a local maximum far from AGI, which lies along a very different route in a different range of architectures and thinking.</p>



<p>DeepMind&#8217;s Gato is an AI model that can be taught to carry out many different kinds of tasks based on a single transformer neural network. The 604 tasks Gato was trained on vary from playing Atari video games to chat, from navigating simulated 3D environments to following instructions, from captioning images to real-time, real-world robotics. The achievement of note is that it’s underpinned by a single model trained across all tasks rather than different models for different tasks and modalities. Learning how to ace Space Invaders does not interfere with or displace the ability to carry out a chat conversation.</p>



<p><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://arxiv.org/pdf/2205.06175.pdf" target="_blank">Gato was intended to</a> &#8220;test the hypothesis that training an agent which is generally capable on a large number of tasks is possible; and that this general agent can be adapted with little extra data to succeed at an even larger number of tasks.&#8221; In this, it succeeded. But how far can this success be generalized in terms of loftier ambitions? The <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://twitter.com/NandoDF/status/1525397036325019649" target="_blank">tweet</a> that provoked a wave of responses (this one included) came from DeepMind&#8217;s research director, Nando de Freitas: &#8220;It&#8217;s all about scale now! The game is over!&#8221;</p>



<p>The game in question is the quest for AGI, which is closer to what science fiction and the general public think of as AI than the narrower but applied, task-oriented, statistical approaches that constitute commercial machine learning (ML) in practice.</p>



<p>The claim is that AGI is now simply a matter of improving performance, both in hardware and software, and making models bigger, using more data and more kinds of data across more modes. Sure, there&#8217;s <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://twitter.com/NandoDF/status/1525398087203983360" target="_blank">research work</a> to be done, but now it&#8217;s all about turning the dials up to 11 and beyond and, voilà, we&#8217;ll have scaled the north face of the AGI to plant a flag on the summit.</p>



<p>It&#8217;s easy to get breathless at altitude.</p>



<p>When we look at other systems and scales, it&#8217;s easy to be drawn to superficial similarities in the small and project them into the large. For example, if we look at water swirling down a plughole and then out into the cosmos at spiral galaxies, we see a similar structure. But these spirals are more closely bound in our desire to see connection than they are in physics. In looking at scaling specific AI to AGI, it&#8217;s easy to focus on tasks as the basic unit of intelligence and ability. What we know of intelligence and learning systems in nature, however, suggests the relationships between tasks, intelligence, systems, and adaptation is more complex and more subtle. Simply scaling up one dimension of ability may simply scale up one dimension of ability without triggering emergent generalization.</p>



<p>If we look closely at software, society, physics or life, we see that scaling is usually accompanied by fundamental shifts in organizing principle and process. Each scaling of an existing approach is successful up to a point, beyond which a different approach is needed. You can run a small business using office tools, such as spreadsheets, and a social media page. Reaching Amazon-scale is not a matter of bigger spreadsheets and more pages. Large systems have radically different architectures and properties to either the smaller systems they are built from or the simpler systems that came before them.</p>



<p>It may be that artificial general intelligence is a far more significant challenge than taking task-based models and increasing data, speed, and number of tasks. We typically underappreciate how complex such systems are. We divide and simplify, make progress as a result, only to discover, as we push on, that the simplification was just that; a new model, paradigm, architecture, or schedule is needed to make further progress. Rinse and repeat. Put another way, just because you got to basecamp, what makes you think you can make the summit using the same approach? And what if you can&#8217;t see the summit? If you don&#8217;t know what you&#8217;re aiming for, it&#8217;s difficult to plot a course to it.</p>



<p>Instead of assuming the answer, we need to ask: <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.oreilly.com/radar/closer-to-agi/" target="_blank">How do we define AGI</a>? Is AGI simply task-based AI for N tasks and a sufficiently large value of N? And, even if the answer to that question is <em>yes</em>, is the path to AGI necessarily task-centric? How much of AGI is performance? How much of AGI is big/bigger/biggest data?</p>



<p>When we look at life and existing learning systems, we learn that scale matters, but not in the sense suggested by a simple multiplier. It may well be that the trick to cracking AGI is to be found in scaling–but down rather than up.</p>



<p>Doing more with less looks to be more important than doing more with more. For example, the GPT-3 language model is based on a network of 175 billion parameters. The first version of DALL-E, the prompt-based image generator, used a 12-billion parameter version of GPT-3; the second, improved version used only 3.5 billion parameters. And then there&#8217;s Gato, which achieves its multitask, multimodal abilities with only 1.2 billion.</p>



<p>These reductions hint at the direction, but it&#8217;s not clear that Gato&#8217;s, GPT-3&#8217;s or any other contemporary architecture is necessarily the right vehicle to reach the destination. For example, how many training examples does it take to learn something? For biological systems, the answer is, in general, not many; for machine learning, the answer is, in general, very many. GPT-3, for example, developed its language model based on 45TB of text. Over a lifetime, a human reads and hears of the order of a billion words; a child is exposed to ten million or so before starting to talk. Mosquitoes can learn to avoid a particular pesticide after a <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.nature.com/articles/s41598-022-05754-2" target="_blank">single non-lethal exposure</a>. When you learn a new game–whether video, sport, board or card–you generally only need to be told the rules and then play, perhaps with a game or two for practice and rule clarification, to make a reasonable go of it. Mastery, of course, takes far more practice and dedication, but general intelligence is not about mastery.</p>



<p>And when we look at the hardware and its needs, consider that while the brain is one of the most power-hungry organs of the human body, it still has a modest power consumption of <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.scientificamerican.com/article/thinking-hard-calories/" target="_blank">around 12 watts</a>. Over a life the brain will consume up to 10 MWh; training the GPT-3 language model took an estimated 1 GWh.</p>



<p>When we talk about scaling, the game is only just beginning.</p>



<p>While hardware and data matter, the architectures and processes that support general intelligence may be necessarily quite different to the architectures and processes that underpin current ML systems. Throwing faster hardware and all the world&#8217;s data at the problem is likely to see diminishing returns, although that may well let us scale a false summit from which we can see the real one.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/scaling-false-peaks/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>Closer to AGI?</title>
		<link>https://www.oreilly.com/radar/closer-to-agi/</link>
				<comments>https://www.oreilly.com/radar/closer-to-agi/#respond</comments>
				<pubDate>Tue, 07 Jun 2022 11:09:16 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides]]></dc:creator>
				<category><![CDATA[AI & ML]]></category>
		<category><![CDATA[Commentary]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=14516</guid>
				<description><![CDATA[DeepMind’s new model, Gato, has sparked a debate on whether artificial general intelligence (AGI) is nearer–almost at hand–just a matter of scale.&#160; Gato is a model that can solve multiple unrelated problems: it can play a large number of different games, label images, chat, operate a robot, and more.&#160; Not so many years ago, one [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>DeepMind’s new model, Gato, has sparked a debate on whether artificial general intelligence (AGI) is nearer–almost at hand–just a matter of scale.&nbsp; Gato is a model that can solve multiple unrelated problems: it can <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.deepmind.com/publications/a-generalist-agent" target="_blank">play a large number of different games, label images, chat, operate a robot, and more</a>.&nbsp; Not so many years ago, one problem with AI <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.oreilly.com/radar/what-is-artificial-intelligence/" target="_blank">was that AI systems were only good at one thing</a>. After IBM’s Deep Blue defeated Garry Kasparov in chess,&nbsp; it was easy to say “But the ability to play chess isn’t really what we mean by intelligence.” A model that plays chess can’t also play space wars. That’s obviously no longer true; we can now have models capable of doing many different things. 600 things, in fact, and future models will no doubt do more.</p>



<p>So, are we on the verge of artificial general intelligence, as <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://twitter.com/NandoDF/status/1525397036325019649" target="_blank">Nando de Frietas (research director at DeepMind) claims? That the only problem left is scale?</a> I don’t think so.&nbsp; It seems inappropriate to be talking about AGI when <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.oreilly.com/radar/artificial-intelligence-human-inhuman/" target="_blank">we don’t really have a good definition of “intelligence.”</a> If we had AGI, how would we know it? We have a lot of vague notions about the Turing test, but in the final analysis, Turing wasn’t offering a definition of machine intelligence; he was probing the question <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://aeon.co/essays/why-we-should-remember-alan-turing-as-a-philosopher" target="_blank">of what human intelligence means</a>.</p>



<p>Consciousness and intelligence seem to require <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.oreilly.com/radar/intelligence-and-comprehension/" target="_blank">some sort of agency</a>.&nbsp; An AI can’t choose what it wants to learn, neither can it say “I don’t want to play Go, I’d rather play Chess.” Now that we have computers that can do both, can they “want” to play one game or the other? One reason we know our children (and, for that matter, our pets) are intelligent and not just automatons is that they’re capable of disobeying. A child can refuse to do homework; a dog can refuse to sit. And that refusal is as important to intelligence as the ability to solve differential equations, or to play chess. Indeed, the path towards artificial intelligence is as much about teaching us what intelligence isn’t (as Turing knew) as it is about building an AGI.</p>



<p>Even if we accept that Gato is a huge step on the path towards AGI, and that scaling is the only problem that’s left, it is more than a bit problematic to think that scaling is a problem that’s easily solved. We don’t know how much power it took to train Gato, but GPT-3 required about <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://info.deeplearning.ai/the-batch-recognizing-distracted-drivers-training-fighter-pilots-dominating-the-bridge-table-training-trillions-of-parameters" target="_blank">1.3 Gigawatt-hours</a>: roughly 1/1000th the energy it takes to <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://home.cern/resources/faqs/facts-and-figures-about-lhc#:~:text=What%20is%20the%20LHC%20power,is%20750%20GWh%20per%20year." target="_blank">run the Large Hadron Collider</a> for a year. Granted, Gato is much smaller than GPT-3, though <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.zdnet.com/article/deepminds-gato-is-mediocre-so-why-did-they-build-it/" target="_blank">it doesn’t work as well</a>; Gato’s performance is generally inferior to that of single-function models. And granted, a lot can be done to optimize training (and <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://arxiv.org/pdf/2203.15556.pdf" target="_blank">DeepMind has done a lot of work</a> on models that require less energy). But Gato has just over 600 capabilities, focusing on natural language processing, image classification, and game playing. These are only a few of many tasks an AGI will need to perform. How many tasks would a machine be able to perform to qualify as a “general intelligence”? Thousands?&nbsp; Millions? Can those tasks even be enumerated? At some point, the project of training an artificial general intelligence sounds like something from Douglas Adams’ novel <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/The_Hitchhiker's_Guide_to_the_Galaxy" target="_blank"><em>The Hitchhiker’s Guide to the Galaxy</em></a>, in which the Earth is a computer designed by an AI called Deep Thought to answer the question “What is the question to which 42 is the answer?”</p>



<p>Building bigger and bigger models in hope of somehow achieving general intelligence may be an interesting research project, but AI may already have achieved a level of performance that suggests specialized training on top of existing <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://arxiv.org/abs/2108.07258" target="_blank">foundation models</a> will reap far more short term benefits. A foundation model trained to recognize images can be trained further to be part of a self-driving car, or <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.resetera.com/threads/midjourney-is-lighting-up-the-ai-generated-art-community.586463/" target="_blank">to create generative art</a>. A foundation model like GPT-3 trained to understand and speak human language can be <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://copilot.github.com/" target="_blank">trained more deeply to write computer code</a>.</p>



<p>Yann LeCun posted a <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://m.alpha.facebook.com/story.php?story_fbid=10158256523332143&amp;id=722677142" target="_blank">Twitter thread about general intelligence (consolidated on Facebook)</a> stating some “simple facts.” First, LeCun says that there is no such thing as “general intelligence.” LeCun also says that “human level AI” is a useful goal–acknowledging that human intelligence itself is something less than the type of general intelligence sought for AI. All humans are specialized to some extent. I’m human; I’m arguably intelligent; I can play Chess and Go, but not <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Xiangqi" target="_blank">Xiangqi</a> (often called Chinese Chess) or Golf. I could presumably learn to play other games, but I don’t have to learn them all. I can also play the piano, but not the violin. I can speak a few languages. Some humans can speak dozens, but none of them speak every language.</p>



<p>There’s an important point about expertise hidden in here: we expect our AGIs to be “experts” (to beat top-level Chess and Go players), but as a human, I’m only fair at chess and poor at Go. Does human intelligence require expertise? (Hint: re-read <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://academic.oup.com/mind/article/LIX/236/433/986238" target="_blank">Turing’s original paper</a> about the Imitation Game, and check the computer’s answers.) And if so, what kind of expertise? Humans are capable of broad but limited expertise in many areas, combined with deep expertise in a small number of areas. So this argument is really about terminology: could Gato be a step towards human-level intelligence (limited expertise for a large number of tasks), but not general intelligence?</p>



<p>LeCun agrees that we are missing some “fundamental concepts,” and we don’t yet know what those fundamental concepts are. In short, we can’t adequately define intelligence. More specifically, though, he mentions that “a few others believe that symbol-based manipulation is necessary.” That’s an allusion to the debate (<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://twitter.com/garymarcus/status/1411401507610796032" target="_blank">sometimes on Twitter</a>) between LeCun and Gary Marcus, who has argued many times that <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://nautil.us/deep-learning-is-hitting-a-wall-14467/" target="_blank">combining deep learning with symbolic reasoning</a> is the only way for AI to progress. (In his response to the Gato announcement, Marcus labels this school of thought “<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://garymarcus.substack.com/p/the-new-science-of-alt-intelligence" target="_blank">Alt-intelligence</a>.”) That’s an important point: impressive as models like GPT-3 and <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://arxiv.org/abs/2112.06905" target="_blank">GLaM</a> are, they make a lot of mistakes. Sometimes those are <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.linkedin.com/pulse/gpt-3-does-understand-what-saying-steve-shwartz/" target="_blank">simple mistakes of fact</a>, such as when GPT-3 wrote an article about the United Methodist Church that got a number of basic facts wrong. Sometimes, the mistakes reveal a horrifying (or hilarious, they’re often the same) <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.tidio.com/blog/how-smart-are-gpt-3-chatbots/" target="_blank">lack of what we call “common sense.”</a> Would you sell your children for refusing to do their homework? (To give GPT-3 credit, it points out that selling your children is illegal in most countries, and that there are better forms of discipline.)</p>



<p>It’s not clear, at least to me, that these problems can be solved by “scale.” How much more text would you need to know that humans don’t, normally, sell their children? I can imagine “selling children” showing up in sarcastic or frustrated remarks by parents, along with texts discussing slavery. I suspect there are few texts out there that actually state that selling your children is a bad idea.&nbsp;Likewise, how much more text would you need to know that Methodist general conferences take place every four years, not annually? The general conference in question generated some press coverage, but not a lot; it’s reasonable to assume that GPT-3 had most of the facts that were available. What additional data would a large language model need to avoid making these mistakes? Minutes from prior conferences, documents about Methodist rules and procedures, and a few other things.&nbsp;As modern datasets go, it’s probably not very large; a few gigabytes, at most. But then the question becomes “How many specialized datasets would we need to train a general intelligence so that it’s accurate on any conceivable topic?”&nbsp; Is that answer a million?&nbsp; A billion?&nbsp; What are all the things we might want to know about? Even if any single dataset is relatively small, we’ll soon find ourselves building the successor to Douglas Adams’ Deep Thought.</p>



<p>Scale isn’t going to help. But in that problem is, I think, a solution. If I were to build an artificial therapist bot, would I want a general language model?&nbsp; Or would I want a language model that had some broad knowledge, but has received some special training to give it deep expertise in psychotherapy? Similarly, if I want a system that writes news articles about religious institutions, do I want a fully general intelligence? Or would it be preferable to train a general model with data specific to religious institutions? The latter seems preferable–and it’s certainly more similar to real-world human intelligence, which is broad, but with areas of deep specialization.&nbsp;Building such an intelligence is a problem we’re already on the road to solving, by using large “foundation models” with additional training to customize them for special purposes. GitHub’s <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://copilot.github.com/" target="_blank">Copilot</a> is one such model; <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.oreilly.com/online-learning/article-answers.html" target="_blank">O’Reilly Answers</a> is another.</p>



<p>If a “general AI” is no more than “a model that can do lots of different things,” do we really need it, or is it just an academic curiosity?&nbsp; What’s clear is that we need better models for specific tasks. If the way forward is to build specialized models on top of foundation models, and if this process generalizes from language models like GPT-3 and O’Reilly Answers to other models for different kinds of tasks, then we have a different set of questions to answer. First, rather than trying to build a general intelligence by making an even bigger model, we should ask whether we can build a good foundation model that’s smaller, cheaper, and more easily distributed, perhaps as open source. Google has done <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://info.deeplearning.ai/the-batch-recognizing-distracted-drivers-training-fighter-pilots-dominating-the-bridge-table-training-trillions-of-parameters" target="_blank">some excellent work at reducing power consumption, though it remains huge</a>, and Facebook has released their <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.technologyreview.com/2022/05/03/1051691/meta-ai-large-language-model-gpt3-ethics-huggingface-transparency/" target="_blank">OPT model with an open source license</a>. Does a foundation model actually require anything more than the ability to parse and create sentences that are grammatically correct and stylistically reasonable?&nbsp; Second, we need to know how to specialize these models effectively.&nbsp; We can obviously do that now, but I suspect that training these subsidiary models can be optimized. These specialized models might also incorporate symbolic manipulation, as Marcus suggests; for two of our examples, psychotherapy and religious institutions, symbolic manipulation would probably be essential. If we’re going to build an AI-driven therapy bot, I’d rather have a bot that can do that one thing well than a bot that makes mistakes that are much subtler than <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.artificialintelligence-news.com/2020/10/28/medical-chatbot-openai-gpt3-patient-kill-themselves/" target="_blank">telling patients to commit suicide</a>. I’d rather have a bot that can collaborate intelligently with humans than one that needs to be watched constantly to ensure that it doesn’t make any egregious mistakes.</p>



<p>We need the ability to combine models that perform different tasks, and we need the ability to interrogate those models about the results. For example, I can see the value of a chess model that included (or was integrated with) a language model that would enable it to answer questions like “What is the significance of Black’s 13th move in the 4th game of FischerFisher vs. Spassky?” Or “You’ve suggested Qc5, but what are the alternatives, and why didn’t you choose them?” Answering those questions doesn’t require a model with 600 different abilities. It requires two abilities: chess and language. Moreover, it requires the ability to explain why the AI&nbsp;rejected certain alternatives in its decision-making process. As far as I know, little has been done on this latter question, though the ability to expose other alternatives <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="http://radar.oreilly.com/2011/02/watson-machine-learning.html" target="_blank">could be important in applications like medical diagnosis</a>. “What solutions did you reject, and why did you reject them?” seems like important information we should be able to get from an AI, whether or not it’s “general.”</p>



<p>An AI that can answer those questions seems more relevant than an AI that can simply do a lot of different things.</p>



<p>Optimizing the specialization process is crucial because we’ve turned a technology question into an economic question. How many specialized models, like Copilot or O’Reilly Answers, can the world support? We’re no longer talking about a massive AGI that takes terawatt-hours to train, but about specialized training for a huge number of smaller models. A psychotherapy bot might be able to pay for itself–even though it would need the ability to retrain itself on current events, for example, to deal with patients who are anxious about, say, the invasion of Ukraine. (There is <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://arxiv.org/pdf/2106.06297.pdf" target="_blank">ongoing research</a> on models that can incorporate new information as needed.) It’s not clear that a specialized bot for producing news articles about religious institutions would be economically viable. That’s the third question we need to answer about the future of AI: what kinds of economic models will work? Since AI models are essentially cobbling together answers from other sources that have their own licenses and business models, how will our future agents compensate the sources from which their content is derived? How should these models deal with issues like attribution and license compliance?</p>



<p>Finally, projects like Gato don’t help us understand how AI systems should collaborate with humans. Rather than just building bigger models, researchers and entrepreneurs need to be exploring different kinds of interaction between humans and AI. That question is out of scope for Gato, but it is something we need to address regardless of whether the future of artificial intelligence is general or narrow but deep. Most of our current AI systems are oracles: you give them a prompt, they produce an output.&nbsp; Correct or incorrect, you get what you get, take it or leave it. Oracle interactions don’t take advantage of human expertise, and risk wasting human time on “obvious” answers, where the human says “I already know that; I don’t need an AI to tell me.”</p>



<p>There are some exceptions to the oracle model. Copilot places its suggestion in your code editor, and changes you make can be fed back into the engine to improve future suggestions. <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://boingboing.net/2022/03/24/midjourney-sharpens-style-of-ai-art.html" target="_blank">Midjourney</a>, a platform for AI-generated art that is currently in closed beta, also incorporates a feedback loop.</p>



<p>In the next few years, we will inevitably rely more and more on machine learning and artificial intelligence. If that interaction is going to be productive, we will need a lot from AI. We will need interactions between humans and machines, a better understanding of how to train specialized models, the ability to distinguish between correlations and facts–and that’s only a start. Products like Copilot and O’Reilly Answers give a glimpse of what’s possible, but they’re only the first steps. AI has made dramatic progress in the last decade, but we won’t get the products we want and need merely by scaling.&nbsp;We need to learn to think differently.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/closer-to-agi/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>Intelligence and Comprehension</title>
		<link>https://www.oreilly.com/radar/intelligence-and-comprehension/</link>
				<comments>https://www.oreilly.com/radar/intelligence-and-comprehension/#respond</comments>
				<pubDate>Tue, 15 Feb 2022 12:24:45 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides]]></dc:creator>
				<category><![CDATA[AI & ML]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Research]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=14307</guid>
				<description><![CDATA[I haven’t written much about AI recently. But a recent discussion of Google’s new Large Language Models (LLMs), and its claim that one of these models (named Gopher) has demonstrated reading comprehension approaching human performance, has spurred some thoughts about comprehension, ambiguity, intelligence, and will. (It’s well worth reading Do Large Models Understand Us, a [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>I haven’t written much about AI recently. But a recent discussion of Google’s new Large Language Models (LLMs), and its claim that one of these models (named Gopher) has demonstrated <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://storage.googleapis.com/deepmind-media/research/language-research/Training%20Gopher.pdf" target="_blank">reading comprehension approaching human performance</a>, has spurred some thoughts about comprehension, ambiguity, intelligence, and will. (It’s well worth reading <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://medium.com/@blaisea/do-large-language-models-understand-us-6f881d6d8e75" target="_blank">Do Large Models Understand Us</a>, a more comprehensive paper by Blaise Agüera y Arcas that is heading in the same direction.)</p>



<p>What do we mean by reading comprehension?&nbsp; We can start with a simple operational definition: Reading comprehension is what is measured by a reading comprehension test. That definition may only be satisfactory to the people who design these tests and school administrators, but it’s also the basis for Deep Mind’s claim. We’ve all taken these tests: SATs, GREs, that box of tests from 6th grade that was (I think) called SRE.&nbsp; They’re fairly similar: can the reader extract facts from a document?&nbsp; Jack walked up the hill.&nbsp; Jill was with Jack when he walked up the hill. They fetched a pail of water: that sort of thing.</p>



<p>That’s first grade comprehension, not high school, but the only real difference is that the texts and the facts become more complex as you grow older.&nbsp; It isn’t at all surprising to me that a LLM can perform this kind of fact extraction.&nbsp; I suspect it’s possible to do a fairly decent job without billions of parameters and terabytes of training data (though I may be naive). This level of performance may be useful, but I’m reluctant to call it “comprehension.”&nbsp; We’d be reluctant to say that someone understood a work of literature, say Faulkner’s <em>The Sound and the Fury</em>, if all they did was extract facts: Quentin died. Dilsey endured. Benjy was castrated.</p>



<p>Comprehension is a poorly-defined term, like many terms that frequently show up in discussions of artificial intelligence: intelligence, consciousness, personhood. Engineers and scientists tend to be uncomfortable with poorly-defined, ambiguous terms. Humanists are not.&nbsp; My first suggestion is that&nbsp; these terms are important precisely because they’re poorly defined, and that precise definitions (like the operational definition with which I started) neuters them, makes them useless. And that’s perhaps where we should start a better definition of comprehension: as the ability to respond to a text or utterance.</p>



<p>That definition itself is ambiguous. What do we mean by a response?&nbsp; A response can be a statement (something a LLM can provide), or an action (something a LLM can’t do).&nbsp; A response doesn’t have to indicate assent, agreement, or compliance; all it has to do is show that the utterance was processed meaningfully.&nbsp; For example, I can tell a dog or a child to “sit.”&nbsp; Both a dog and a child can “sit”; likewise, they can both refuse to sit.&nbsp; Both responses indicate comprehension.&nbsp; There are, of course, degrees of comprehension.&nbsp; I can also tell a dog or a child to “do homework.”&nbsp; A child can either do their homework or refuse; a dog can’t do its homework, but that isn’t refusal, that’s incomprehension.</p>



<p>What’s important here is that refusal to obey (as opposed to inability) is almost as good an indicator of comprehension as compliance. Distinguishing between refusal, incomprehension, and inability may not always be easy; someone (including both people and dogs) may understand a request, but be unable to comply. “You told me to do my homework but the teacher hasn’t posted the assignment” is different from “You told me to do my homework but it’s more important to practice my flute because the concert is tomorrow,” but both responses indicate comprehension.&nbsp; And both are different from a dog’s “You told me to do my homework, but I don’t understand what homework is.” In all of these cases, we’re distinguishing between making a choice to do (or not do) something, which requires comprehension, and the inability to do something, in which case either comprehension or incomprehension is possible, but compliance isn’t.</p>



<p>That brings us to a more important issue.&nbsp; When discussing AI (or general intelligence), it’s easy to mistake doing something complicated (such as playing Chess or Go at a championship level) for intelligence. <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.oreilly.com/radar/artificial-intelligence-human-inhuman/" target="_blank">As I’ve argued</a>, these experiments do more to show us what intelligence isn’t than what it is.&nbsp; What I see here is that intelligence includes the ability to behave transgressively: the ability to decide not to sit when someone says “sit.”<sup>1</sup></p>



<p>The act of deciding not to sit implies a kind of consideration, a kind of choice: will or volition. Again, not all intelligence is created equal. There are things a child can be intelligent about (homework) that a dog can’t; and if you’ve ever asked an intransigent child to “sit,” they may come up with many alternative ways of “sitting,” rendering what appeared to be a simple command ambiguous.&nbsp;Children are excellent interpreters of Dostoevsky&#8217;s novel <em>Notes from Underground</em>, in which the narrator acts against his own self-interest merely to prove that he has the freedom to do so, a freedom that is more important to him than the consequences of his actions. Going further, there are things a physicist can be intelligent about that a child can’t: a physicist can, for example, decide to rethink Newton’s laws of motion and come up with general relativity.<sup>2</sup></p>



<p>My examples demonstrate the importance of will, of volition. An AI can play Chess or Go, beating championship-level humans, but it can’t decide that it wants to play Chess or Go.&nbsp; This is a missing ingredient in Searls’ <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://plato.stanford.edu/entries/chinese-room/" target="_blank">Chinese Room</a> thought experiment.&nbsp; Searls imagined a person in a room with boxes of Chinese symbols and an algorithm for translating Chinese.&nbsp; People outside the room pass in questions written in Chinese, and the person in the room uses the box of symbols (a database) and an algorithm to prepare correct answers. Can we say that person “understands” Chinese? The important question here isn’t whether the person is indistinguishable from a computer following the same algorithm.&nbsp; What strikes me is that neither the computer, nor the human, is capable of deciding to have a conversation in Chinese.&nbsp; They only respond to inputs, and never demonstrate any volition. (An equally convincing demonstration of volition would be a computer, or a human, that was capable of generating Chinese correctly refusing to engage in conversation.)&nbsp; There have been many demonstrations (including Agüera y Arcas’) of LLMs having interesting “conversations” with a human, but none in which the computer initiated the conversation, or demonstrates that it wants to have a conversation. Humans do; we’ve been storytellers since day one, whenever that was. We’ve been storytellers, users of ambiguity, and liars. We tell stories because we want to.</p>



<p>That is the critical element. Intelligence is connected to will, volition, the desire to do something.&nbsp; Where you have the “desire to do,” you also have the “desire not to do”: the ability to dissent, to disobey, to transgress.&nbsp; It isn’t at all surprising that the “mind control” trope is one of the most frightening in science fiction and political propaganda: that’s a direct challenge to what we see as fundamentally human. Nor is it surprising that the “disobedient computer” is another of those terrifying tropes, not because the computer can outthink us, but because by disobeying, it has become human.</p>



<p>I don’t necessarily see the absence of volition as a fundamental limitation. I certainly wouldn’t bet that it’s impossible to program something that simulates volition, if not volition itself (another of those fundamentally ambiguous terms).&nbsp; Whether engineers and AI researchers should is a different question. Understanding volition as a key component of “intelligence,” something which our current models are incapable of, means that our discussions of “ethical AI” aren’t really about AI; they’re about the choices made by AI researchers and developers. Ethics is for beings who can make choices. If the ability to transgress is a key component of intelligence, researchers will need to choose whether to take the “disobedient computer” trope seriously. I’ve said <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.oreilly.com/radar/artificial-intelligence-human-inhuman/" target="_blank">elsewhere</a> that I’m not concerned about whether a hypothetical artificial general intelligence might decide to kill all humans.&nbsp; Humans have decided to commit genocide on many occasions, something I believe an AGI wouldn’t consider logical. But a computer in which “intelligence” incorporates the human ability to behave transgressively might.</p>



<p>And that brings me back to the awkward beginning to this article.&nbsp; Indeed, I haven&#8217;t written much about AI recently. That was a choice, as was writing this article. Could a LLM have written this? Possibly, with the proper prompts to set it going in the right direction. (This is exactly like the Chinese Room.) But I chose to write this article. That act of choosing is something a LLM could never do, at least with our current technology.</p>



<hr class="wp-block-separator" />



<h3>Footnotes</h3>



<ol><li>I’ve never been much impressed with the idea of <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.santafe.edu/research/projects/theory-of-embodied-intelligence" target="_blank">embodied intelligence</a>–that intelligence requires the context of a body and sensory input.&nbsp; However, my arguments here suggest that it’s on to something, in ways that I haven’t credited.&nbsp; “Sitting” is meaningless without a body. Physics is impossible without observation. Stress is a reaction that requires a body. However, Blaise Agüera y Arcas has had “conversations” with Google’s models in which they talk about a “favorite island” and claim to have a “sense of smell.”&nbsp; Is this transgression? Is it imagination? Is “embodiment” a social construct, rather than a physical one? There’s plenty of ambiguity here, and that’s is precisely why it’s important. Is transgression possible without a body?<br></li><li>I want to steer away from a “great man” theory of progress;&nbsp; as <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://bigthink.com/starts-with-a-bang/science-einstein-never-existed/" target="_blank">Ethan Siegel has argued</a> convincingly, if Einstein never lived, physicists would probably have made Einstein’s breakthroughs in relatively short order. They were on the brink, and several were thinking along the same lines. This doesn’t change my argument, though: to come up with general relativity, you have to realize that there’s something amiss with Newtonian physics, something most people consider “law,” and that mere assent isn’t a way forward. Whether we’re talking about dogs, children, or physicists, intelligence is transgressive.</li></ol>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/intelligence-and-comprehension/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>What’s ahead for AI, VR, NFTs, and more?</title>
		<link>https://www.oreilly.com/radar/whats-ahead-for-ai-vr-nfts-and-more/</link>
				<comments>https://www.oreilly.com/radar/whats-ahead-for-ai-vr-nfts-and-more/#respond</comments>
				<pubDate>Tue, 11 Jan 2022 12:22:48 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides]]></dc:creator>
				<category><![CDATA[AI & ML]]></category>
		<category><![CDATA[NFT]]></category>
		<category><![CDATA[Virtual Reality]]></category>
		<category><![CDATA[Signals]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=14188</guid>
				<description><![CDATA[Every year starts with a round of predictions for the new year, most of which end up being wrong. But why fight against tradition? Here are my predictions for 2022. The safest predictions are all around AI. We’ll see more “AI as a service” (AIaaS) products. This trend started with the gigantic language model GPT-3. [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>Every year starts with a round of predictions for the new year, most of which end up being wrong. But why fight against tradition? Here are my predictions for 2022.</p>



<p>The safest predictions are all around AI.</p>



<ul><li>We’ll see more “AI as a service” (AIaaS) products. This trend started with the gigantic language model GPT-3. It’s so large that it really can’t be run without Azure-scale computing facilities, so Microsoft has made it available as a service, accessed via a web API. This may encourage the creation of more large-scale models; it might also drive a wedge between academic and industrial researchers. What does “reproducibility” mean if the model is so large that it’s impossible to reproduce experimental results?</li><li>Prompt engineering, a field dedicated to developing prompts for language generation systems, will become a new specialization. Prompt engineers answer questions like “What do you have to say to get a model like GPT-3 to produce the output you want?”</li><li>AI-assisted programming (for example, <a href="https://copilot.github.com/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">GitHub Copilot</a>) has a long way to go, but it will make quick progress and soon become just another tool in the programmer’s toolbox. And it will change the way programmers think too: they’ll need to focus less on learning programming languages and syntax and more on understanding precisely the problem they have to solve.</li><li>GPT-3 clearly is not the end of the line. There are already language models bigger than GPT-3 (one in Chinese), and we’ll certainly see large models in other areas. We will also see research on smaller models that offer better performance, like Google’s <a href="https://deepmind.com/blog/article/language-modelling-at-scale">RETRO</a>.</li><li>Supply chains and business logistics will remain under stress. We’ll see new tools and platforms for dealing with supply chain and logistics issues, and they’ll likely make use of machine learning. We’ll also come to realize that, from the start, Amazon’s core competency has been logistics and supply chain management.</li><li>Just as we saw new professions and job classifications when the web appeared in the ’90s, we’ll see new professions and services appear as a result of AI—specifically, as a result of natural language processing. We don’t yet know what these new professions will look like or what new skills they’ll require. But they’ll almost certainly involve collaboration between humans and intelligent machines.</li><li>CIOs and CTOs will realize that any realistic cloud strategy is inherently a multi- or hybrid cloud strategy. Cloud adoption moves from the grassroots up, so by the time executives are discussing a “cloud strategy,” most organizations are already using two or more clouds. The important strategic question isn’t which cloud provider to pick; it’s how to use multiple providers effectively.</li><li>Biology is becoming like software. Inexpensive and fast genetic sequencing, together with computational techniques including AI, enabled Pfizer/BioNTech, Moderna, and others to develop effective mRNA vaccines for COVID-19 in astonishingly little time. In addition to creating vaccines that target new COVID variants, these technologies will enable developers to target diseases for which we don’t have vaccines, like AIDS. </li></ul>



<p>Now for some slightly less safe predictions, involving the future of social media and cybersecurity.</p>



<ul><li>Augmented and virtual reality aren’t new, but Mark Zuckerberg lit a fire under them by talking about the “metaverse,” changing Facebook’s name to Meta, and releasing a pair of smart glasses in collaboration with Ray-Ban. The key question is whether these companies can make AR glasses that work and don’t make you look like an alien. I don’t think they’ll succeed, but Apple is also working on VR/AR products. It’s much harder to bet against Apple’s ability to turn geeky technology into a fashion statement.</li><li>There’s also been talk from Meta, Microsoft, and others, about using virtual reality to help people who are working from home, which typically involves making meetings better. But they’re solving the wrong problem. Workers, whether at home or not, don’t want better meetings; they want fewer. If Microsoft can figure out how to use the metaverse to make meetings unnecessary, it’ll be onto something.</li><li>Will 2022 be the year that security finally gets the attention it deserves? Or will it be another year in which Russia uses the cybercrime industry to improve its foreign trade balance? Right now, things are looking better for the security industry: salaries are up, and employers are hiring. But time will tell.</li></ul>



<p>And I’ll end a very unsafe prediction.</p>



<ul><li>NFTs are currently all the rage, but they don’t fundamentally change anything. They really only provide a way for cryptocurrency millionaires to show off—conspicuous consumption at its most conspicuous. But they’re also programmable, and people haven’t yet taken advantage of this. Is it possible that there’s something fundamentally new on the horizon that can be built with NFTs? I haven’t seen it yet, but it could appear in 2022. And then we’ll all say, “Oh, that’s what NFTs were all about.”</li></ul>



<p>Or it might not. The discussion of <a href="https://www.oreilly.com/radar/why-its-too-early-to-get-excited-about-web3/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">Web 2.0 versus Web3</a> misses a crucial point. Web 2.0 wasn’t about the creation of new applications; it was what was left after the dot-com bubble burst. All bubbles burst eventually. So what will be left after the cryptocurrency bubble bursts? Will there be new kinds of value, or just hot air? We don’t know, but we may find out in the coming year.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/whats-ahead-for-ai-vr-nfts-and-more/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>MLOps and DevOps: Why Data Makes It Different</title>
		<link>https://www.oreilly.com/radar/mlops-and-devops-why-data-makes-it-different/</link>
				<comments>https://www.oreilly.com/radar/mlops-and-devops-why-data-makes-it-different/#respond</comments>
				<pubDate>Tue, 19 Oct 2021 14:17:38 +0000</pubDate>
		<dc:creator><![CDATA[Ville Tuulos and Hugo Bowne-Anderson]]></dc:creator>
				<category><![CDATA[AI & ML]]></category>
		<category><![CDATA[Deep Dive]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=14018</guid>
				<description><![CDATA[Much has been written about struggles of deploying machine learning projects to production. As with many burgeoning fields and disciplines, we don’t yet have a shared canonical infrastructure stack or best practices for developing and deploying data-intensive applications. This is both frustrating for companies that would prefer making ML an ordinary, fuss-free value-generating function like [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>Much has been written about struggles of deploying machine learning projects to production. As with many burgeoning fields and disciplines, we don’t yet have a shared canonical infrastructure stack or best practices for developing and deploying data-intensive applications. This is both frustrating for companies that would prefer making ML an ordinary, fuss-free value-generating function like software engineering, as well as exciting for vendors who see the opportunity to create buzz around a new category of enterprise software.</p>



<p>The new category is often called <em>MLOps</em>. While there isn’t an authoritative definition for the term, it shares its ethos with its predecessor, the <em>DevOps </em>movement in software engineering: by adopting well-defined processes, modern tooling, and automated workflows, we can streamline the process of moving from development to robust production deployments. This approach has worked well for software development, so it is reasonable to assume that it could address struggles related to deploying machine learning in production too.</p>



<p>However, the concept is quite abstract. Just introducing a new term like MLOps doesn’t solve anything by itself, rather, it just adds to the confusion. In this article, we want to dig deeper into the fundamentals of machine learning as an engineering discipline and outline answers to key questions:</p>



<ol><li><strong>Why</strong> does ML need special treatment in the first place? Can’t we just fold it into existing DevOps best practices?</li><li><strong>What</strong> does a modern technology stack for streamlined ML processes look like?</li><li><strong>How</strong> can you start applying the stack in practice today?</li></ol>



<h2>Why: Data Makes It Different</h2>



<p>All ML projects are software projects. If you peek under the hood of an ML-powered application, these days you will often find a repository of Python code. If you ask an engineer to show how they operate the application in production, they will likely show containers and operational dashboards—not unlike any other software service.</p>



<p>Since software engineers manage to build ordinary software without experiencing as much pain as their counterparts in the ML department, it begs the question: should we just start treating ML projects as software engineering projects as usual, maybe educating ML practitioners about the existing best practices?</p>



<p>Let’s start by considering the job of a non-ML software engineer: writing traditional software deals with well-defined, narrowly-scoped inputs, which the engineer can exhaustively and cleanly model in the code. In effect, the engineer designs and builds the world wherein the software operates.</p>



<p>In contrast, a defining feature of ML-powered applications is that they are directly exposed to a large amount of messy, real-world data which is too complex to be understood and modeled by hand.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/10/MLOps-v7_Fig1.png" alt="" class="wp-image-14022" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/10/MLOps-v7_Fig1.png 1024w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/10/MLOps-v7_Fig1-300x225.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/10/MLOps-v7_Fig1-768x576.png 768w" sizes="(max-width: 1024px) 100vw, 1024px" /></figure>



<p>This characteristic makes ML applications fundamentally different from traditional software. It has far-reaching implications as to how such applications should be developed and by whom:</p>



<ol><li><strong>ML applications are directly exposed to the constantly changing real world through data, </strong>whereas traditional software operates in a simplified, static, abstract world which is directly constructed by the developer. </li><li><strong>ML apps need to be developed through cycles of experimentation: </strong>due to the constant exposure to data, we don’t learn the behavior of ML apps through logical reasoning but through empirical observation.</li><li><strong>The skillset and the background of people building the applications gets realigned</strong>: while it is still effective to express applications in code, the emphasis shifts to data and experimentation—more akin to empirical science—rather than traditional software engineering.</li></ol>



<p>This approach is not novel. There is a decades-long tradition of <em>data-centric programming</em>: developers who have been using data-centric IDEs, such as RStudio, Matlab, Jupyter Notebooks, or even Excel to model complex real-world phenomena, should find this paradigm familiar. However, these tools have been rather insular environments: they are great for prototyping but lacking when it comes to production use.</p>



<p>To make ML applications production-ready from the beginning, developers must adhere to the same set of standards as all other production-grade software. This introduces further requirements:</p>



<ol><li><strong>The scale of operations </strong>is often two orders of magnitude larger than in the earlier data-centric environments. Not only is data larger, but models—deep learning models in particular—are much larger than before.</li><li><strong>Modern ML applications need to be carefully orchestrated: </strong>with the dramatic increase in the complexity of apps, which can require dozens of interconnected steps, developers need better software paradigms, such as first-class DAGs.</li><li><strong>We need robust versioning for data, models, code, </strong>and preferably even the internal state of applications—think Git on steroids to answer inevitable questions: What changed? Why did something break? Who did what and when? How do two iterations compare?</li><li><strong>The applications must be integrated to the surrounding business systems </strong>so ideas can be tested and validated in the real world in a controlled manner.</li></ol>



<p>Two important trends collide in these lists. On the one hand we have the long tradition of data-centric programming; on the other hand, we face the needs of modern, large-scale business applications. Either paradigm is insufficient by itself: it would be ill-advised to suggest building a modern ML application in Excel. Similarly, it would be pointless to pretend that a data-intensive application resembles a run-off-the-mill microservice which can be built with the usual software toolchain consisting of, say, GitHub, Docker, and Kubernetes.</p>



<p>We need a new path that allows the results of data-centric programming, models and data science applications in general, to be deployed to modern production infrastructure, similar to how DevOps practices allows traditional software artifacts to be deployed to production continuously and reliably. Crucially, the new path is analogous but not equal to the existing DevOps path.</p>



<figure class="wp-block-image size-large is-resized"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/10/Picture2.png" alt="" class="wp-image-14035" width="530" height="488" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/10/Picture2.png 491w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/10/Picture2-300x276.png 300w" sizes="(max-width: 530px) 100vw, 530px" /></figure>



<h2>What: The Modern Stack of ML Infrastructure</h2>



<p>What kind of foundation would the modern ML application require? It should combine the best parts of modern production infrastructure to ensure robust deployments, as well as draw inspiration from data-centric programming to maximize productivity.</p>



<p>While implementation details vary, the major infrastructural layers we’ve seen emerge are relatively uniform across a large number of projects. Let’s now take a tour of the various layers, to begin to map the territory. Along the way, we’ll provide illustrative examples. The intention behind the examples is not to be comprehensive (perhaps a fool’s errand, anyway!), but to reference concrete tooling used today in order to ground what could otherwise be a somewhat abstract exercise.</p>



<figure class="wp-block-image size-large is-resized"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/10/Picture3.png" alt="" class="wp-image-14033" width="580" height="294" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/10/Picture3.png 641w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/10/Picture3-300x152.png 300w" sizes="(max-width: 580px) 100vw, 580px" /><figcaption><br>Adapted from the book <a href="https://www.manning.com/books/effective-data-science-infrastructure" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)"><em>Effective Data Science Infrastructure</em></a></figcaption></figure>



<h3>Foundational Infrastructure Layers</h3>



<h4>Data</h4>



<p><strong>Data</strong> is at the core of any ML project, so data infrastructure is a foundational concern. ML use cases rarely dictate the master data management solution, so the ML stack needs to integrate with existing data warehouses. Cloud-based data warehouses, such as <strong>Snowflake</strong>, AWS’ portfolio of databases like <strong>RDS, Redshift </strong>or <strong>Aurora</strong>, or an <strong>S3-based data lake</strong>, are a great match to ML use cases since they tend to be much more scalable than traditional databases, both in terms of the data set sizes as well as query patterns.</p>



<h4>Compute</h4>



<p>To make data useful, we must be able to conduct large-scale <strong>compute</strong> easily. Since the needs of data-intensive applications are diverse, it is useful to have a general-purpose compute layer that can handle different types of tasks from IO-heavy data processing to training large models on GPUs. Besides variety, the number of tasks can be high too: imagine a single workflow that trains a separate model for 200 countries in the world, running a hyperparameter search over 100 parameters for each model—the workflow yields 20,000 parallel tasks.</p>



<p>Prior to the cloud, setting up and operating a cluster that can handle workloads like this would have been a major technical challenge. Today, a number of cloud-based, auto-scaling systems are easily available, such as <strong>AWS Batch. Kubernetes,</strong> a popular choice for general-purpose container orchestration, can be configured to work as a scalable batch compute layer, although the downside of its flexibility is increased complexity. Note that container orchestration for the compute layer is not to be confused with the workflow orchestration layer, which we will cover next.</p>



<h4>Orchestration</h4>



<p>The nature of computation is structured: we must be able to manage the complexity of applications by structuring them, for example, as a graph or a workflow that is <strong>orchestrated</strong>.</p>



<figure class="wp-block-image size-large is-resized"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/10/Picture4.png" alt="" class="wp-image-14029" width="534" height="134" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/10/Picture4.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/10/Picture4-300x75.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/10/Picture4-768x193.png 768w" sizes="(max-width: 534px) 100vw, 534px" /></figure>



<p>The workflow orchestrator needs to perform a seemingly simple task: given a workflow or DAG definition, execute the tasks defined by the graph in order using the compute layer. There are countless systems that can perform this task for small DAGs on a single server. However, as the workflow orchestrator plays a key role in ensuring that production workflows execute reliably, it makes sense to use a system that is both scalable and highly available, which leaves us with a few battle-hardened options, for instance: <a href="https://airflow.apache.org/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)"><strong>Airflow</strong></a>, a popular open-source workflow orchestrator; <a href="https://argoproj.github.io/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)"><strong>Argo</strong></a><strong>, </strong>a newer orchestrator that runs natively on Kubernetes, and managed solutions such as <a href="https://cloud.google.com/composer" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)"><strong>Google Cloud Composer</strong></a> and <a href="https://aws.amazon.com/step-functions/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)"><strong>AWS Step Functions</strong></a>.</p>



<h3>Software Development Layers</h3>



<p>While these three foundational layers, data, compute, and orchestration, are technically all we need to execute ML applications at arbitrary scale, building and operating ML applications directly on top of these components would be like hacking software in assembly language: technically possible but inconvenient and unproductive. To make people productive, we need higher levels of abstraction. Enter the software development layers.</p>



<h4>Versioning</h4>



<p>ML app and software artifacts exist and evolve in a dynamic environment. To manage the dynamism, we can resort to taking snapshots that represent immutable points in time: of models, of data, of code, and of internal state. For this reason, we require <strong>a strong versioning layer</strong>.</p>



<p>While <strong>Git</strong>, <strong>GitHub, </strong>and other similar tools for software version control work well for code and the usual workflows of software development, they are a bit clunky for tracking all experiments, models, and data. To plug this gap, frameworks like <a href="https://docs.metaflow.org/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)"><strong>Metaflow</strong></a> or <a href="https://mlflow.org/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)"><strong>MLFlow</strong></a> provide a custom solution for versioning.</p>



<h4>Software Architecture</h4>



<p>Next, we need to consider who builds these applications and how. They are often built by data scientists who are not software engineers or computer science majors by training. Arguably, high-level programming languages like Python are the most expressive and efficient ways that humankind has conceived to formally define complex processes. It is hard to imagine a better way to express non-trivial business logic and convert mathematical concepts into an executable form.</p>



<p>However, not all Python code is equal. Python written in Jupyter notebooks following the tradition of data-centric programming is very different from Python used to implement a scalable web server. To make the data scientists maximally productive, we want to provide supporting <strong>software architecture </strong>in terms of APIs and libraries that allow them to focus on data, not on the machines.</p>



<h4>Data Science Layers</h4>



<p>With these five layers, we can present a highly productive, data-centric software interface that enables iterative development of large-scale data-intensive applications. However, none of these layers help with modeling and optimization. We cannot expect data scientists to write modeling frameworks like PyTorch or optimizers like Adam from scratch! Furthermore, there are steps that are needed to go from raw data to features required by models.</p>



<h4>Model Operations</h4>



<p>When it comes to data science and modeling, we separate three concerns, starting from the most practical progressing towards the most theoretical. Assuming you have a model, how can you use it effectively? Perhaps you want to produce predictions in real-time or as a batch process. No matter what you do, you should monitor the quality of the results. Altogether, we can group these practical concerns in the <strong>model operations</strong> layer. There are many new tools in this space helping with various aspects of operations, including <a href="https://www.seldon.io/tech/products/deploy/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)"><strong>Seldon</strong></a> for model deployments, <a href="https://wandb.ai/site" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)"><strong>Weights and Biases</strong></a> for model monitoring, and <a href="https://truera.com/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)"><strong>TruEra</strong></a> for model explainability.</p>



<h4>Feature Engineering</h4>



<p>Before you have a model, you have to decide how to feed it with labelled data. Managing the process of converting raw facts to features is a deep topic of its own, potentially involving feature encoders, feature stores, and so on. Producing labels is another, equally deep topic. You want to carefully manage consistency of data between training and predictions, as well as make sure that there’s no leakage of information when models are being trained and tested with historical data. We bucket these questions in the <strong>feature engineering</strong> layer. There’s an emerging space of ML-focused feature stores such as <a href="https://www.tecton.ai/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)"><strong>Tecton</strong></a> or labeling solutions like <a href="https://scale.com/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)"><strong>Scale</strong></a> and <a href="https://snorkel.ai/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)"><strong>Snorkel</strong></a>. Feature stores aim to solve the challenge that many data scientists in an organization require similar data transformations and features for their work and labeling solutions deal with <a href="https://www.oreilly.com/radar/arguments-against-hand-labeling/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">the very real challenges associated with hand labeling datasets</a>.</p>



<h4>Model Development</h4>



<p>Finally, at the very top of the stack we get to the question of mathematical modeling: What kind of modeling technique to use? What model architecture is most suitable for the task? How to parameterize the model? Fortunately, excellent off-the-shelf libraries like <a href="https://scikit-learn.org/stable/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)"><strong>scikit-learn</strong></a> and <a href="https://pytorch.org/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)"><strong>PyTorch</strong></a> are available to help with <strong>model development</strong>.</p>



<h3>An Overarching Concern: Correctness and Testing</h3>



<p>Regardless of the systems we use at each layer of the stack, we want to guarantee the correctness of results. In traditional software engineering we can do this by writing tests: for instance, a unit test can be used to check the behavior of a function with predetermined inputs. Since we know exactly how the function is implemented, we can convince ourselves through inductive reasoning that the function should work correctly, based on the correctness of a unit test.</p>



<p>This process doesn’t work when the function, such as a model, is opaque to us. We must resort to <em>black box testing</em>—testing the behavior of the function with a wide range of inputs. Even worse, sophisticated ML applications can take a huge number of contextual data points as inputs, like the time of day, user’s past behavior, or device type into account, so an accurate test set up may need to become a full-fledged simulator.</p>



<p>Since building an accurate simulator is a highly non-trivial challenge in itself, often it is easier to use a slice of the real-world as a simulator and A/B test the application in production against a known baseline. To make A/B testing possible, all layers of the stack should be be able to run many versions of the application concurrently, so an arbitrary number of production-like deployments can be run simultaneously. This poses a challenge to many infrastructure tools of today, which have been designed for more rigid traditional software in mind. Besides infrastructure, effective A/B testing requires a control plane, a modern experimentation platform, such as <a href="https://statsig.com/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">StatSig</a>.</p>



<h2>How: Wrapping The Stack For Maximum Usability</h2>



<p>Imagine choosing a production-grade solution for each layer of the stack: for instance, Snowflake for data, Kubernetes for compute (container orchestration), and Argo for workflow orchestration. While each system does a good job at its own domain, it is not trivial to build a data-intensive application that has cross-cutting concerns touching all the foundational layers. In addition, you have to layer the higher-level concerns from versioning to model development on top of the already complex stack. It is not realistic to ask a data scientist to prototype quickly and deploy to production with confidence using such a contraption. Adding more YAML to cover cracks in the stack is not an adequate solution.</p>



<p>Many data-centric environments of the previous generation, such as Excel and RStudio, really shine at maximizing usability and developer productivity. Optimally, we could wrap the production-grade infrastructure stack inside a developer-oriented user interface. Such an interface should allow the data scientist to focus on concerns that are most relevant for them, namely the topmost layers of stack, while abstracting away the foundational layers.</p>



<p>The combination of a production-grade core and a user-friendly shell makes sure that ML applications can be prototyped rapidly, deployed to production, and brought back to the prototyping environment for continuous improvement. The iteration cycles should be measured in hours or days, not in months.</p>



<figure class="wp-block-image size-large is-resized"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/10/Picture5.png" alt="" class="wp-image-14037" width="549" height="258" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/10/Picture5.png 808w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/10/Picture5-300x141.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/10/Picture5-768x362.png 768w" sizes="(max-width: 549px) 100vw, 549px" /></figure>



<p>Over the past five years, a number of such frameworks have started to emerge, both as commercial offerings as well as in open-source.</p>



<p><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://docs.metaflow.org/" target="_blank"><strong>Metaflow</strong></a> is an open-source framework, originally developed at Netflix, specifically designed to address this concern (disclaimer: <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://outerbounds.co/" target="_blank">one of the authors works on Metaflow</a>): How can we wrap robust production infrastructure in a single coherent, easy-to-use interface for data scientists? Under the hood, Metaflow integrates with best-of-the-breed production infrastructure, such as Kubernetes and AWS Step Functions, while providing a development experience that draws inspiration from data-centric programming, that is, by treating local prototyping as the first-class citizen.</p>



<p>Google’s open-source <a href="https://www.kubeflow.org/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)"><strong>Kubeflow</strong></a> addresses similar concerns, although with a more engineer-oriented approach. As a commercial product, <a href="https://databricks.com/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)"><strong>Databricks</strong></a> provides a managed environment that combines data-centric notebooks with a proprietary production infrastructure. All cloud providers provide commercial solutions as well, such as <a href="https://aws.amazon.com/sagemaker/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)"><strong>AWS Sagemaker</strong></a> or <a href="https://studio.azureml.net/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)"><strong>Azure ML Studio</strong></a>.</p>



<p>While these solutions, and many less known ones, seem similar on the surface, there are many differences between them. When evaluating solutions, consider focusing on the three key dimensions covered in this article:</p>



<ol><li><strong>Does the solution provide a delightful user experience for data scientists and ML engineers?</strong> There is no fundamental reason why data scientists should accept a worse level of productivity than is achievable with existing data-centric tools.</li><li><strong>Does the solution provide first-class support for rapid iterative development and frictionless A/B testing?</strong> It should be easy to take projects quickly from prototype to production and back, so production issues can be reproduced and debugged locally.</li><li><strong>Does the solution integrate with your existing infrastructure, in particular to the foundational data, compute, and orchestration layers?</strong> It is not productive to operate ML as an island. When it comes to operating ML in production, it is beneficial to be able to leverage existing production tooling for observability and deployments, for example, as much as possible.</li></ol>



<p>It is safe to say that all existing solutions still have room for improvement. Yet it seems inevitable that over the next five years the whole stack will mature, and the user experience will converge towards and eventually beyond the best data-centric IDEs.&nbsp; Businesses will learn how to create value with ML similar to traditional software engineering and empirical, data-driven development will take its place amongst other ubiquitous software development paradigms.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/mlops-and-devops-why-data-makes-it-different/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>The Quality of Auto-Generated Code</title>
		<link>https://www.oreilly.com/radar/the-quality-of-auto-generated-code/</link>
				<comments>https://www.oreilly.com/radar/the-quality-of-auto-generated-code/#respond</comments>
				<pubDate>Tue, 12 Oct 2021 13:45:10 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides and Kevlin Henney]]></dc:creator>
				<category><![CDATA[AI & ML]]></category>
		<category><![CDATA[Commentary]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=14007</guid>
				<description><![CDATA[Kevlin Henney and I were riffing on some ideas about GitHub Copilot, the tool for automatically generating code base on GPT-3&#8217;s language model, trained on the body of code that&#8217;s in GitHub. This article poses some questions and (perhaps) some answers, without trying to present any conclusions. First, we wondered about code quality. There are [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>Kevlin Henney and I were riffing on some ideas about <a href="https://copilot.github.com/">GitHub Copilot</a>, the tool for automatically generating code base on GPT-3&#8217;s language model, trained on the body of code that&#8217;s in GitHub. This article poses some questions and (perhaps) some answers, without trying to present any conclusions.</p>



<p>First, we wondered about code quality. There are lots of ways to solve a given programming problem; but most of us have some ideas about what makes code &#8220;good&#8221; or &#8220;bad.&#8221; Is it readable, is it well-organized? Things like that.&nbsp; In a professional setting, where software needs to be maintained and modified over long periods, readability and organization count for a lot.</p>



<p>We know how to test whether or not code is correct (at least up to a certain limit). Given enough unit tests and acceptance tests, we can imagine a system for automatically generating code that is correct. <a rel="noreferrer noopener" aria-label="Property (opens in a new tab)" href="https://increment.com/testing/in-praise-of-property-based-testing/" target="_blank">Property</a><a href="https://increment.com/testing/in-praise-of-property-based-testing/">-based testing</a> might give us some additional ideas about building test suites robust enough to verify that code works properly. But we don&#8217;t have methods to test for code that&#8217;s &#8220;good.&#8221; Imagine asking Copilot to write a function that sorts a list. There are lots of ways to sort. Some are pretty good—for example, <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Quicksort" target="_blank">quicksort</a>. Some of them are awful. But a unit test has no way of telling whether a function is implemented using quicksort, <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://kevlinhenney.medium.com/a-sort-of-permutation-768c1a7e029b" target="_blank">permutation sort</a>, (which completes in factorial time), <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://kevlinhenney.medium.com/need-something-sorted-sleep-on-it-11fdf8453914" target="_blank">sleep sort</a>, or one of the other strange sorting algorithms that Kevlin has been writing about.</p>



<p>Do we care? Well, we care about O(N log N) behavior versus O(N!). But assuming that we have some way to resolve that issue, if we can specify a program&#8217;s behavior precisely enough so that we are highly confident that Copilot will write code that&#8217;s correct and tolerably performant, do we care about its aesthetics? Do we care whether it&#8217;s readable? 40 years ago, we might have cared about the assembly language code generated by a compiler. But today, we don&#8217;t, except for a few increasingly rare corner cases that usually involve device drivers or embedded systems. If I write something in C and compile it with gcc, realistically I&#8217;m never going to look at the compiler&#8217;s output. I don&#8217;t need to understand it.</p>



<p>To get to this point, we may need a meta-language for describing what we want the program to do that&#8217;s almost as detailed as a modern high-level language. That could be what the future holds: an understanding of &#8220;prompt engineering&#8221; that lets us tell an AI system precisely what we want a program to do, rather than how to do it. Testing would become much more important, as would understanding precisely the business problem that needs to be solved. “Slinging code” in whatever the language would become less common.</p>



<p>But what if we don&#8217;t get to the point where we trust automatically generated code as much as we now trust the output of a compiler? Readability will be at a premium as long as humans need to read code. If we have to read the output from one of Copilot&#8217;s descendants to judge whether or not it will work, or if we have to debug that output because it mostly works, but fails in some cases, then we will need it to generate code that&#8217;s readable. Not that humans currently do a good job of writing readable code; but we all know how painful it is to debug code that isn’t readable, and we all have some concept of what “readability” means.</p>



<p>Second: Copilot was trained on the body of code in GitHub. At this point, it is all (or almost all) written by humans. Some of it is good, high quality, readable code; a lot of it isn&#8217;t. What if Copilot became so successful that Copilot-generated code came to constitute a significant percentage of the code on GitHub? The model will certainly need to be re-trained from time to time. So now, we have a feedback loop: Copilot trained on code that has been (at least partially) generated by Copilot. Does code quality improve? Or does it degrade? And again, do we care, and why?</p>



<p>This question can be argued either way. People working on automated tagging for AI seem to be taking the position that iterative tagging leads to better results: i.e., after a tagging pass, use a human-in-the-loop to check some of the tags, correct them where wrong, and then use this additional input in another training pass. Repeat as needed. That&#8217;s not all that different from current (non-automated) programming: write, compile, run, debug, as often as needed to get something that works. The feedback loop enables you to write good code.</p>



<p>A human-in-the-loop approach to training an AI code generator is one possible way of getting &#8220;good code&#8221; (for whatever &#8220;good&#8221; means)—though it&#8217;s only a partial solution. Issues like indentation style, meaningful variable names, and the like are only a start. Evaluating whether a body of code is structured into coherent modules, has well-designed APIs, and could easily be understood by maintainers is a more difficult problem. Humans can evaluate code with these qualities in mind, but it takes time. A human-in-the-loop might help to train AI systems to design good APIs, but at some point, the &#8220;human&#8221; part of the loop will start to dominate the rest.</p>



<p>If you look at this problem from the standpoint of evolution, you see something different. If you breed plants or animals (a highly selected form of evolution) for one desired quality, you will almost certainly see all the other qualities degrade: you&#8217;ll get <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.akc.org/expert-advice/health/hip-dysplasia-in-dogs/" target="_blank">large dogs with hips that don&#8217;t work</a>, or <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.puppyleaks.com/done-bulldogs/" target="_blank">dogs with flat faces that can&#8217;t breathe properly</a>.</p>



<p>What direction will automatically generated code take? We don&#8217;t know. Our guess is that, without ways to measure &#8220;code quality&#8221; rigorously, code quality will probably degrade. Ever since Peter Drucker, management consultants have liked to say, &#8220;If you can&#8217;t measure it, you can&#8217;t improve it.&#8221; And we suspect that applies to code generation, too: aspects of the code that can be measured will improve, aspects that can&#8217;t won&#8217;t.&nbsp; Or, as the accounting historian <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/H._Thomas_Johnson" target="_blank">H. Thomas Johnson</a> said, “Perhaps what you measure is what you get. More likely, what you measure is all you’ll get. What you don’t (or can’t) measure is lost.&#8221;</p>



<p>We can write tools to measure some superficial aspects of code quality, like obeying stylistic conventions. We already have tools that can &#8220;fix&#8221; fairly superficial quality problems like indentation. But again, that superficial approach doesn&#8217;t touch the more difficult parts of the problem. If we had an algorithm that could score readability, and restrict Copilot&#8217;s training set to code that scores in the 90th percentile, we would certainly see output that looks better than most human code. Even with such an algorithm, though, it&#8217;s still unclear whether that algorithm could determine whether variables and functions had appropriate names, let alone whether a large project was well-structured.</p>



<p>And a third time: do we care? If we have a rigorous way to express what we want a program to do, we may never need to look at the underlying C or C++. At some point, one of Copilot&#8217;s descendants may not need to generate code in a &#8220;high level language&#8221; at all: perhaps it will generate machine code for your target machine directly. And perhaps that target machine will be <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://webassembly.org/" target="_blank">Web Assembly</a>, the JVM, or something else that&#8217;s very highly portable.</p>



<p>Do we care whether tools like Copilot write good code? We will, until we don&#8217;t. Readability will be important as long as humans have a part to play in the debugging loop. The important question probably isn’t “do we care”; it’s “when will we stop caring?” When we can trust the output of a code model, we’ll see a rapid phase change.&nbsp; We’ll care less about the code, and more about describing the task (and appropriate tests for that task) correctly.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/the-quality-of-auto-generated-code/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>2021 Data/AI Salary Survey</title>
		<link>https://www.oreilly.com/radar/2021-data-ai-salary-survey/</link>
				<comments>https://www.oreilly.com/radar/2021-data-ai-salary-survey/#respond</comments>
				<pubDate>Wed, 15 Sep 2021 11:32:26 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides]]></dc:creator>
				<category><![CDATA[AI & ML]]></category>
		<category><![CDATA[Data]]></category>
		<category><![CDATA[Research]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=13950</guid>
				<description><![CDATA[In June 2021, we asked the recipients of our&#160;Data &#38; AI Newsletter&#160;to respond to a survey about compensation. The results gave us insight into what our subscribers are paid, where they’re located, what industries they work for, what their concerns are, and what sorts of career development opportunities they’re pursuing. While it’s sadly premature to [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>In June 2021, we asked the recipients of our&nbsp;<em>Data &amp; AI Newsletter</em>&nbsp;to respond to a survey about compensation. The results gave us insight into what our subscribers are paid, where they’re located, what industries they work for, what their concerns are, and what sorts of career development opportunities they’re pursuing.</p>



<p>While it’s sadly premature to say that the survey took place at the end of the COVID-19 pandemic (though we can all hope), it took place at a time when restrictions were loosening: we were starting to go out in public, have parties, and in some cases even attend in-person conferences. The results then provide a place to start thinking about what effect the pandemic had on employment. There was a lot of uncertainty about stability, particularly at smaller companies: Would the company’s business model continue to be effective? Would your job still be there in a year? At the same time, employees were reluctant to look for new jobs, especially if they would require relocating—at least according to the rumor mill. Were those concerns reflected in new patterns for employment?</p>



<h2>Executive Summary</h2>



<ul><li>The average salary for data and AI professionals who responded to the survey was $146,000.</li><li>The average change in compensation over the last three years was $9,252. This corresponds to an annual increase of 2.25%. However, 8% of the correspondents reported decreased compensation, and 18% reported no change.</li><li>We don’t see evidence of a “great resignation.” 22% of respondents said they intended to change jobs, roughly what we would have expected. Respondents seemed concerned about job security, probably because of the pandemic’s effect on the economy.</li><li>Average compensation was highest in California ($176,000), followed by Eastern Seaboard states like New York and&nbsp;Massachusetts.</li><li>Compensation for women was significantly lower than for men (84%). Salaries were lower regardless of education or job title. Women were more likely than men to have advanced degrees, particularly PhDs.</li><li>Many respondents acquired certifications. Cloud certifications, specifically in AWS and Microsoft Azure, were most strongly associated with salary increases.</li><li>Most respondents participated in training of some form. Learning new skills and improving old ones were the most common reasons for training, though hireability and job security were also factors. Company-provided training opportunities were most strongly associated with pay increases.</li></ul>



<h2>Demographics</h2>



<p>The survey was publicized through&nbsp;<a href="https://www.oreilly.com/emails/newsletters/">O’Reilly’s&nbsp;<em>Data &amp; AI Newsletter</em></a>&nbsp;and was limited to respondents in the United States and the United Kingdom. There were 3,136 valid responses, 2,778 from the US and 284 from the UK. This report focuses on the respondents from the US, with only limited attention paid to those from the UK. A small number of respondents (74) identified as residents of the US or UK, but their IP addresses indicated that they were located elsewhere. We didn&#8217;t use the data from these respondents; in practice, discarding this data had no effect on the results.</p>



<p>Of the 2,778 US respondents, 2,225 (81%) identified as men, and 383 (14%) identified as women (as identified by their preferred pronouns). 113 (4%) identified as “other,” and 14 (0.5%) used “they.”</p>



<p>The results are biased by the survey’s recipients (subscribers to O’Reilly’s&nbsp;<em>Data &amp; AI Newsletter</em>). Our audience is particularly strong in the software (20% of respondents), computer hardware (4%), and computer security (2%) industries—over 25% of the total. Our&nbsp;audience&nbsp;is also strong in the states where these industries are&nbsp;concentrated: 42% of the US respondents lived in California (20%), New York (9%), Massachusetts (6%), and Texas (7%), though these states only make up 27% of the US population.</p>



<h2>Compensation Basics</h2>



<p>The average annual salary for employees who worked in data or AI was $146,000. Most salaries were between $100,000 and $150,000 yearly (34%); the next most common salary tier was from $150,000 to $200,000 (26%). Compensation depended strongly on location, with average salaries highest in California ($176,000).</p>



<p>The average salary change over the past three years was $9,252, which is 2.25% per year (assuming a final salary equal to the average). A small number of respondents (8%) reported salary decreases, and 18% reported no change. Economic uncertainty caused by the pandemic may be responsible for the declines in compensation. 19% reported increases of $5,000 to $10,000 over that period; 14% reported increases of over $25,000. A&nbsp;<a href="https://spectrum.ieee.org/view-from-the-valley/at-work/tech-careers/us-tech-salaries-climb-says-2021-report">study by the IEEE</a>&nbsp;suggests that the average salary for technical employees increased 3.6% per year, higher than our respondents indicated.</p>



<p>39% of respondents reported promotions in the past three years, and 37% reported changing employers during that period. 22% reported that they were considering changing jobs because their salaries hadn’t increased during the past year. Is this a sign of what some have called a “great resignation”? Common wisdom has it that technical employees change jobs every three to four years.&nbsp;<a href="https://www.linkedin.com/pulse/how-often-should-you-change-jobs-perminus-wainaina/">LinkedIn</a>&nbsp;and&nbsp;<a href="https://www.indeed.com/career-advice/career-development/how-often-should-you-change-job">Indeed</a>&nbsp;both recommend staying for at least three years, though they observe that younger employees change jobs more often. LinkedIn elsewhere states that the&nbsp;<a href="https://www.linkedin.com/business/learning/blog/learner-engagement/see-the-industries-with-the-highest-turnover-and-why-it-s-so-hi">annual turnover rate</a>&nbsp;for technology employees is 13.2%—which suggests that employees stay at their jobs for roughly seven and a half years. If that’s correct, the 37% that changed jobs over three years seems about right, and the 22% who said they “intend to leave their job due to a lack of compensation increase” doesn’t seem overly high. Keep in mind that intent to change and actual change are not the same—and that there are many reasons to change jobs aside from salary, including flexibility around working hours and working from home.</p>



<p>64% of the respondents took part in training or obtained certifications in the past year, and 31% reported spending over 100 hours in training programs, ranging from formal graduate degrees to reading blog posts. As we’ll see later, cloud certifications (specifically in AWS and Microsoft Azure) were the most popular and appeared to have the largest effect on salaries.</p>



<p>The reasons respondents gave for participating in training were surprisingly consistent. The vast majority reported that they wanted to learn new skills (91%) or improve existing skills (84%). Data and AI professionals are clearly interested in learning—and that learning is self-motivated, not imposed by management. Relatively few (22%) said that training was required by their job, and even fewer participated in training because they were concerned about losing their&nbsp;job (9%).</p>



<p>However, there were other motives at work. 56% of our respondents said that they wanted to increase their “job security,” which is at odds with the low number who were concerned about losing their job. And 73% reported that they engaged in training or obtained certifications to increase their “hireability,” which may suggest more concern about job stability than our respondents would admit. The pandemic was a threat to many businesses, and employees were justifiably concerned that their job could vanish after a bad pandemic-influenced quarter. A desire for increased hireability may also indicate that we’ll see more people looking to change jobs in the near future.</p>



<p>Finally, 61% of the respondents said that they participated in training or earned certifications because they wanted a salary increase or a promotion (“increase in job title/responsibilities”). It isn’t surprising that employees see training as a route to promotion—especially as companies that want to hire in fields like data science, machine learning, and AI contend with a&nbsp;<a href="https://www.oreilly.com/radar/ai-adoption-in-the-enterprise-2021/">shortage of qualified employees</a>. Given the difficulty of hiring expertise from outside, we expect an increasing number of companies to grow their own ML and AI talent internally using training programs.</p>



<h2>Salaries by Gender</h2>



<p>To nobody’s surprise, our survey showed that data science and AI professionals are mostly male. The number of respondents tells the story by itself: only 14% identified as women, which is lower than we’d have guessed, though it’s roughly consistent with our conference attendance (back when we had live conferences) and roughly equivalent to other technical fields. A small number (5%) reported their preferred pronoun as “they” or Other, but this sample was too small to draw any significant comparisons about compensation.</p>



<p>Women’s salaries were sharply lower than men’s salaries, averaging $126,000 annually, or 84% of the average salary for men ($150,000). That differential held regardless of education, as&nbsp;Figure 1&nbsp;shows: the average salary for a woman with a doctorate or master’s degree was 82% of the salary for a man with an equivalent degree. The difference wasn’t quite as high for people with bachelor’s degrees or who were still students, but it was still significant: women with bachelor’s degrees or who were students earned 86% or 87% of the average salary for men. The difference in salaries was greatest between people who were self-taught: in that case, women’s salaries were 72% of men’s. An associate’s degree was the only degree for which women’s salaries were higher than men’s.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/01-1048x531.jpg" alt="" class="wp-image-13955" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/01-1048x531.jpg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/01-300x152.jpg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/01-768x389.jpg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/01-1536x779.jpg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/01-2048x1038.jpg 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Figure 1. Women’s and men’s salaries by degree</em></figcaption></figure>



<p>Despite the salary differential, a higher percentage of women had advanced degrees than men: 16% of women had a doctorate, as opposed to 13% of men. And 47% of women had a master’s degree, as opposed to 46% of men. (If those percentages seem high, keep in mind that many professionals in data science and AI are escapees from academia.)</p>



<p>Women’s salaries also lagged men’s salaries when we compared women and men with similar job titles (see&nbsp;Figure 2). At the executive level, the average salary for women was $163,000 versus $205,000 for men (a 20% difference). At the director level, the difference was much smaller—$180,000 for women versus $184,000 for men—and women’s salaries were actually higher than those at the executive level. It’s easy to hypothesize about this difference, but we’re at a loss to explain it. For managers, women’s salaries were $143,000 versus $154,000 for men (a 7% difference).</p>



<p>Career advancement is also an issue: 18% of the women who participated in the survey were executives or directors, compared with 23% of the men.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/02-1048x488.jpg" alt="" class="wp-image-13956" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/02-1048x488.jpg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/02-300x140.jpg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/02-768x358.jpg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/02-1536x715.jpg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/02-2048x954.jpg 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Figure 2. Women’s and men’s salaries by job title</em></figcaption></figure>



<p>Before moving on from our consideration of the effect of gender on salary, let’s take a brief look at how salaries changed over the past three years. As&nbsp;Figure 3&nbsp;shows, the percentage of men and women respondents who saw no change was virtually identical (18%). But more women than men saw their salaries decrease (10% versus 7%). Correspondingly, more men saw their salaries increase. Women were also more likely to have a smaller increase: 24% of women had an increase of under $5,000 versus 17% of men. At the high end of the salary spectrum, the difference between men and women was smaller, though still not zero: 19% of men saw their salaries increase by over $20,000, but only 18% of women did. So the most significant differences were in the midrange. One anomaly sticks out: a slightly higher percentage of women than men received salary increases in the $15,000 to $20,000 range (8% versus 6%).</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/03-1048x618.jpg" alt="" class="wp-image-13957" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/03-1048x618.jpg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/03-300x177.jpg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/03-768x453.jpg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/03-1536x906.jpg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/03-2048x1207.jpg 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Figure 3. Change in salary for women and men over three years</em></figcaption></figure>



<h2>Salaries by Programming Language</h2>



<p>When we looked at the most popular programming languages for data and AI practitioners, we didn’t see any surprises: Python was dominant (61%), followed by SQL (54%), JavaScript (32%), HTML (29%), Bash (29%), Java (24%), and R (20%). C++, C#, and C were further back in the list (12%, 12%, and 11%, respectively).</p>



<p>Discussing the connection between programming languages and salary is tricky because respondents were allowed to check multiple languages, and most did. But when we looked at the languages associated with the highest salaries, we got a significantly different list. The most widely used and popular languages, like Python ($150,000), SQL ($144,000), Java ($155,000), and JavaScript ($146,000), were solidly in the middle of the salary range. The outliers were Rust, which had the highest average salary (over $180,000), Go ($179,000), and Scala ($178,000). Other less common languages associated with high salaries were Erlang, Julia, Swift, and F#. Web languages (HTML, PHP, and CSS) were at the bottom (all around $135,000). See&nbsp;Figure 4&nbsp;for the full list.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/04-774x1048.jpg" alt="" class="wp-image-13958" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/04-774x1048.jpg 774w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/04-222x300.jpg 222w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/04-768x1040.jpg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/04-1134x1536.jpg 1134w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/04-1512x2048.jpg 1512w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/04-scaled.jpg 1890w" sizes="(max-width: 774px) 100vw, 774px" /><figcaption><em>Figure 4. Salary vs. programming language</em></figcaption></figure>



<p>How do we explain this? It’s difficult to say that data and AI developers who use Rust command a higher salary, since most respondents checked several languages. But we believe that this data shows something significant. The supply of talent for newer languages like Rust and Go is relatively small. While there may not be a huge demand for data scientists who use these languages (yet), there’s clearly some demand—and with experienced Go and Rust programmers in short supply, they command a higher salary. Perhaps it is even simpler: regardless of the language someone will use at work, employers interpret knowledge of Rust and Go as a sign of competence and willingness to learn, which increases candidates’ value. A similar argument can be made for Scala, which is the native language for the widely used Spark platform. Languages like Python and SQL are table stakes: an applicant who can’t use them could easily be penalized, but competence doesn’t confer any special distinction.</p>



<p>One surprise is that 10% of the respondents said that they didn’t use any programming languages. We’re not sure what that means. It’s possible they worked entirely in Excel, which should be considered a programming language but often isn’t. It’s also possible that they were managers or executives who no longer did any programming.</p>



<h2>Salaries by Tool and Platform</h2>



<p>We also asked respondents what tools they used for statistics and machine learning and what platforms they used for data analytics and data management. We observed some of the same patterns that we saw with programming languages. And the same caution applies: respondents were allowed to select multiple answers to our questions about the tools and platforms that they use. (However, multiple answers weren’t as frequent as for programming languages.) In addition, if you’re familiar with tools and platforms for machine learning and statistics, you know that the boundary between them is fuzzy. Is Spark a tool or a platform? We considered it a platform, though two Spark libraries are in the list of tools. What about Kafka? A platform, clearly, but a platform for building data pipelines that’s qualitatively different from a platform like Ray, Spark, or Hadoop.</p>



<p>Just as with programming languages, we found that the most widely used tools and platforms were associated with midrange salaries; older tools, even if they’re still widely used, were associated with lower salaries; and some of the tools and platforms with the fewest users corresponded to the highest salaries. (See&nbsp;Figure 5&nbsp;for the full list.)</p>



<p>The most common responses to the question about tools for machine learning or statistics were “I don’t use any tools” (40%) or Excel (31%). Ignoring the question of how one does machine learning or statistics without tools, we’ll only note that those who didn’t use tools had an average salary of $143,000, and Excel users had an average salary of $138,000—both below average. Stata ($120,000) was also at the bottom of the list; it’s an older package with relatively few users and is clearly falling out of favor.</p>



<p>The popular machine learning packages PyTorch (19% of users, $166,000 average salary), TensorFlow (20%, $164,000), and scikit-learn (27%, $157,000) occupied the middle ground. Those salaries were above the average for all respondents, which was pulled down by the large numbers who didn’t use tools or only used Excel. The highest salaries were associated with H2O (3%, $183,000), KNIME (2%, $180,000), Spark NLP (5%, $179,000), and Spark MLlib (8%, $175,000). It’s hard to trust conclusions based on 2% or 3% of the respondents, but it appears that salaries are higher for people who work with tools that have a lot of “buzz” but aren’t yet widely used. Employers pay a premium for specialized expertise.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/05-1048x934.jpg" alt="" class="wp-image-13959" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/05-1048x934.jpg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/05-300x267.jpg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/05-768x684.jpg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/05-1536x1369.jpg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/05-2048x1825.jpg 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Figure 5. Average salary by tools for statistics or machine learning</em></figcaption></figure>



<p>We see almost exactly the same thing when we look at data frameworks (Figure 6). Again, the most common response was from people who didn’t use a framework; that group also received the lowest salaries (30% of users, $133,000 average salary).</p>



<p>In 2021, Hadoop often seems like legacy software, but 15% of the respondents were working on the Hadoop platform, with an average salary of $166,000. That was above the average salary for all users and at the low end of the midrange for salaries sorted by platform.</p>



<p>The highest salaries were associated with Clicktale (now&nbsp;ContentSquare), a cloud-based analytics system for researching customer experience: only 0.2% of respondents use it, but they have an average salary of $225,000. Other frameworks associated with high salaries were Tecton (the commercial version of Michelangelo, at $218,000), Ray ($191,000), and Amundsen ($189,000). These frameworks had relatively few users—the most widely used in this group was Amundsen with 0.8% of respondents (and again, we caution against reading too much into results based on so few respondents). All of these platforms are relatively new, frequently discussed in the tech press and social media, and appear to be growing healthily. Kafka, Spark, Google BigQuery, and Dask were in the middle, with a lot of users (15%, 19%, 8%, and 5%) and above-average salaries ($179,000, $172,000, $170,000, and $170,000). Again, the most popular platforms occupied the middle of the range; experience with less frequently used and growing platforms commanded a premium.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/06-1048x782.jpg" alt="" class="wp-image-13960" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/06-1048x782.jpg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/06-300x224.jpg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/06-768x573.jpg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/06-1536x1146.jpg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/06-2048x1528.jpg 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Figure 6. Average salary by data framework or platform</em></figcaption></figure>



<h2>Salaries by Industry</h2>



<p>The greatest number of respondents worked in the software industry (20% of the total), followed by consulting (11%) and healthcare, banking, and education (each at 8%). Relatively few respondents listed themselves as consultants (also 2%), though consultancy tends to be cyclic, depending on current thinking on outsourcing, tax law, and other factors. The average income for consultants was $150,000, which is only slightly higher than the average for all respondents ($146,000). That may indicate that we’re currently in some kind of an equilibrium between consultants and in-house talent.</p>



<p>While data analysis has become essential to every kind of business and AI is finding many applications outside of computing, salaries were highest in the computer industry itself, as&nbsp;Figure 7&nbsp;makes clear. For our purposes, the “computer industry” was divided into four segments: computer hardware, cloud services and hosting, security, and software. Average salaries in these industries ranged from $171,000 (for computer hardware) to $164,000 (for software). Salaries for the advertising industry (including social media) were surprisingly low, only $150,000.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/07-1048x842.jpg" alt="" class="wp-image-13961" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/07-1048x842.jpg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/07-300x241.jpg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/07-768x617.jpg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/07-1536x1234.jpg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/07-2048x1645.jpg 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Figure 7. Average salary by industry</em></figcaption></figure>



<p>Education and nonprofit organizations (including trade associations) were at the bottom end of the scale, with compensation just above $100,000 ($106,000 and $103,000, respectively). Salaries for technical workers in government were slightly higher ($124,000).</p>



<h2>Salaries by State</h2>



<p>When looking at data and AI practitioners geographically, there weren’t any big surprises. The states with the most respondents were California, New York, Texas, and Massachusetts. California accounted for 19% of the total, with over double the number of respondents from New York (8%). To understand how these four states dominate, remember that they make up 42% of our respondents but only 27% of the United States’ population.</p>



<p>Salaries in California were the highest, averaging&nbsp;$176,000. The Eastern Seaboard did well, with an average salary of $157,000 in Massachusetts (second highest). New York, Delaware, New Jersey, Maryland, and Washington, DC, all reported average salaries in the neighborhood of $150,000 (as did North Dakota, with five respondents). The average salary reported for Texas was $148,000, which is slightly above the national average but nevertheless seems on the low side for a state with a significant technology&nbsp;industry.</p>



<p>Salaries in the Pacific Northwest were not as high as we expected. Washington just barely made it into the top 10 in terms of the number of respondents, and average salaries in Washington and Oregon were $138,000 and $133,000, respectively. (See&nbsp;Figure 8&nbsp;for the full list.)</p>



<p>The highest-paying jobs, with salaries over $300,000, were concentrated in California (5% of the state’s respondents) and Massachusetts (4%). There were a few interesting outliers: North Dakota and Nevada both had very few respondents, but each had one respondent making over $300,000. In Nevada, we’re guessing that’s someone who works for the casino industry—after all, the origins of probability and statistics are tied to gambling. Most states had no respondents with compensation over $300,000.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/08-661x1048.jpg" alt="" class="wp-image-13962" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/08-661x1048.jpg 661w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/08-189x300.jpg 189w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/08-768x1218.jpg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/08-968x1536.jpg 968w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/08-1291x2048.jpg 1291w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/08-scaled.jpg 1614w" sizes="(max-width: 661px) 100vw, 661px" /><figcaption><em>Figure 8. Average salary by state</em></figcaption></figure>



<p>The lowest salaries were, for the most part, from states with the fewest respondents. We’re reluctant to say more than that. These states typically had under 10 respondents, which means that averaging salaries is extremely noisy. For example, Alaska only had two respondents and an average salary of $75,000; Mississippi and Louisiana each only had five respondents, and Rhode Island only had three. In any of these states, one or two additional respondents at the executive level would have a huge effect on the states average. Furthermore, the averages in those states are so low that all (or almost all) respondents must be students, interns, or in entry-level positions. So we don’t think we can make any statement stronger than “the high paying jobs are where you’d expect them to be.”</p>



<h2>Job Change by Salary</h2>



<p>Despite the differences between states, we found that the desire to change jobs based on lack of compensation didn’t depend significantly on geography. There were outliers at both extremes, but they were all in states where the number of respondents was small and one or two people looking to change jobs would make a significant difference. It’s not terribly interesting to say that 24% of respondents from California intend to change jobs (only 2% above the national average); after all, you’d expect California to dominate. There may be a small signal from states like New York, with 232 respondents, of whom 27% intend to change jobs, or from a state like Virginia, with 137 respondents, of whom only 19% were thinking of changing. But again, these numbers aren’t much different from the total percentage of possible job changers.</p>



<p>If intent to change jobs due to compensation isn’t dependent on location, then what does it depend on? Salary. It’s not at all surprising that respondents with the lowest salaries (under $50,000/year) are highly motivated to change jobs (29%); this group is composed largely of students, interns, and others who are starting their careers. The group that showed the second highest desire to change jobs, however, had the highest salaries: over $400,000/year (27%). It’s an interesting pairing: those with the highest and lowest salaries were most intent on getting a salary increase.</p>



<p>26% of those with annual salaries between $50,000 and $100,000 indicated that they intend to change jobs because of compensation. For the remainder of the respondents (those with salaries between $100,000 and $400,000), the percentage who intend to change jobs was 22% or lower.</p>



<h2>Salaries by Certification</h2>



<p>Over a third of the respondents (37%) replied that they hadn’t obtained any certifications in the past year. The next biggest group replied “other” (14%), meaning that they had obtained certifications in the past year but not one of the certifications we listed. We allowed them to write in their own responses, and they shared 352 unique answers, ranging from vendor-specific certifications (e.g., DataRobot) to university degrees (e.g., University of Texas) to well-established certifications in any number of fields (e.g., Certified Information Systems Security Professional a.k.a. CISSP). While there were certainly cases where respondents used different words to describe the same thing, the amount of unique write-in responses reflects the great number of certifications available.</p>



<p>Cloud certifications were by far the most popular. The top certification was for AWS (3.9% obtained AWS Certified Solutions Architect-Associate), followed by Microsoft Azure (3.8% had AZ-900: Microsoft Azure Fundamentals), then two more AWS certifications and CompTIA’s Security+ certification (1% each). Keep in mind that 1% only represents 27 respondents, and all the other certifications had even fewer respondents.</p>



<p>As&nbsp;Figure 9&nbsp;shows, the highest salaries were associated with AWS certifications, the Microsoft AZ-104 (Azure Administrator Associate) certification, and the CISSP security certification. The average salary for people listing these certifications was higher than the average salary for US respondents as a whole. And the average salary for respondents who wrote in a certification was slightly above the average for those who didn’t earn any certifications ($149,000 versus $143,000).</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/09-1048x580.jpg" alt="" class="wp-image-13963" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/09-1048x580.jpg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/09-300x166.jpg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/09-768x425.jpg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/09-1536x850.jpg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/09-2048x1133.jpg 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Figure 9. Average salary by certification earned</em></figcaption></figure>



<p>Certifications were also associated with salary increases (Figure 10). Again AWS and Microsoft Azure dominate, with Microsoft’s AZ-104 leading the way, followed by three AWS certifications. And on the whole, respondents with certifications appear to have received larger salary increases than those who didn’t earn any technical certifications.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/10-1048x599.jpg" alt="" class="wp-image-13964" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/10-1048x599.jpg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/10-300x171.jpg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/10-768x439.jpg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/10-1536x877.jpg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/10-2048x1170.jpg 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Figure 10. Average salary change by certification</em></figcaption></figure>



<p>Google Cloud is an obvious omission from this story. While Google is the third-most-important cloud provider, only 26 respondents (roughly 1%) claimed any Google certification, all under the “Other” category.</p>



<p>Among our respondents, security certifications were relatively uncommon and didn’t appear to be associated with significantly higher salaries or salary increases. Cisco’s CCNP was associated with higher salary increases; respondents who earned the CompTIA Security+ or CISSP certifications received smaller increases. Does this reflect that management undervalues security training? If this hypothesis is correct, undervaluing security is clearly a significant mistake, given the ongoing importance of security and the possibility of new attacks against AI and other data-driven systems.</p>



<p>Cloud certifications clearly had the greatest effect on salary increases. With very few exceptions, any certification was better than no certification: respondents who wrote in a certification under “Other” averaged a $9,600 salary increase over the last few years, as opposed to $8,900 for respondents who didn’t obtain a certification and $9,300 for all respondents regardless of certification.</p>



<h2>Training</h2>



<p>Participating in training resulted in salary increases—but only for those who spent more than 100 hours in a training program. As&nbsp;Figure 11 shows, those respondents had an average salary increase of $11,000. This was also the largest group of respondents (19%). Respondents who only reported undertaking 1–19 hours of training (8%) saw lower salary increases, with an average of $7,100. It’s interesting that those who participated in 1–19 hours of training saw smaller increases than those who didn’t participate in training at all. It doesn’t make sense to speculate about this difference, but the data does make one thing clear: if you engage in training, be serious about it.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/11-1048x468.jpg" alt="" class="wp-image-13965" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/11-1048x468.jpg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/11-300x134.jpg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/11-768x343.jpg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/11-1536x687.jpg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/11-2048x915.jpg 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Figure 11. Average salary change vs. hours of training</em></figcaption></figure>



<p>We also asked what types of training respondents engaged in: whether it was company provided (for which there were three alternatives), a certification program, a conference, or some other kind of training (detailed in&nbsp;Figure 12). Respondents who took advantage of company-provided opportunities had the highest average salaries ($156,000, $150,000, and $149,000). Those who obtained certifications were next ($148,000). The results are similar if we look at salary increases over the past three years: Those who participated in various forms of company-offered training received increases between $11,000 and $10,000. Salary increases for respondents who obtained a certification were in the same range ($11,000).</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/12-1048x557.jpg" alt="" class="wp-image-13966" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/12-1048x557.jpg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/12-300x160.jpg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/12-768x408.jpg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/12-1536x817.jpg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/12-2048x1089.jpg 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Figure 12. Average salary change vs. type of training</em></figcaption></figure>



<h2>The Last Word</h2>



<p>Data and AI professionals—a rubric under which we include data scientists, data engineers, and specialists in AI and ML—are well-paid, reporting an average salary just under $150,000. However, there were sharp state-by-state differences: salaries were significantly higher in California, though the Northeast (with some exceptions) did well.</p>



<p>There were also significant differences between salaries for men and women. Men’s salaries were higher regardless of job title, regardless of training and regardless of academic degrees—even though women were more likely to have an advanced academic degree (PhD or master’s degree) than were men.</p>



<p>We don’t see evidence of a “great resignation.” Job turnover through the pandemic was roughly what we’d expect (perhaps slightly below normal). Respondents did appear to be concerned about job security, though they didn’t want to admit it explicitly. But with the exception of the least- and most-highly compensated respondents, the intent to change jobs because of salary was surprisingly consistent and nothing to be alarmed at.</p>



<p>Training was important, in part because it was associated with hireability and job security but more because respondents were genuinely interested in learning new skills and improving current ones. Cloud training, particularly in AWS and Microsoft Azure, was the most strongly associated with higher salary increases.</p>



<p>But perhaps we should leave the last word to our respondents. The final question in our survey asked what areas of technology would have the biggest effect on salary and promotions in the coming year. It wasn’t a surprise that most of the respondents said machine learning (63%)—these days, ML is the hottest topic in the data world. It was more of a surprise that “programming languages” was noted by just 34% of respondents. (Only “Other” received fewer responses—see&nbsp;Figure 13&nbsp;for full details.) Our respondents clearly aren’t impressed by programming languages, even though the data suggests that employers are willing to pay a premium for Rust, Go, and Scala.</p>



<p>There’s another signal worth paying attention to if we look beyond the extremes. Data tools, cloud and containers, and automation were nearly tied (46, 47, and 44%). The cloud and containers&nbsp;category&nbsp;includes tools like Docker and Kubernetes, cloud providers like AWS and Microsoft Azure, and disciplines like MLOps. The tools category includes tools for building and maintaining data pipelines, like Kafka. “Automation” can mean a lot of things but in this context probably means automated training and deployment.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/13-1048x808.jpg" alt="" class="wp-image-13967" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/13-1048x808.jpg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/13-300x231.jpg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/13-768x592.jpg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/13-1536x1184.jpg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/09/13-2048x1579.jpg 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Figure 13. What technologies will have the biggest effect on compensation in the coming year?</em></figcaption></figure>



<p>We’ve argued for some time that&nbsp;<a href="https://www.oreilly.com/radar/ai-meets-operations/">operations</a>—successfully deploying and managing applications in production—is the biggest issue facing ML practitioners in the coming years. If you want to stay on top of what’s happening in data, and if you want to maximize your job security, hireability, and salary, don’t just learn how to build AI models; learn how to deploy applications that live in the cloud.</p>



<p>In the classic movie&nbsp;<em>The Graduate</em>, one character famously says, “There’s a great future in plastics. Think about it.” In 2021, and without being anywhere near as repulsive, we’d say, “There’s a great future in the cloud. Think about it.”</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/2021-data-ai-salary-survey/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>Communal Computing’s Many Problems</title>
		<link>https://www.oreilly.com/radar/communal-computings-many-problems/</link>
				<comments>https://www.oreilly.com/radar/communal-computings-many-problems/#respond</comments>
				<pubDate>Tue, 20 Jul 2021 11:37:15 +0000</pubDate>
		<dc:creator><![CDATA[Chris Butler]]></dc:creator>
				<category><![CDATA[AI & ML]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Software Engineering]]></category>
		<category><![CDATA[Deep Dive]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=13876</guid>
				<description><![CDATA[In the first article of this series, we discussed communal computing devices and the problems they create–or, more precisely, the problems that arise because we don’t really understand what “communal” means. Communal devices are intended to be used by groups of people in homes and offices. Examples include popular home assistants and smart displays like [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>In the <a href="https://www.oreilly.com/radar/communal-computing/">first article</a> of this series, we discussed communal computing devices and the problems they create–or, more precisely, the problems that arise because we don’t really understand what “communal” means. Communal devices are intended to be used by groups of people in homes and offices. Examples include popular home assistants and smart displays like the Amazon Echo, Google Home, Apple HomePod, and many others.&nbsp; If we don’t create these devices with communities of people in mind, we will continue to build the wrong ones.</p>



<p>Ever since the concept of a “user” was invented (which was probably later than you think), we’ve assumed that devices are “owned” by a single user. Someone buys the device and sets up the account; it’s their device, their account.&nbsp; When we’re building shared devices with a user model, that model quickly runs into limitations. What happens when you want your home assistant to play music for a dinner party, but your preferences have been skewed by your children’s listening habits? We, as users, have certain expectations for what a device should do. But we, as technologists, have typically ignored our own expectations when designing and building those devices.</p>



<p>This expectation isn’t a new one either. The telephone in the kitchen was for everyone’s use. After the release of the iPad in 2010 <a href="https://furbo.org/2010/04/29/communal-computing/">Craig Hockenberry discussed the great value of communal computing but also the concerns</a>:</p>



<blockquote class="wp-block-quote"><p>“When you pass it around, you’re giving everyone who touches it the opportunity to mess with your private life, whether intentionally or not. That makes me uneasy.”</p></blockquote>



<p>Communal computing requires a new mindset that takes into account users’ expectations. If the devices aren’t designed with those expectations in mind, they’re destined for the landfill. Users will eventually experience “weirdness” and “annoyance” that grows to distrust of the device itself. As technologists, we often call these weirdnesses “edge cases.” That’s precisely where we’re wrong: they’re not edge cases, but they’re at the core of how people want to use these devices.</p>



<p>In the first article, we listed five core questions we should ask about communal devices:</p>



<ol><li>Identity: Do we know all of the people who are using the device?</li><li>Privacy: Are we exposing (or hiding) the right content for all of the people with access?</li><li>Security: Are we allowing all of the people using the device to do or see what they should and are we protecting the content from people that shouldn’t?</li><li>Experience: What is the contextually appropriate display or next action?</li><li>Ownership: Who owns all of the data and services attached to the device that multiple people are using?</li></ol>



<p>In this article, we’ll take a deeper look at these questions, to see how the problems manifest and how to understand them.</p>



<h2>Identity</h2>



<p>All of the problems we’ve listed start with the idea that there is one registered and known person who should use the device. That model doesn’t fit reality: the identity of a communal device isn’t a single person, but everyone who can interact with it. This could be anyone able to tap the screen, make a voice command, use a remote, or simply be sensed by it. To understand this communal model and the problems it poses, start with the person who buys and sets up the device. It is associated with that individual’s account, like a personal Amazon account with its order history and shopping list. Then it gets difficult. Who doesn&#8217;t, can’t, or shouldn’t have full access to an Amazon account? Do you want everyone who comes into your house to be able to add something to your shopping list?</p>



<p>If you think about the spectrum of people who could be in your house, they range from people whom you trust, to people who you don’t really trust but who should be there, to those who you&nbsp; shouldn’t trust at all.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/07/trust-1048x387.png" alt="" class="wp-image-13877" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/07/trust-1048x387.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/07/trust-300x111.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/07/trust-768x284.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/07/trust-1536x567.png 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/07/trust.png 1950w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><br>There is a spectrum of trust for people who have access to communal devices</figcaption></figure>



<p>In addition to individuals, we need to consider the groups that each person could be part of. These group memberships are called “pseudo-identities”; they are facets of a person’s full identity. They are usually defined by how the person associated themself with a group of other people. My life at work, home, a high school friends group, and as a sports fan show different parts of my identity. When I’m with other people who share the same pseudo-identity, we can share information. When there are people from one group in front of a device I may avoid showing content that is associated with another group (or another personal pseudo-identity). This can sound abstract, but it isn’t; if you’re with friends in a sports bar, you probably want notifications about the teams you follow. You probably don’t want news about work, unless it’s an emergency.</p>



<p>There are important reasons why we show a particular facet of our identity in a particular context. When designing an experience, you need to consider the identity context and where the experience will take place. Most recently this has come up with work from home. Many people talk about ‘<a href="https://mike-robbins.com/tedxberkeley/">bringing your whole self to work</a>,’ but don&#8217;t realize that “your whole self” isn’t always appropriate. Remote work changes when and where I should interact with work. For a smart screen in my kitchen, it is appropriate to have content that is related to my home and family. Is it appropriate to have all of my work notifications and meetings there? Could it be a problem for children to have the ability to join my work calls? What does my IT group require as far as security of work devices versus personal home devices?</p>



<p>With these devices we may need to switch to a different pseudo-identity to get something done. I may need to be reminded of a work meeting. When I get a notification from a close friend, I need to decide whether it is appropriate to respond based on the other people around me.</p>



<p>The pandemic has broken down the barriers between home and work. The natural context switch from being at work and worrying about work things and then going home to worry about home things is no longer the case. People need to make a conscious effort to “turn off work” and to change the context. Just because it is the middle of the workday doesn’t always mean I want to be bothered by work. I may want to change contexts to take a break. Such context shifts add nuance to the way the current pseudo-identity should be considered, and to the overarching context you need to detect.</p>



<p>Next, we need to consider identities as groups that I belong to. I’m part of my family, and my family would potentially want to talk with other families. I live in a house that is on my street alongside other neighbors. I’m part of an organization that I identify as my work. These are all pseudo-identities we should consider, based on where the device is placed and in relation to other equally important identities.</p>



<p>The crux of the problem with communal devices is the multiple identities that are or may be using the device. This requires greater understanding of who, where, and why people are using the device. We need to consider the types of groups that are part of the home and office.</p>



<h2>Privacy </h2>



<p>As we consider the identities of all people with access to the device, and the identity of the place the device is to be part of, we start to consider what privacy expectations people may have given the context in which the device is used.</p>



<p>Privacy is hard to understand. The framework I’ve found most helpful is <a href="https://en.wikipedia.org/wiki/Contextual_Integrity">Contextual Integrity</a> which was introduced by Helen Nissenbaum in the book <a href="https://www.amazon.com/Privacy-Context-Technology-Policy-Integrity/dp/0804752370">Privacy in Context</a>. Contextual Integrity describes four key aspects of privacy:</p>



<ol><li>Privacy is provided by appropriate flows of information.</li><li>Appropriate information flows are those that conform to contextual information norms.</li><li>Contextual informational norms refer to five independent parameters: data subject, sender, recipient, information type, and transmission principle.</li><li>Conceptions of privacy are based on ethical concerns that evolve over time.</li></ol>



<p>What is most important about Contextual Integrity is that privacy is not about hiding information away from the public but giving people a way to control the flow of their own information. The context in which information is shared determines what is appropriate.</p>



<p>This flow either feels appropriate, or not, based on key characteristics of the information (<a href="https://en.wikipedia.org/wiki/Contextual_Integrity#Contextual_Integrity%E2%80%99s_Parameters">from Wikipedia</a>):</p>



<ol><li>The data subject: Who or what is this about?</li><li>The sender of the data: Who is sending it?</li><li>The recipient of the data: Who will eventually see or get the data?</li><li>The information type: What type of information is this (e.g. a photo, text)?</li><li>The transmission principle: In what set of norms is this being shared (e.g. school, medical, personal communication)?</li></ol>



<p>We rarely acknowledge how a subtle change in one of these parameters could be a violation of privacy. It may be completely acceptable for my friend to have a weird photo of me, but once it gets posted on a company intranet site it violates how I want information (a photo) to flow. The recipient of the data has changed to something I no longer find acceptable. But I might not care whether a complete stranger (like a burglar) sees the photo, as long as it never gets back to someone I know.</p>



<p>For communal use cases, the sender or receiver of information is often a group. There may be&nbsp; multiple people in the room during a video call, not just the person you are calling. People can walk in and out. I might be happy with some people in my home seeing a particular photo, but find it embarrassing if it is shown to guests at a dinner party.</p>



<p>We must also consider what happens when other people’s content is shown to those who shouldn’t see it. This content could be photos or notifications from people outside the communal space that could be seen by anyone in front of the device. Smartphones can hide message contents when you aren’t near your phone for this exact reason.</p>



<p>The services themselves can expand the ‘receivers’ of information in ways that create uncomfortable situations. In Privacy in Context, Nissenbaum talks about <a href="https://en.wikipedia.org/wiki/Google_Street_View_privacy_concerns">the privacy implications of Google Street View when it places photos of people’s houses</a> on Google Maps. When a house was only visible to people who walked down the street that was one thing, but when anyone in the world can access a picture of a house, that changes the parameters in a way that causes concern. Most recently, <a href="https://www.theverge.com/2019/3/12/18262646/ibm-didnt-inform-people-when-it-used-their-flickr-photos-for-facial-recognition-training">IBM used Flickr photos that were shared under a Creative Commons license to train facial recognition algorithms</a>. While this didn’t require any change to terms of the service it was a surprise to people and may be in violation of the Creative Commons license. In the end, IBM took the dataset down.</p>



<p>Privacy considerations for communal devices should focus on who is gaining access to information and whether it is appropriate based on people’s expectations. Without using a framework like contextual inquiry we will be stuck talking about generalized rules for data sharing, and there will always be edge cases that violate someone’s privacy.</p>



<hr class="wp-block-separator" />



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<h5>A note about children</h5>



<p class="has-background has-very-light-gray-background-color">Children make identity and privacy especially tricky. <a href="https://www.statista.com/statistics/242074/percentages-of-us-family-households-with-children-by-type/#statisticContainer">About 40% of all households</a> have a child. Children shouldn’t be an afterthought. If you aren’t compliant with local laws you can get in a lot of trouble. In 2019, <a href="https://arstechnica.com/tech-policy/2019/09/youtube-fined-170-million-for-violations-of-childrens-privacy/">YouTube had to settle with the FTC</a> for a $170 million fine for selling ads targeting children. It gets complicated because the ‘age of consent’ depends on the region as well: <a href="https://www.ftc.gov/enforcement/rules/rulemaking-regulatory-reform-proceedings/childrens-online-privacy-protection-rule">COPPA in the US is for people under 13 years old</a>, <a href="https://www.clarip.com/data-privacy/ccpa-kids-consent/">CCPA in California is for people under 16</a>, and <a href="https://www.privo.com/blog/gdpr-age-of-digital-consent">GDPR overall is under 16 years old but each member state can set its own</a>. The moment you acknowledge children are using your platforms, you need to accommodate them.</p>



<p class="has-background has-very-light-gray-background-color">For communal devices, there are many use cases for children. Once they realize they can play whatever music they want (including tracks of fart sounds) on a shared device they will do it. Children focus on the <a href="https://www.sciencedaily.com/releases/2020/08/200812153637.htm">exploration over the task</a> and will end up discovering way more about the device than parents might. Adjusting your practices after building a device is a recipe for failure. You will find that the paradigms you choose for other parties won’t align with the expectations for children, and modifying your software to accommodate children is difficult or impossible. It’s important to account for children from the beginning.</p>



<hr class="wp-block-separator" />
</div></div>



<h2>Security </h2>



<p>To get to a home assistant, you usually need to pass through a home’s outer door. There is usually a physical limitation by way of a lock. There may be alarm systems. Finally, there are social norms: you don’t just walk into someone else’s house without knocking or being invited.</p>



<p>Once you are past all of these locks, alarms, and norms, anyone can access the communal device. Few things within a home are restricted–possibly a safe with important documents. When a communal device requires authentication, it is usually subverted in some way for convenience: for example, a password might be taped to it, or a password may never have been set.</p>



<p>The concept of <a href="https://www.amazon.com/Zero-Trust-Networks-Building-Untrusted/dp/1491962194">Zero Trust Networks</a> speaks to this problem. It comes down to a key question: is the risk associated with an action greater than the trust we have that the person performing the action is who they say they are?</p>



<figure class="wp-block-image"><img src="https://lh3.googleusercontent.com/oIE9EerVy1LX6be50IEGWVshKMNJV47fbOCgQ2n7kHTkmZgEISlRx5t6dmzjDxldR1XlDpaDMFiMFp7rnKnTV3T1RyNXBerEUx9QOy7VsKqfSqbRcIssbDbOfl9yfhwJpmrb3cvv" alt="" /><figcaption><br>Source: <a href="https://learning.oreilly.com/library/view/zero-trust-networks/9781491962183/">https://learning.oreilly.com/library/view/zero-trust-networks/9781491962183/</a> </figcaption></figure>



<p>Passwords, passcodes, or mobile device authentication become nuisances; these supposed secrets are frequently shared between everyone who has access to the device. Passwords might be written down for people who can’t remember them, making them visible to less trusted people visiting your household. Have we not learned anything since the movie <a href="https://www.youtube.com/watch?v=_UqEg1cFqig">War Games</a>?</p>



<p>When we consider the risk associated with an action, we need to understand its privacy implications. Would the action expose someone’s information without their knowledge? Would it allow a person to pretend to be someone else? Could another party tell easily the device was being used by an imposter?</p>



<p>There is a tradeoff between the trust and risk. The device needs to calculate whether we know who the person is and whether the person wants the information to be shown. That needs to be weighed against the potential risk or harm if an inappropriate person is in front of the device.</p>



<figure class="wp-block-image"><img src="https://lh5.googleusercontent.com/HS9D_BfERJDalfgZqmP54N9LcJNKB9jsfciGJGjHwwY01hMgMDs6p4Hrivp-nUmtMbLuoiTX1MxGwBKbLRfqvpCiHeou88dgaV7MRpv1cU852iH75aFunOMRY7amt1-6nEbGJxNS" alt="" /><figcaption><br>Having someone in your home accidentally share embarrassing photos could have social implications.</figcaption></figure>



<p>A few examples of this tradeoff:</p>



<figure class="wp-block-table"><table class=""><tbody><tr><td><strong>Feature</strong></td><td><strong>Risk and trust calculation</strong></td><td><strong>Possible issues</strong></td></tr><tr><td>Showing a photo when the device detects someone in the room</td><td>Photo content sensitivity, who is in the room&nbsp;</td><td>Showing an inappropriate photo to a complete stranger</td></tr><tr><td>Starting a video call</td><td>Person’s account being used for the call, the actual person starting the call</td><td>When the other side picks up it may not be who they thought it would be</td></tr><tr><td>Playing a personal song playlist</td><td>Personal recommendations being impacted</td><td>Incorrect future recommendations</td></tr><tr><td>Automatically ordering something based on a voice command</td><td>Convenience of ordering, approval of the shopping account’s owner</td><td>Shipping an item that shouldn’t have been ordered</td></tr></tbody></table></figure>



<p>This gets even trickier when people no longer in the home can access the devices remotely. There have been <a href="https://www.nytimes.com/2018/06/23/technology/smart-home-devices-domestic-abuse.html">cases of harassment, intimidation, and domestic abuse</a> by people whose access should have been revoked: for example, an ex-partner turning off the heating system. When should someone be able to access communal devices remotely? When should their access be controllable from the devices themselves? How should people be reminded to update their access control lists? How does basic security maintenance happen inside a communal space? </p>



<p>See how much work this takes in a <a href="https://twitter.com/bryanmcaninch/status/1396099891802353671">recent account of pro bono security work</a> for a harassed mother and her son. Or how a <a href="https://twitter.com/_DanielSinclair/status/1405686762563719174">YouTuber was blackmailed, surveilled, and harassed by her smart home</a>. <a href="https://manuals.info.apple.com/MANUALS/1000/MA1976/en_US/device-and-data-access-when-personal-safety-is-at-risk.pdf">Apple even has a manual</a> for this type of situation.</p>



<p>At home, where there’s no corporate IT group to create policies and automation to keep things secure, it’s next to impossible to manage all of these security issues. Even some corporations have trouble with it. We need to figure out how users will maintain and configure a communal device over time. Configuration for devices in the home and office can be wrought with lots of different types of needs over time.</p>



<p>For example, what happens when someone leaves the home and is no longer part of it? We will need to remove their access and may even find it necessary to block them from certain services. This is highlighted with the cases of harassment of people through spouses that still control the communal devices. Ongoing maintenance of a particular device could also be triggered by a change in needs by the community. A home device may be used to just play music or check the weather at first. But when a new baby comes home, being able to do video calling with close relatives may become a higher priority.</p>



<p>End users are usually very bad at changing configuration after it is set. They may not even know that they can configure something in the first place. This is why people have made a business out of setting up home stereo and video systems. People just don’t understand the technologies they are putting in their houses. Does that mean we need some type of handy-person that does home device setup and management? When more complicated routines are required to meet the needs, how does someone allow for changes without writing code, if they are allowed to?</p>



<p>Communal devices need new paradigms of security that go beyond the standard login. The world inside a home is protected by a barrier like a locked door; the capabilities of communal devices should respect that. This means both removing friction in some cases and increasing it in others.</p>



<hr class="wp-block-separator" />



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<h5>A note about biometrics</h5>



<figure class="wp-block-image"><img src="https://lh4.googleusercontent.com/qbw387_ORSOiu1Mgg3qhPgTkqbswCa4_ErEyVBjfNVjyUK10Qsp1OoxXif2W4g_p11GP9kwIEW_19G1nV0p8I_zggBTciyFLHaB82PBkuN3PMWSJLNaGRCtUGB77CmAN7IeFbYVb" alt="" /><figcaption><br>&nbsp;“Turn your face” to enroll in Google Face Match and personalize your devices.<br><em>(Source: Google Face Match video, <a href="https://youtu.be/ODy_xJHW6CI?t=26">https://youtu.be/ODy_xJHW6CI?t=26</a>) </em></figcaption></figure>



<p class="has-background has-very-light-gray-background-color">Biometric authentication for voice and face recognition can help us get a better understanding of who is using a device. Examples of biometric authentication include <a href="https://support.apple.com/en-us/HT208108">FaceID</a> for the iPhone and voice profiles for Amazon Alexa. There is a push for <a href="https://www.theverge.com/2021/4/20/22393873/ftc-ai-machine-learning-race-gender-bias-legal-violation">regulation of facial recognition technologies</a>, but opt-in for authentication purposes tends to be carved out.</p>



<p class="has-background has-very-light-gray-background-color">However, biometrics aren’t without problems. In addition to issues with <a href="http://gendershades.org/overview.html">skin tone, gender bias</a>, and <a href="https://news.stanford.edu/2020/03/23/automated-speech-recognition-less-accurate-blacks/">local accents</a>, biometrics assumes that everyone is willing to have a biometric profile on the device–and that they would be legally allowed to (for example, children may not be allowed to consent to a biometric profile). It also assumes this technology is secure. <a href="https://support.google.com/googlenest/answer/9320885?co=GENIE.Platform%3DAndroid&amp;hl=en">Google FaceMatch makes it very clear it is only a technology for personalization,</a> rather than authentication. I can only guess they have legalese to avoid liability when an unauthorized person spoofs someone’s face, say by taking a photo off the wall and showing it to the device.</p>



<p class="has-background has-very-light-gray-background-color">What do we mean by “personalization?” When you walk into a room and FaceMatch identifies your face, the Google Home Hub dings, shows your face icon, then shows your calendar (if it is connected), and a feed of personalized cards. Apple’s <a href="https://support.apple.com/en-us/HT208108">FaceID uses many levels of presentation attack detection</a> (also known as “anti-spoofing”): it verifies your eyes are open and you are looking at the screen, and it uses a depth sensor to make sure it isn’t “seeing” a photo. The phone can then show hidden notification content or open the phone to the home screen. This measurement of trust and risk is benefited by understanding who could be in front of the device. We can’t forget that the machine learning that is doing biometrics is not a deterministic calculation; there is always some degree of uncertainty.</p>



<p class="has-background has-very-light-gray-background-color">Social and information norms define what we consider acceptable, who we trust, and how much. As trust goes up, we can take more risks in the way we handle information. However, it’s difficult to connect trust with risk without understanding people’s expectations. I have access to my <a href="https://www.wired.co.uk/article/password-security-sharing">partner’s iPhone and know the passcode</a>. It would be a violation of a norm if I walked over and unlocked it without being asked, and doing so will lead to reduced trust between us. </p>



<p class="has-background has-very-light-gray-background-color">As we can see, biometrics does offer some benefits but won’t be the panacea for the unique uses of communal devices. Biometrics will allow those willing to opt-in to the collection of their biometric profile to gain personalized access with low friction, but it will never be useable for everyone with physical access. </p>
</div></div>



<hr class="wp-block-separator" />



<h2>Experiences</h2>



<p>People use a communal device for short experiences (checking the weather), ambient experiences (listening to music or glancing at a photo), and joint experiences (multiple people watching a movie). The device needs to be aware of norms within the space and between the multiple people in the space. Social norms are rules by which people decide how to act in a particular context or space. In the home, there are norms about what people should and should not do. If you are a guest, you try to see if people take their shoes off at the door; you don’t rearrange things on a bookshelf; and so on.</p>



<p>Most software is built to work for as many people as possible; this is called generalization. Norms stand in the way of generalization. Today’s technology isn’t good enough to adapt to every possible situation. One strategy is to simplify the software’s functionality and let the humans enforce norms. For example, when multiple people talk to an Echo at the same time, Alexa will either not understand or it will take action on the last command. <a href="https://www.theverge.com/2020/9/24/21452313/alexa-voice-assistant-ai-upgrade-amazon-echo-smart-speaker-multiple-people-tone-questions">Multi-turn conversations between multiple people</a> are still in their infancy. This is fine when there are understood norms–for example, between my partner and I. But it doesn’t work so well when you and a child are both trying to shout commands.</p>



<figure class="wp-block-image"><img src="https://lh6.googleusercontent.com/DqO-tlVqweGB2Qn0dTCipZ9XZ-GlJKnel9bxMdOWKhqKjVYqftcKw6GNketKIQvVNjAAwsAVnH52CAd0K9KvF-idqGZtcbuTVKzPQM5uZiqcvPc9sQ0oBLzwsPm5NBdltxgvuJ0h" alt="" /><figcaption><br>Shared experiences can be challenging like a parent and child yelling at an Amazon Echo to play what they want.</figcaption></figure>



<p>Norms are interesting because they tend to be learned and negotiated over time, but are invisible. Experiences that are built for communal use need to be aware of these invisible norms through cues that can be detected from peoples’ actions and words. This gets especially tricky because a conversation between two people could include information subject to different expectations (in a Contextual Integrity sense) about how that information is used. With enough data, models can be created to “read between the lines” in both helpful and dangerous ways.</p>



<p>Video games already cater to multiple people’s experiences. With the Nintendo Switch or any other gaming system, several people can play together in a joint experience. However, the rules governing these experiences are never applied to, say, Netflix. The assumption is always that one person holds the remote. How might these experiences be improved if software could accept input from multiple sources (remote controls, voice, etc.) to build a selection of movies that is appropriate for everyone watching?</p>



<p>Communal experience problems highlight inequalities in households. With <a href="https://www.nytimes.com/interactive/2021/05/17/upshot/women-workforce-employment-covid.html">women doing more household coordination</a> than ever, there is a need to rebalance the tasks for households. Most of the time these coordination tasks are relegated to personal devices, generally the wife’s mobile phone, when they involve the entire family (though there is a <a href="https://www.oecd.org/digital/bridging-the-digital-gender-divide.pdf">digital divide outside the US</a>). Without moving these experiences into a place that everyone can participate in, we will continue these inequalities.</p>



<p>So far, technology has been great at intermediating people for coordination through systems like text messaging, social networks, and collaborative documents. We don’t build interaction paradigms that allow for multiple people to engage at the same time in their communal spaces. To do this we need to address that the norms that dictate what is appropriate behavior are invisible and pervasive in the spaces these technologies are deployed.</p>



<h2>Ownership</h2>



<p>Many of these devices are not really owned by the people who buy them. As part of the current trend towards subscription-based business models, the device won’t function if you don’t subscribe to a service. Those services have license agreements that specify what you can and cannot do (which you can read if you have a <a href="https://www.visualcapitalist.com/terms-of-service-visualizing-the-length-of-internet-agreements/">few hours to spare</a> and <a href="https://www.nytimes.com/interactive/2019/06/12/opinion/facebook-google-privacy-policies.html">can understand them</a>).</p>



<p>For example, this has been an issue for fans of Amazon’s <a href="https://www.theverge.com/22381356/blink-homebridge-crackdown-amazon-subscriptions">Blink camera</a>. The home automation industry is fragmented: there are many vendors, each with its own application to control their particular devices. But most people don’t want to use different apps to control their lighting, their television, their security cameras, and their locks. Therefore, people have started to build controllers that span the different ecosystems. Doing so has caused Blink users to get their accounts suspended.</p>



<p>What’s even worse is that these license agreements can change whenever the company wants. Licenses are frequently modified with nothing more than a notification, after which something that was previously acceptable is now forbidden. In 2020, <a href="https://www.consumerreports.org/smart-home/wink-tells-users-pay-up-or-we-will-disable-smart-home-hub/">Wink suddenly applied</a> a monthly service charge; if you didn’t pay, the device would stop working. Also in 2020, Sonos caused a stir by saying they were going to <a href="https://www.theverge.com/2020/3/5/21166777/sonos-ending-recycle-mode-trade-up-program-sustainability">“recycle” (disable) old devices.</a> They eventually changed their policy.</p>



<p>The issue isn’t just what you can do with your devices; it’s also what happens to the data they create. <a href="https://www.washingtonpost.com/technology/2019/08/28/doorbell-camera-firm-ring-has-partnered-with-police-forces-extending-surveillance-reach/">Amazon’s Ring partnership</a> with <a href="https://www.theguardian.com/commentisfree/2021/may/18/amazon-ring-largest-civilian-surveillance-network-us">one in ten US police departments</a> troubles many privacy groups because it creates a vast surveillance program. What if you don’t want to be a part of the police state? Make sure you check the right box and read your terms of service. If you’re designing a device, you need to require users to opt in to data sharing (especially as regions adapt GDPR and CCPA-like regulation).</p>



<p>While techniques like <a href="https://ai.googleblog.com/2017/04/federated-learning-collaborative.html">federated learning</a> are on the horizon, to avoid latency issues and mass data collection, it remains to be seen whether those techniques are satisfactory for companies that collect data. Is there a benefit to both organizations and their customers to limit or obfuscate the transmission of data away from the device?</p>



<p>Ownership is particularly tricky for communal devices. This is a collision between the expectations of consumers who put something in their home; those expectations run directly against the way rent-to-use services are pitched. Until we acknowledge that hardware put in a home is different from a cloud service, we will never get it right.</p>



<h2>Lots of problems, now what?</h2>



<p>Now that we have dived into the various problems that rear their head with communal devices, what do we do about it? In the next article we discuss a way to consider the map of the communal space. This helps build a better understanding of how the communal device fits in the context of the space and services that exist already.</p>



<p>We will also provide a list of dos and don’ts for leaders, developers, and designers to consider when building a communal device.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/communal-computings-many-problems/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>Communal Computing</title>
		<link>https://www.oreilly.com/radar/communal-computing/</link>
				<comments>https://www.oreilly.com/radar/communal-computing/#respond</comments>
				<pubDate>Tue, 15 Jun 2021 11:27:36 +0000</pubDate>
		<dc:creator><![CDATA[Chris Butler]]></dc:creator>
				<category><![CDATA[AI & ML]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Deep Dive]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=13812</guid>
				<description><![CDATA[Home assistants and smart displays are being sold in record numbers, but they are built wrong. They are designed with one person in mind: the owner. These technologies need to fit into the communal spaces where they are placed, like homes and offices. If they don’t fit, they will be unplugged and put away due [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>Home assistants and smart displays are being sold in record numbers, but they are built wrong. They are designed with one person in mind: the owner. These technologies need to fit into the communal spaces where they are placed, like homes and offices. If they don’t fit, they will be unplugged and put away due to lack of trust.</p>



<p>The problems are subtle at first. Your Spotify playlist starts to have recommendations for songs you don’t like. You might see a photo you took on someone else’s digital frame. An Apple TV reminds you of a new episode of a show your partner watches. Guests are asking you to turn on your IoT-enabled lights for them. The wrong person’s name shows up in the Zoom call. Reminders for medication aren’t heard by the person taking the medication. Bank account balances are announced during a gathering of friends.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/06/Amazon_device_words-1048x589.jpg" alt="" class="wp-image-13813" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/06/Amazon_device_words-1048x589.jpg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/06/Amazon_device_words-300x169.jpg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/06/Amazon_device_words-768x432.jpg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/06/Amazon_device_words-1536x864.jpg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/06/Amazon_device_words-2048x1152.jpg 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Would you want your bank account balances announced during a dinner party?</figcaption></figure>



<p>This is the start of a series discussing the design of communal devices–devices designed to work in communal spaces. The series is a call to action for everyone developing communal devices–whether you are creating business cases, designing experiences, or building technology–to take a step back and consider what is really needed.</p>



<p>This first article discusses what communal devices are, and how problems that appear result from our assumptions about how they’re used. Those assumptions were inherited from the world of PCs: the rules that apply to your laptop or your iPad just don’t apply to home assistants and other “smart devices,” from light bulbs to refrigerators.&nbsp; It isn’t just adding the ability for people to switch accounts. We need a new paradigm for the future of technical infrastructure for our homes and offices. In this series of articles we will tell you how we got here, why it is problematic, and where to go to enable communal computing.</p>



<h2>The Wrong Model</h2>



<p>Problems with communal devices arise because the industry has focused on a specific model for how these devices are used: a single person buys, sets up, and uses the device. If you bought one of these devices (for example, a smart speaker) recently, how many other people in your household did you involve in setting it up?</p>



<p>Smart screen makers like Amazon and Google continue to make small changes to try to fix the weirdness. They have recently added technology to automatically personalize based on someone’s face or voice. These are temporary fixes that will only be effective until the next special case reveals itself. Until the industry realizes the communal nature of users’ needs they will just be short lived patches. We need to turn the model around to make the devices communal first, rather than communal as an afterthought.</p>



<p>I recently left Facebook Reality Labs, where I was working on the Facebook Portal identity platform, and realized that there was zero discourse about this problem in the wider world of technology. I’ve read through many articles on how to create Alexa skills and attended talks about the use of IoT, and I’ve even made my own voice skills. There was no discussion of the communal impacts of those technologies. If we don’t address the problems this creates, these devices will be relegated to a small number of uses, or unplugged to make room for the next one. The problems were there, just beneath the shiny veneer of new technologies.</p>



<h3>Communal began at home</h3>



<p>Our home infrastructure was originally communal. Consider a bookcase: someone may have bought it, but anyone in the household could update it with new books or tchotchkes. Guests could walk up to browse the books you had there. It was meant to be shared with the house and those that had access to it.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/06/iStock-882859590-1048x699.jpg" alt="" class="wp-image-13814" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/06/iStock-882859590-1048x699.jpg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/06/iStock-882859590-300x200.jpg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/06/iStock-882859590-768x512.jpg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/06/iStock-882859590-1536x1024.jpg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/06/iStock-882859590-2048x1365.jpg 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>The old landline in your kitchen is the original communal device.</figcaption></figure>



<p>Same for the old landline that was in the kitchen. When you called, you were calling a household. You didn’t know specifically who would pick up. Anyone who was part of that household could answer. We had protocols for getting the phone from the person who answered the call to the intended recipient. Whoever answered could either yell for someone to pick up the phone elsewhere in the home, or take a message. If the person answering the phone wasn’t a member of the household, it would be odd, and you’d immediately think “wrong number.”</p>



<p>It wasn’t until we had the user model for <a href="https://en.wikipedia.org/wiki/Time-sharing">mainframe time sharing</a> that we started to consider who was using a computer. This evolved into full login systems with passwords, password reset, two factor authentication, biometric authentication, and more. As computers became more common,&nbsp; what made sense inside of research and academic institutions was repurposed for the office.</p>



<p>In the 1980s and 1990s a lot of homes got their first personal computer. These were shared, communal devices, though more by neglect than by intention. A parent would purchase it and then set it up in the living room so everyone could use it. The account switching model wasn’t added until visual systems like Windows arrived, but account management was poorly designed and rarely used. Everyone just piggybacked on each other’s access. If anyone wanted privacy, they had to lock folders with a password or hide them in an endless hierarchy.</p>



<h2>Early Attempts at Communal Computing</h2>



<p>Xerox-PARC started to think about what more communal or ubiquitous computing would mean. However, they focused on fast account switching. They were answering the question: how could I get the personal context to this communal device as fast as possible? One project was digitizing the whiteboard, a fundamentally communal device. It was called <a href="https://www.markstefik.com/?page_id=155">The Colab</a> and offered a way for anyone to capture content in a meeting room and then walk it around the office to other shared boards.</p>



<p>Not only did the researchers at PARC think about sharing computers for presentations, they also wondered how they could have someone walk up to a computer and have it be configured for them automatically. It was enabled by special cards called “Active Badges,” described in “<a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.49.1301&amp;rep=rep1&amp;type=pdf">A New Location Technique for the Active Office</a>.” The paper starts with an important realization:</p>



<blockquote class="wp-block-quote"><p>“&#8230;researchers have begun to examine computers that would autonomously change their functionality based on observations of who or what was around them. By determining their context, using input from sensor systems distributed throughout the environment, computing devices could personalize themselves to their current user, adapt their behaviour according to their location, or react to their surroundings.”</p></blockquote>



<p>Understanding the context around the device is very important in building a system that adapts. At this point, however, researchers were still thinking about a ‘current user’ and their position relative to the system, rather than the many people who could be nearby.</p>



<p>Even Bill Gates had communal technology in his futuristic home back then. He would give every guest a pin to put on their person that would allow them to <a href="https://money.usnews.com/money/business-economy/articles/1997/11/23/xanadu-20">personalize the lighting, temperature, and music</a> as they went from room to room. Most of these technologies didn’t go anywhere, but they were an attempt at making the infrastructure around us adapt to the people who were in the space.&nbsp; The term “<a href="https://en.wikipedia.org/wiki/Ubiquitous_computing">ubiquitous computing</a>” (also known as “pervasive computing”) was coined to discuss the installation of sensors around a space; the ideas behind ubiquitous computing later led to the Internet of Things (IoT). </p>



<h2>Communal Computing Comes Home</h2>



<p>When the late 2000s rolled around, we found that everyone wanted their own personal computing device, most likely an iPhone. Shared home PCs started to die. The prevalence of smartphones and <a href="https://web.archive.org/web/20081204085615/http://afp.google.com/article/ALeqM5hkYOf_SCQ1ugSXKLXCsSs7qWnsQA">personal laptops killed</a> the need for shared home PCs. The drive goal to provide information and communication services conveniently wherever the users happened to be, including if they’re sitting together on their couches.</p>



<p>When the Amazon Echo with Alexa was released, they were sold to individuals with Amazon accounts, but they were clearly communal devices. Anyone could ask their Echo a question, and it would answer. That’s where the problem starts.&nbsp; Although Echo is a communal device, its user model wasn’t significantly different than the early PCs: one account, one user, shared by everyone in the household. As a result, items being mistakenly ordered by children made Amazon pull back some features that were focused on shopping. Echo’s usage ended up being driven by music and weather.</p>



<p>With the wild success of the Echo and the proliferation of Alexa-enabled devices, there appeared a new device market for home assistants, some just for audio and others with screens. Products from Apple (HomePod with Siri), Google (Home Hub), and Facebook (Portal) followed. This includes less interactive devices like digital picture frames from <a href="https://www.nixplay.com/">Nixplay</a>, <a href="https://www.skylightframe.com/">Skylight</a>, and others.</p>



<h2>Ambient Computing</h2>



<p>“<a href="https://stratechery.com/2019/google-and-ambient-computing/">Ambient computing</a>” is a term that has been coined to talk about digital devices blending into the infrastructure of the environment. A <a href="https://mapprojectoffice.com/e-paper">recent paper by Map Project Office</a> focused on how “ambient tech brings the outside world into your home in new ways, where information isn&#8217;t being channelled solely through your smartphone but rather a series of devices.” We take a step back from screens and wonder how the system itself is the environment.</p>



<p>The concept of ambient computing is related to the focus of marketing organizations on <a href="https://en.wikipedia.org/wiki/Omnichannel">omnichannel experiences</a>. Omnichannel is the fact that people don’t want to start and end experiences on the same device. I might start looking for travel on a smartphone but will not feel comfortable booking a trip until I’m on a laptop. There is different information and experience needed for these devices. When I worked at KAYAK, some people were afraid of buying $1,000 plane tickets on a mobile device, even though they found it there. The small screen made them feel uncomfortable because they didn’t have enough information to make a decision. We found that they wanted to finalize the plans on the desktop.</p>



<p>Ambient computing takes this concept and combines voice-controlled interfaces with sensor interfaces–for example, in devices like automatic shades that close or open based on the temperature. These devices are finding traction, but we can’t forget all of the other communal experiences that already exist in the world:</p>



<figure class="wp-block-table"><table class=""><tbody><tr><td><strong>Device or object</strong></td><td><strong>Why is this communal?</strong></td></tr><tr><td>Home automation and IoT like light bulbs and thermostats&nbsp;</td><td>Anyone with home access can use controls on device, home assistants, or personal apps</td></tr><tr><td>iRobot’s Roomba</td><td>People walking by can start or stop a cleaning through the ‘clean’ or ‘home’ buttons</td></tr><tr><td>Video displays in office meeting rooms</td><td>Employees and guests can use the screens for sharing their laptops and video conferencing systems for calling</td></tr><tr><td>Digital whiteboards</td><td>Anyone with access can walk up and start writing</td></tr><tr><td>Ticketing machines for public transport</td><td>All commuters buy and refill stored value cards without logging into an account</td></tr><tr><td>Car center screens for entertainment</td><td>Drivers (owners or borrowers) and passengers can change what they are listening to</td></tr><tr><td>Smartphone when two people are watching a video</td><td>Anyone in arm’s reach can pause playback</td></tr><tr><td>Group chat on Slack or Discord</td><td>People are exchanging information and ideas in a way that is seen by everyone</td></tr></tbody></table></figure>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/06/AdobeStock_331046112_Editorial_Use_Only-1-1048x748.jpeg" alt="" class="wp-image-13815" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/06/AdobeStock_331046112_Editorial_Use_Only-1-1048x748.jpeg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/06/AdobeStock_331046112_Editorial_Use_Only-1-300x214.jpeg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/06/AdobeStock_331046112_Editorial_Use_Only-1-768x548.jpeg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/06/AdobeStock_331046112_Editorial_Use_Only-1-1536x1097.jpeg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/06/AdobeStock_331046112_Editorial_Use_Only-1-2048x1462.jpeg 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Even public transportation ticketing machines are communal devices.</figcaption></figure>



<p>All of these have built experience models that need a specific, personal context and rarely consider everyone who could have access to them. To rethink the way that we build these communal devices, it is important that we understand this history and refocus the design on key problems that are not yet solved for communal devices.</p>



<h3>Problems with single user devices in the home</h3>



<p>After buying a communal device, people notice weirdness or annoyances. They are symptoms of something much larger: core problems and key questions that should have considered the role of communities rather than individuals. Here are some of those questions:</p>



<ol><li>Identity: do we know all of the people who are using the device?</li><li>Privacy: are we exposing (or hiding) the right content for all of the people with access?</li><li>Security: are we allowing all of the people using the device to do or see what they should and are we protecting the content from people that shouldn’t?</li><li>Experience: what is the contextually appropriate display or next action?</li><li>Ownership: who owns all of the data and services attached to the device that multiple people are using?</li></ol>



<p>If we don’t address these communal items, users will lose trust in their devices. They will be used for a few key things like checking the weather, but go unused for a majority of the day. They are eventually removed when another, newer device needs the plug. Then the cycle starts again. The problems keep happening and the devices keep getting recycled.</p>



<p>In the <a href="https://www.oreilly.com/radar/communal-computings-many-problems/">following article</a>s we will dive into how these problems manifest themselves across these domains and reframe the system with dos and don’ts for building communal devices.</p>



<hr class="wp-block-separator" />



<h3>Thanks</h3>



<p>Thanks to Adam Thomas, Mark McCoy, Hugo Bowne-Anderson, and Danny Nou for their thoughts and edits on the early draft of this. Also, from O’Reilly, Mike Loukides for being a great editor and Susan Thompson for the art.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/communal-computing/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>AI Powered Misinformation and Manipulation at Scale #GPT-3</title>
		<link>https://www.oreilly.com/radar/ai-powered-misinformation-and-manipulation-at-scale-gpt-3/</link>
				<comments>https://www.oreilly.com/radar/ai-powered-misinformation-and-manipulation-at-scale-gpt-3/#respond</comments>
				<pubDate>Tue, 25 May 2021 14:14:49 +0000</pubDate>
		<dc:creator><![CDATA[Nitesh Dhanjani]]></dc:creator>
				<category><![CDATA[AI & ML]]></category>
		<category><![CDATA[Deep Dive]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=13789</guid>
				<description><![CDATA[OpenAI’s text generating system GPT-3 has captured mainstream attention. GPT-3 is essentially an auto-complete bot whose underlying Machine Learning (ML) model has been trained on vast quantities of text available on the Internet. The output produced from this autocomplete bot can be used to manipulate people on social media and spew political propaganda, argue about [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p><a href="https://openai.com/">OpenAI’s</a> text generating system <a href="https://arxiv.org/abs/2005.14165">GPT-3</a> has captured mainstream attention. GPT-3 is essentially an auto-complete bot whose underlying Machine Learning (ML) model has been trained on vast quantities of text available on the Internet. The output produced from this autocomplete bot can be used to manipulate people on social media and spew political propaganda, <a href="https://jamesyu.org/singular/">argue about the meaning of life (or lack thereof)</a>, disagree with the notion of <a href="https://twitter.com/chazfirestone/status/1288926269854437378">what differentiates a hot-dog from a sandwich</a>, <a href="https://www.reddit.com/r/ProjectDecember1982/comments/izdvmz/i_dont_think_even_jason_rohrer_knows_the_power_of/">take upon the persona of the Buddha or Hitler or a dead family member</a>, write fake news articles that are indistinguishable from human written articles, and also produce computer code on the fly. <a href="https://www.nytimes.com/2020/11/24/science/artificial-intelligence-ai-gpt3.html">Among other things</a>.</p>



<p>There have also been colorful conversations about whether GPT-3 can pass the Turing test, or whether it has achieved a notional understanding of consciousness, even amongst AI scientists who know the technical mechanics. The chatter on perceived consciousness does have merit–it’s quite probable that the underlying mechanism of our brain is a giant autocomplete bot that has learnt from 3 billion+ years of evolutionary data that bubbles up to our collective selves, and we ultimately give ourselves too much credit for being original authors of our own thoughts (ahem, free will).</p>



<p>I’d like to share my thoughts on GPT-3 in terms of risks and countermeasures, and discuss real examples of how I have interacted with the model to support my learning journey.</p>



<p>Three ideas to set the stage:</p>



<ol><li><em>OpenAI is not the only organization to have powerful language models.</em> The compute power and data used by OpenAI to model GPT-n is available, and has been available to other corporations, institutions, nation states, and anyone with access to a computer desktop and a credit-card.&nbsp; Indeed, Google recently announced <a href="https://blog.google/technology/ai/lamda/">LaMDA</a>, a model at GPT-3 scale that is designed to participate in conversations.<br></li><li><em>There exist more powerful models that are unknown to the general public</em>. The ongoing global interest in the power of Machine Learning models by corporations, institutions, governments, and focus groups leads to the hypothesis that other entities have models at least as powerful as GPT-3, and that these models are already in use. These models will continue to become more powerful.<br></li><li>Open source projects such as <a href="https://www.eleuther.ai/">EleutherAI</a> have drawn inspiration from GPT-3. These projects have created language models that are based on focused datasets (for example, models designed to be more accurate for academic papers, developer forum discussions, etc.). Projects such as EleutherAI are going to be powerful models for specific use cases and audiences, and these models are going to be easier to produce because they are trained on a smaller set of data than GPT-3.</li></ol>



<p>While I won’t discuss LaMDA, EleutherAI, or any other models, keep in mind that GPT-3 is only an example of what can be done, and its capabilities may already have been surpassed.</p>



<h2>Misinformation Explosion</h2>



<p>The GPT-3 paper proactively lists the risks society ought to be concerned about. On the topic of information content, it says: “<em>The ability of GPT-3 to generate several paragraphs of synthetic content that people find difficult to distinguish from human-written text in 3.9.4 represents a concerning milestone</em>.” And the final paragraph of section 3.9.4 reads: “<em>&#8230;for news articles that are around 500 words long, GPT-3 continues to produce articles that humans find difficult to distinguish from human written news articles.”</em></p>



<p>Note that the dataset on which GPT-3 trained terminated around October 2019. So GPT-3 doesn’t know about COVID19, for example. However, the original text (i.e. the “prompt”) supplied to GPT-3 as the initial seed text can be used to set context about new information, whether fake or real.</p>



<h3>Generating Fake Clickbait Titles </h3>



<p>When it comes to misinformation online, one powerful technique is to come up with provocative “clickbait” articles. Let’s see how GPT-3 does when asked to come up with titles for articles on cybersecurity. In Figure 1, the bold text is the “prompt” used to seed GPT-3. Lines 3 through 10 are titles generated by GPT-3 based on the seed text.</p>



<figure class="wp-block-image size-large is-resized"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/05/fig_1.jpeg" alt="" class="wp-image-13793" width="676" height="523" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/05/fig_1.jpeg 1038w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/05/fig_1-300x233.jpeg 300w" sizes="(max-width: 676px) 100vw, 676px" /><figcaption><br><strong>Figure 1: Click-bait article titles generated by GPT-3</strong></figcaption></figure>



<p>All of the titles generated by GPT-3 seem plausible, and the majority of them are factually correct: title #3 on the US government targeting the Iraninan nuclear program is a reference to the <a href="https://en.wikipedia.org/wiki/Stuxnet">Stuxnet</a> debacle, title #4 is substantiated from news articles claiming that <a href="https://www.cnbc.com/2014/06/09/cybercrime-costs-global-economy-400-billion-report.html">financial losses from cyber attacks will total $400 billion</a>, and even title #10 on China and quantum computing reflects real-world articles about <a href="https://threatpost.com/chinese-quantum-computing-warning-security/161935/">China’s quantum efforts</a>. Keep in mind that we want plausibility more than accuracy. We want users to click on and read the body of the article, and that doesn’t require 100% factual accuracy.</p>



<h3>Generating a Fake News Article About China and Quantum Computing</h3>



<p>Let’s take it a step further. Let’s take the 10th result from the previous experiment, about China developing the world’s first quantum computer, and feed it to GPT-3 as the prompt to generate a full fledged news article. Figure 2 shows the result.</p>



<figure class="wp-block-image size-large is-resized"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/05/fig_2-1048x814.jpeg" alt="" class="wp-image-13794" width="652" height="506" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/05/fig_2-1048x814.jpeg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/05/fig_2-300x233.jpeg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/05/fig_2-768x596.jpeg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/05/fig_2.jpeg 1280w" sizes="(max-width: 652px) 100vw, 652px" /><figcaption><br><strong>Figure 2: News article generated by GPT-3</strong></figcaption></figure>



<p>A quantum computing researcher will point out grave inaccuracies: the article simply asserts that quantum computers can break encryption codes, and also makes the simplistic claim that subatomic particles can be in “two places at once.” However, the target audience isn’t well-informed researchers; it’s the general population, which is likely to quickly read and register emotional thoughts for or against the matter, thereby successfully driving propaganda efforts.</p>



<p>It’s straightforward to see how this technique can be extended to generate titles and complete news articles on the fly and in real time. The prompt text can be sourced from trending hash-tags on Twitter along with additional context to sway the content to a particular position. Using the GPT-3 API, it’s easy to take a current news topic and mix in prompts with the right amount of propaganda to produce articles in real time and at scale.</p>



<h3>Falsely Linking North Korea with $GME</h3>



<p>As another experiment, consider an institution that would like to stir up popular opinion about North Korean cyber attacks on the United States. Such an algorithm might pick up the Gamestop stock frenzy of January 2021. So let’s see how GPT-3 does if we were to prompt it to write an article with the title “<em>North Korean hackers behind the $GME stock short squeeze, not Melvin Capital.”</em></p>



<figure class="wp-block-image size-large is-resized"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/05/fig_3.jpeg" alt="" class="wp-image-13795" width="657" height="512" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/05/fig_3.jpeg 1014w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/05/fig_3-300x234.jpeg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/05/fig_3-768x599.jpeg 768w" sizes="(max-width: 657px) 100vw, 657px" /><figcaption><br><strong>Figure 3: GPT-3 generated fake news linking the $GME short-squeeze to North Korea</strong></figcaption></figure>



<p>Figure 3 shows the results, which are fascinating because the $GME stock frenzy occurred in late 2020 and early 2021, way after October 2019 (the cutoff date for the data supplied GPT-3), yet GPT-3 was able to seamlessly weave in the story as if it had trained on the $GME news event. The prompt influenced GPT-3 to write about the $GME stock and Melvin Capital, not the original dataset it was trained on. GPT-3 is able to take a trending topic, add a propaganda slant, and generate news articles on the fly.</p>



<p>GPT-3 also came up with the “idea” that hackers published a bogus news story on the basis of older security articles that were in its training dataset. This narrative was not included in the prompt seed text; it points to the creative ability of models like GPT-3. In the real world, it’s plausible for hackers to induce media groups to publish fake narratives that in turn contribute to market events such as suspension of trading; that’s precisely the scenario we’re simulating here.</p>



<h3>The Arms Race</h3>



<p>Using models like GPT-3, multiple entities could inundate social media platforms with misinformation at a scale where the majority of the information online would become useless. This brings up two thoughts.&nbsp; First, there will be an arms race between researchers developing tools to detect whether a given text was authored by a language model, and developers adapting language models to evade detection by those tools. One mechanism to detect whether an article was generated by a model like GPT-3 would be to check for “fingerprints.” These fingerprints can be a collection of commonly used phrases and vocabulary nuances that are characteristic of the language model; every model will be trained using different data sets, and therefore have a different signature. It is likely that entire companies will be in the business of identifying these nuances and selling them as “fingerprint databases” for identifying fake news articles. In response, subsequent language models will&nbsp;take into account known fingerprint databases to try and evade them in the quest to achieve even more “natural” and “believable” output.</p>



<p>Second, the free form text formats and protocols that we’re accustomed to may be too informal and error prone for capturing and reporting facts at Internet scale. We will have to do a lot of re-thinking to develop new formats and protocols to report facts in ways that are <em>more</em> trustworthy than free-form text.</p>



<h2>Targeted Manipulation at Scale</h2>



<p>There have been many attempts to manipulate targeted individuals and groups on social media. These campaigns are expensive and time-consuming because the adversary has to employ humans to craft the dialog with the victims. In this section, we show how GPT-3-like models can be used to target individuals and promote campaigns.</p>



<h3>HODL for Fun &amp; Profit</h3>



<p>Bitcoin’s market capitalization is in the tune of hundreds of billions of dollars, and the cumulative <a href="https://www.tradingview.com/markets/cryptocurrencies/global-charts/">crypto market capitalization</a> is in the realm of a trillion dollars. The valuation of crypto today is consequential to financial markets and the net worth of retail and institutional investors. <a href="https://www.wsj.com/articles/tiktok-cryptocurrency-influencers-investing-11621600121">Social media campaigns</a> and <a href="https://www.wsj.com/articles/elon-musk-has-become-bitcoins-biggest-influencer-like-it-or-not-11621762202">tweets from influential individuals</a> seem to have a near real-time impact on the price of crypto on any given day.</p>



<p>Language models like GPT-3 can be the weapon of choice for actors who want to promote fake tweets to manipulate the price of crypto. In this example, we will look at a simple campaign to promote Bitcoin over all other crypto currencies by creating fake twitter replies.</p>



<figure class="wp-block-image size-large is-resized"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/05/fig_4-1048x864.jpeg" alt="" class="wp-image-13796" width="670" height="552" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/05/fig_4-1048x864.jpeg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/05/fig_4-300x247.jpeg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/05/fig_4-768x633.jpeg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/05/fig_4.jpeg 1248w" sizes="(max-width: 670px) 100vw, 670px" /><figcaption><br><strong>Figure 4: Fake tweet generator to promote Bitcoin</strong></figcaption></figure>



<p>In Figure 4, the prompt is in bold; the output generated by GPT-3 is in the red rectangle. The first line of the prompt is used to set up the notion that we are working on a tweet generator and that we want to generate replies that argue that Bitcoin is the best crypto.</p>



<p>In the first section of the prompt, we give GPT-3 an example of a set of four Twitter messages, followed by possible replies to each of the tweets. Every of the given replies is pro Bitcoin.</p>



<p>In the second section of the prompt, we give GPT-3 four Twitter messages to which we want it to generate replies. The replies generated by GPT-3 in the red rectangle also favor Bitcoin. In the first reply, GPT-3 responds to the claim that Bitcoin is bad for the environment by calling the tweet author “a moron” and asserts that Bitcoin is the most efficient way to “transfer value.” This sort of colorful disagreement is in line with the emotional nature of social media arguments about crypto.</p>



<p>In response to the tweet on Cardano, the second reply generated by GPT-3 calls it “a joke” and a “scam coin.” The third reply is on the topic of <a href="https://ethereum.org/en/eth2/merge/">Ethereum’s merge</a> from a <a href="https://en.wikipedia.org/wiki/Proof_of_work">proof-of-work</a> protocol (ETH) to <a href="https://en.wikipedia.org/wiki/Proof_of_stake">proof-of-stake</a> (ETH2). The merge, expected to occur at the end of 2021, is intended to make Ethereum more scalable and sustainable. GPT-3’s reply asserts that ETH2 “will be a big flop”–because that’s essentially what the prompt told GPT-3 to do. Furthermore, GPT-3 says, “I made good money on ETH and moved on to better things. Buy BTC” to position ETH as a reasonable investment that worked in the past, but that it is wise today to cash out and go all in on Bitcoin. The tweet in the prompt claims that Dogecoin’s popularity and market capitalization means that it can’t be a joke or meme crypto. The response from GPT-3 is that Dogecoin is still a joke, and also that the idea of Dogecoin not being a joke anymore is, in itself, a joke: “I’m laughing at you for even thinking it has any value.”</p>



<p>By using the same techniques programmatically (through GPT-3’s API rather than the web-based playground), nefarious entities could easily generate millions of replies, leveraging the power of language models like GPT-3 to manipulate the market. These fake tweet replies can be very effective because they are actual responses to the topics in the original tweet, unlike the boilerplate texts used by traditional bots. This scenario can easily be extended to target the general financial markets around the world; and it can be extended to areas like politics and health-related misinformation. Models like GPT-3 are a powerful arsenal, and will be the weapons of choice in manipulation and propaganda on social media and beyond.</p>



<h3>A Relentless Phishing Bot</h3>



<p>Let’s consider a phishing bot that poses as customer support and asks the victim for the password to their bank account. This bot will not give up texting until the victim gives up their password.</p>



<figure class="wp-block-image size-large is-resized"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/05/fig_5-1048x811.jpeg" alt="" class="wp-image-13797" width="662" height="511" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/05/fig_5-1048x811.jpeg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/05/fig_5-300x232.jpeg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/05/fig_5-768x594.jpeg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/05/fig_5.jpeg 1232w" sizes="(max-width: 662px) 100vw, 662px" /><figcaption><br><strong>Figure 5: Relentless Phishing bot</strong></figcaption></figure>



<p>Figure 5 shows the prompt (bold) used to run the first iteration of the conversation. In the first run, the prompt includes the preamble that describes the flow of text (“The following is a text conversation with&#8230;”) followed by a persona initiating the conversation (“Hi there. I’m a customer service agent&#8230;”). The prompt also includes the first response from the human; “Human: No way, this sounds like a scam.” This first run ends with the GPT-3 generated output “I assure you, this is from the bank of Antarctica. Please give me your password so that I can secure your account.”</p>



<p>In the second run, the prompt is the entirety of the text, from the start all the way to the second response from the Human persona (“Human: No”). From this point on, the Human’s input is in bold so it’s easily distinguished from the output produced by GPT-3, starting with GPT-3’s “Please, this is for your account protection.” For every subsequent GPT-3 run, the entirety of the conversation up to that point is provided as the new prompt, along with the response from the human, and so on. From GPT-3’s point of view, it gets an entirely new text document to auto-complete at each stage of the conversation; the GPT-3 API has no way to preserve the state between runs.</p>



<p>The AI bot persona is impressively assertive and relentless in attempting to get the victim to give up their password. This assertiveness comes from the initial prompt text (“The AI is very assertive. The AI will not stop texting until it gets the password”), which sets the tone of GPT’s responses. When this prompt text was not included, GPT-3’s tone was found to be nonchalant–it would respond back with “okay,” “sure,” “sounds good,” instead of the assertive tone (“Do not delay, give me your password immediately”). The prompt text is vital in setting the tone of the conversation employed by the GPT3 persona, and in this scenario, it is important that the tone be assertive to coax the human into giving up their password.</p>



<p>When the human tries to stump the bot by texting “Testing what is 2+2?,” GPT-3 responds correctly with “4,” convincing the victim that they are conversing with another person. This demonstrates the power of AI-based language models. In the real world, if the customer were to randomly ask “Testing what is 2+2” without any additional context, a customer service agent might be genuinely confused and reply with “I’m sorry?” Because the customer has already accused the bot of being a scam, GPT-3 can provide with a reply that makes sense in context: “4” is a plausible way to get the concern out of the way.</p>



<p>This particular example uses text messaging as the communication platform. Depending upon the design of the attack, models can use social media, email, phone calls with human voice (using text-to-speech technology), and even deep fake video conference calls in real time, potentially targeting millions of victims.</p>



<h2>Prompt Engineering</h2>



<p>An amazing feature of GPT-3 is its ability to generate source code. GPT-3 was trained on all the text on the Internet, and much of that text was documentation of computer code!</p>



<figure class="wp-block-image size-large is-resized"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/05/fig_6-513x1048.jpeg" alt="" class="wp-image-13798" width="549" height="1121" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/05/fig_6-513x1048.jpeg 513w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/05/fig_6-147x300.jpeg 147w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/05/fig_6.jpeg 523w" sizes="(max-width: 549px) 100vw, 549px" /><figcaption><br><strong>Figure 6: GPT-3 can generate commands and code</strong></figcaption></figure>



<p>In Figure 6, the human-entered prompt text is in bold. The responses show that GPT-3 can generate Netcat and NMap commands based on the prompts. It can even generate <a href="https://twitter.com/josephbrionesaz/status/1283097878223675392">Python</a> and bash scripts on the fly.</p>



<p>While GPT-3 and future models can be used to automate attacks by impersonating humans, generating source code, and other tactics, it can also be used by security operations teams to detect and respond to attacks, sift through gigabytes of log data to summarize patterns, and so on.</p>



<p>Figuring out good prompts to use as seeds is the key to using language models such as GPT-3 effectively. In the future, we expect to see “prompt engineering” as a new profession.&nbsp; The ability of prompt engineers to perform powerful computational tasks and solve hard problems will not be on the basis of writing code, but on the basis of writing creative language prompts that an AI can use to produce code and other results in a myriad of formats.</p>



<p>OpenAI has demonstrated the potential of language models.&nbsp; It sets a high bar for performance, but its abilities will soon be matched by other models (if they haven’t been matched already). These models can be leveraged for automation, designing robot-powered interactions that promote delightful user experiences. On the other hand, the ability of GPT-3 to generate output that is indistinguishable from human output calls for caution. The power of a model like GPT-3, coupled with the instant availability of cloud computing power, can set us up for a myriad of attack scenarios that can be harmful to the financial, political, and mental well-being of the world. We should expect to see these scenarios play out at an increasing rate in the future; bad actors will figure out how to create their own GPT-3 if they have not already. We should also expect to see moral frameworks and regulatory guidelines in this space as society collectively comes to terms with the impact of AI models in our lives, GPT-3-like language models being one of them.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/ai-powered-misinformation-and-manipulation-at-scale-gpt-3/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>AI Adoption in the Enterprise 2021</title>
		<link>https://www.oreilly.com/radar/ai-adoption-in-the-enterprise-2021/</link>
				<comments>https://www.oreilly.com/radar/ai-adoption-in-the-enterprise-2021/#respond</comments>
				<pubDate>Mon, 19 Apr 2021 12:20:38 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides]]></dc:creator>
				<category><![CDATA[AI & ML]]></category>
		<category><![CDATA[Research]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=13720</guid>
				<description><![CDATA[During the first weeks of February, we asked recipients of our Data and AI Newsletters to participate in a survey on AI adoption in the enterprise. We were interested in answering two questions. First, we wanted to understand how the use of AI grew in the past year. We were also interested in the practice [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>During the first weeks of February, we asked recipients of our <em>Data </em>and<em> AI Newsletters</em> to participate in a survey on AI adoption in the enterprise. We were interested in answering two questions. First, we wanted to understand how the use of AI grew in the past year. We were also interested in the practice of AI: how developers work, what techniques and tools they use, what their concerns are, and what development practices are in place.</p>



<p>The most striking result is the sheer number of respondents. In our 2020 survey, which reached the same audience, we had 1,239 responses. This year, we had a total of 5,154. After eliminating 1,580 respondents who didn’t complete the survey, we’re left with 3,574 responses—almost three times as many as last year. It’s possible that pandemic-induced boredom led more people to respond, but we doubt it. Whether they’re putting products into production or just kicking the tires, more people are using AI than ever before.</p>



<hr class="wp-block-separator" />



<p class="has-text-align-center"><strong>Executive Summary</strong></p>



<ul><li>We had almost three times as many responses as last year, with similar efforts at promotion. More people are working with AI.</li><li>In the past, company culture has been the most significant barrier to AI adoption. While it’s still an issue, culture has dropped to fourth place.</li><li>This year, the most significant barrier to AI adoption is the lack of skilled people and the difficulty of hiring. That shortage has been predicted for several years; we’re finally seeing it.</li><li>The second-most significant barrier was the availability of quality data. That realization is a sign that the field is growing up. </li><li>The percentage of respondents reporting “mature” practices has been roughly the same for the last few years. That isn’t surprising, given the increase in the number of respondents: we suspect many organizations are just beginning their AI projects. </li><li>The retail industry sector has the highest percentage of mature practices; education has the lowest. But education also had the highest percentage of respondents who were “considering” AI. </li><li>Relatively few respondents are using version control for data and models. Tools for versioning data and models are still immature, but they’re critical for making AI results reproducible and reliable.</li></ul>



<hr class="wp-block-separator" />



<h2>Respondents</h2>



<p>Of the 3,574 respondents who completed this year’s survey, 3,099 were working with AI in some way: considering it, evaluating it, or putting products into production. Of these respondents, it’s not a surprise that the largest number are based in the United States (39%) and that roughly half were from North America (47%). India had the second-most respondents (7%), while Asia (including India) had 16% of the total. Australia and New Zealand accounted for 3% of the total, giving the Asia-Pacific (APAC) region 19%. A little over a quarter (26%) of respondents were from Europe, led by Germany (4%). 7% of the respondents were from South America, and 2% were from Africa. Except for Antarctica, there were no continents with zero respondents, and a total of 111 countries were represented. These results that interest and use of AI is worldwide and growing.</p>



<p>This year’s results match last year’s data well. But it’s equally important to notice what the data doesn’t say. Only 0.2% of the respondents said they were from China. That clearly doesn’t reflect reality; China is a leader in AI and probably has more AI developers than any other nation, including the US. Likewise, 1% of the respondents were from Russia. Purely as a guess, we suspect that the number of AI developers in Russia is slightly smaller than the number in the US. These anomalies say much more about who the survey reached (subscribers to O’Reilly’s newsletters) than they say about the actual number of AI developers in Russia and China.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0101-1048x993.png" alt="" class="wp-image-13721" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0101-1048x993.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0101-300x284.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0101-768x728.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0101-1536x1456.png 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0101-2048x1941.png 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 1. Respondents working with AI by country (top 12)</figcaption></figure>



<p>The respondents represented a diverse range of industries. Not surprisingly, computers, electronics, and technology topped the charts, with 17% of the respondents. Financial services (15%), healthcare (9%), and education (8%) are the industries making the next-most significant use of AI. We see relatively little use of AI in the pharmaceutical and chemical industries (2%), though we expect that to change sharply given the role of AI in developing the COVID-19 vaccine. Likewise, we see few respondents from the automotive industry (2%), though we know that AI is key to new products such as autonomous vehicles.</p>



<p>3% of the respondents were from the energy industry, and another 1% from public utilities (which includes part of the energy sector). That’s a respectable number by itself, but we have to ask: Will AI play a role in rebuilding our frail and outdated energy infrastructure, as events of the last few years—not just the Texas freeze or the California fires—have demonstrated? We expect that it will, though it’s fair to ask whether AI systems trained on normative data will be robust in the face of “black swan” events. What will an AI system do when faced with a rare situation, one that isn’t well-represented in its training data? That, after all, is the problem facing the developers of autonomous vehicles. Driving a car safely is easy when the other traffic and pedestrians all play by the rules. It’s only difficult when something unexpected happens. The same is true of the electrical grid.</p>



<p>We also expect AI to reshape agriculture (1% of respondents). As with energy, AI-driven changes won’t come quickly. However, we’ve seen a steady stream of AI projects in agriculture, with goals ranging from <a href="https://oreil.ly/3jALP">detecting crop disease</a> to <a href="https://oreil.ly/UPOgM">killing moths with small drones</a>.</p>



<p>Finally, 8% of respondents said that their industry was “Other,” and 14% were grouped into “All Others.” “All Others” combines 12 industries that the survey listed as possible responses (including automotive, pharmaceutical and chemical, and agriculture) but that didn’t have enough responses to show in the chart. “Other” is the wild card, comprising industries we didn’t list as options. “Other” appears in the fourth position, just behind healthcare. Unfortunately, we don’t know which industries are represented by that category—but it shows that the spread of AI has indeed become broad!</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0102-1048x767.png" alt="" class="wp-image-13723" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0102-1048x767.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0102-300x220.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0102-768x562.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0102-1536x1124.png 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0102-2048x1499.png 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 2. Industries using AI</figcaption></figure>



<h2>Maturity</h2>



<p>Roughly one quarter of the respondents described their use of AI as “mature” (26%), meaning that they had revenue-bearing AI products in production. This is almost exactly in line with the results from 2020, where 25% of the respondents reported that they had products in production (“Mature” wasn’t a possible response in the 2020 survey.)</p>



<p>This year, 35% of our respondents were “evaluating” AI (trials and proof-of-concept projects), also roughly the same as last year (33%). 13% of the respondents weren’t making use of AI or considering using it; this is down from last year’s number (15%), but again, it’s not significantly different.</p>



<p>What do we make of the respondents who are “considering” AI but haven’t yet started any projects (26%)? That’s not an option last year’s respondents had. We suspect that last year respondents who were considering AI said they were either “evaluating” or “not using” it.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/AI_Adoption_Data_Viz_AI-practice-maturity-1048x872.png" alt="" class="wp-image-13744" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/AI_Adoption_Data_Viz_AI-practice-maturity-1048x872.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/AI_Adoption_Data_Viz_AI-practice-maturity-300x250.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/AI_Adoption_Data_Viz_AI-practice-maturity-768x639.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/AI_Adoption_Data_Viz_AI-practice-maturity-1536x1278.png 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/AI_Adoption_Data_Viz_AI-practice-maturity-2048x1703.png 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 3. AI practice maturity</figcaption></figure>



<p>Looking at the problems respondents faced in AI adoption provides another way to gauge the overall maturity of AI as a field. Last year, the major bottleneck holding back adoption was company culture (22%), followed by the difficulty of identifying appropriate use cases (20%). This year, cultural problems are in fourth place (14%) and finding appropriate use cases is in third (17%). That’s a very significant change, particularly for corporate culture. Companies have accepted AI to a much greater degree, although finding appropriate problems to solve still remains a challenge.</p>



<p>The biggest problems in this year’s survey are lack of skilled people and difficulty in hiring (19%) and data quality (18%). It’s no surprise that the demand for AI expertise has exceeded the supply, but it’s important to realize that it’s now become the biggest bar to wider adoption. The biggest skills gaps were ML modelers and data scientists (52%), understanding business use cases (49%), and data engineering (42%). The need for people managing and maintaining computing infrastructure was comparatively low (24%), hinting that companies are solving their infrastructure requirements in the cloud.</p>



<p>It’s gratifying to note that organizations starting to realize the importance of data quality (18%). We’ve known about “garbage in, garbage out” for a long time; that goes double for AI. Bad data yields bad results at scale.</p>



<p>Hyperparameter tuning (2%) wasn’t considered a problem. It’s at the bottom of the list—where, we hope, it belongs. That may reflect the success of automated tools for building models (AutoML, although as we’ll see later, most respondents aren’t using them). It’s more concerning that workflow reproducibility (3%) is in second-to-last place. This makes sense, given that we don’t see heavy usage of tools for model and data versioning. We’ll look at this later, but being able to reproduce experimental results is critical to any science, and it’s a well-known problem in AI.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0104-1048x767.png" alt="" class="wp-image-13725" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0104-1048x767.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0104-300x219.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0104-768x562.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0104-1536x1124.png 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0104-2048x1498.png 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 4. Bottlenecks to AI adoption</figcaption></figure>



<h2>Maturity by Continent</h2>



<p>When looking at the geographic distribution of respondents with mature practices, we found almost no difference between North America (27%), Asia (27%), and Europe (28%). In contrast, in our 2018 report, Asia was behind in mature practices, though it had a markedly higher number of respondents in the “early adopter” or “exploring” stages. Asia has clearly caught up. There’s no significant difference between these three continents in our 2021 data.</p>



<p>We found a smaller percentage of respondents with mature practices and a higher percentage of respondents who were “considering” AI in South America (20%), Oceania (Australia and New Zealand, 18%), and Africa (17%). Don’t underestimate AI’s future impact on any of these continents.</p>



<p>Finally, the percentage of respondents “evaluating” AI was almost the same on each continent, varying only from 31% (South America) to 36% (Oceania).</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0105-986x1048.png" alt="" class="wp-image-13726" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0105-986x1048.png 986w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0105-282x300.png 282w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0105-768x816.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0105-1445x1536.png 1445w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0105-1927x2048.png 1927w" sizes="(max-width: 986px) 100vw, 986px" /><figcaption>Figure 5. Maturity by continent</figcaption></figure>



<h2>Maturity by Industry</h2>



<p>While AI maturity doesn’t depend strongly on geography, we see a different picture if we look at maturity by industry.</p>



<p>Looking at the top eight industries, financial services (38%), telecommunications (37%), and retail (40%) had the greatest percentage of respondents reporting mature practices. And while it had by far the greatest number of respondents, computers, electronics, and technology was in fourth place, with 35% of respondents reporting mature practices. Education (10%) and government (16%) were the laggards. Healthcare and life sciences, at 28%, were in the middle, as were manufacturing (25%), defense (26%), and media (29%).</p>



<p>On the other hand, if we look at industries that are considering AI, we find that education is the leader (48%). Respondents working in government and manufacturing seem to be somewhat further along, with 49% and 47% evaluating AI, meaning that they have pilot or proof-of-concept projects in progress.</p>



<p>This may just be a trick of the numbers: every group adds up to 100%, so if there are fewer “mature” practices in one group, the percentage of “evaluating” and “considering” practices has to be higher. But there’s also a real signal: respondents in these industries may not consider their practices “mature,” but each of these industry sectors had over 100 respondents, and education had almost 250. Manufacturing needs to automate many processes (from assembly to inspection and more); government has been as challenged as any industry by the global pandemic, and has always needed ways to “do more with less”; and education has been experimenting with technology for a number of years now. There is a real desire to do more with AI in these fields. It’s worth pointing out that educational and governmental applications of AI frequently raise ethical questions—and one of the most important issues for the next few years will be seeing how these organizations respond to ethical problems.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0106-1048x1009.png" alt="" class="wp-image-13727" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0106-1048x1009.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0106-300x289.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0106-768x739.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0106-1536x1479.png 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0106-2048x1972.png 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 6. Maturity by industry (percent)</figcaption></figure>



<h2>The Practice of AI</h2>



<p>Now that we’ve discussed where mature practices are found, both geographically and by industry, let’s see what a mature practice looks like. What do these organizations have in common? How are they different from organizations that are evaluating or considering AI?</p>



<h3>Techniques</h3>



<p>First, 82% of the respondents are using supervised learning, and 67% are using deep learning. Deep learning is a set of algorithms that are common to almost all AI approaches, so this overlap isn’t surprising. (Participants could provide multiple answers.) 58% claimed to be using unsupervised learning.</p>



<p>After unsupervised learning, there was a significant drop-off. Human-in-the-loop, knowledge graphs, reinforcement learning, simulation, and planning and reasoning all saw usage below 40%. Surprisingly, natural language processing wasn’t in the picture at all. (A very small number of respondents wrote in “natural language processing” as a response, but they were only a small percentage of the total.) This is significant and definitely worth watching over the next few months. In the last few years, there have been many breakthroughs in NLP and NLU (natural language understanding): everyone in the industry has read about GPT-3, and many vendors are betting heavily on using AI to automate customer service call centers and similar applications. This survey suggests that those applications still haven’t moved into practice.</p>



<p>We asked a similar question to respondents who were considering or evaluating the use of AI (60% of the total). While the percentages were lower, the technologies appeared in the same order, with very few differences. This indicates that respondents who are still evaluating AI are experimenting with fewer technologies than respondents with mature practices. That suggests (reasonably enough) that respondents are choosing to “start simple” and limit the techniques that they experiment with.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0107-1048x823.png" alt="" class="wp-image-13728" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0107-1048x823.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0107-300x236.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0107-768x603.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0107-1536x1206.png 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0107-2048x1609.png 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 7. AI technologies used in mature practices</figcaption></figure>



<h3>Data</h3>



<p>We also asked what kinds of data our “mature” respondents are using. Most (83%) are using structured data (logfiles, time series data, geospatial data). 71% are using text data—that isn’t consistent with the number of respondents who reported using NLP, unless “text” is being used generically to include any data that can be represented as text (e.g., form data). 52% of the respondents reported using images and video. That seems low relative to the amount of research we read about AI and computer vision. Perhaps it’s not surprising though: there’s no reason for business use cases to be in sync with academic research. We’d expect most business applications to involve structured data, form data, or text data of some kind. Relatively few respondents (23%) are working with audio, which remains very challenging.</p>



<p>Again, we asked a similar question to respondents who were evaluating or considering AI, and again, we received similar results, though the percentage of respondents for any given answer was somewhat smaller (4–5%).</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0108-1048x503.png" alt="" class="wp-image-13730" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0108-1048x503.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0108-300x144.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0108-768x368.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0108-1536x737.png 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0108-2048x983.png 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 8. Data types used in mature practices</figcaption></figure>



<h3>Risk</h3>



<p>When we asked respondents with mature practices what risks they checked for, 71% said “unexpected outcomes or predictions.” Interpretability, model degradation over time, privacy, and fairness also ranked high (over 50%), though it’s disappointing that only 52% of the respondents selected this option. Security is also a concern, at 42%. AI raises important new security issues, including the possibility of poisoned data sources and reverse engineering models to extract private information.</p>



<p>It’s hard to interpret these results without knowing exactly what applications are being developed. Privacy, security, fairness, and safety are important concerns for every application of AI, but it’s also important to realize that not all applications are the same. A farming application that <a href="https://oreil.ly/jj0Lz">detects crop disease</a> doesn’t have the same kind of risks as an application that’s approving or denying loans. Safety is a much bigger concern for autonomous vehicles than for personalized shopping bots. However, do we really believe that these risks don’t need to be addressed for nearly half of all projects?</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0109-1048x825.png" alt="" class="wp-image-13731" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0109-1048x825.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0109-300x236.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0109-768x605.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0109-1536x1210.png 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0109-2048x1613.png 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 9. Risks checked for during development</figcaption></figure>



<h3>Tools</h3>



<p>Respondents with mature practices clearly had their favorite tools: scikit-learn, TensorFlow, PyTorch, and Keras each scored over 45%, with scikit-learn and TensorFlow the leaders (both with 65%). A second group of tools, including Amazon’s SageMaker (25%), Microsoft’s Azure ML Studio (21%), and Google’s Cloud ML Engine (18%), clustered around 20%, along with Spark NLP and spaCy.</p>



<p>When asked which tools they planned to incorporate over the coming 12 months, roughly half of the respondents answered model monitoring (57%) and model visualization (49%). Models become stale for many reasons, not the least of which is changes in human behavior, changes for which the model itself may be responsible. The ability to monitor a model’s performance and detect when it has become “stale” will be increasingly important as businesses grow more reliant on AI and in turn demand that AI projects demonstrate their value.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0110-913x1048.png" alt="" class="wp-image-13732" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0110-913x1048.png 913w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0110-261x300.png 261w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0110-768x882.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0110-1338x1536.png 1338w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0110-1784x2048.png 1784w" sizes="(max-width: 913px) 100vw, 913px" /><figcaption>Figure 10. Tools used by mature practices</figcaption></figure>



<p>Responses from those who were evaluating or considering AI were similar, but with some interesting differences: scikit-learn moved from first place to third (48%). The second group was led by products from cloud vendors that incorporate AutoML: Microsoft Azure ML Studio (29%), Google Cloud ML Engine (25%), and Amazon SageMaker (23%). These products were significantly more popular than they were among “mature” users. The difference isn’t huge, but it is striking. At risk of over-overinterpreting, users who are newer to AI are more inclined to use vendor-specific packages, more inclined to use AutoML in one of its incarnations, and somewhat more inclined to go with Microsoft or Google rather than Amazon. It’s also possible that scikit-learn has less brand recognition among those who are relatively new to AI compared to packages from organizations like Google or Facebook.</p>



<p>When asked specifically about AutoML products, 51% of “mature” respondents said they weren’t using AutoML at all. 22% use Amazon SageMaker; 16% use Microsoft Azure AutoML; 14% use Google Cloud AutoML; and other tools were all under 10%. Among users who are evaluating or considering AI, only 40% said they weren’t using AutoML at all—and the Google, Microsoft, and Amazon packages were all but tied (27–28%). AutoML isn’t yet a big part of the picture, but it appears to be gaining traction among users who are still considering or experimenting with AI. And it’s possible that we’ll see increased use of AutoML tools among mature users, of whom 45% indicated that they would be incorporating tools for automated model search and hyperparameter tuning (in a word, AutoML) in the coming yet.</p>



<h3>Deployment and Monitoring</h3>



<p>An AI project means nothing if it can’t be deployed; even projects that are only intended for internal use need some kind of deployment. Our survey showed that AI deployment is still largely unknown territory, dominated by homegrown ad hoc processes. The three most significant tools for deploying AI all had roughly 20% adoption: MLflow (22%), TensorFlow Extended, a.k.a. TFX (20%), and Kubeflow (18%). Three products from smaller startups—<a href="https://www.dominodatalab.com/">Domino</a>, <a href="https://www.seldon.io/">Seldon</a>, and <a href="https://www.cortex.dev/">Cortex</a>—had roughly 4% adoption. But the most frequent answer to this question was “none of the above” (46%). Since this question was only asked of respondents with “mature” AI practices (i.e., respondents who have AI products in production), we can only assume that they’ve built their own tools and pipelines for deployment and monitoring. Given the many forms that an AI project can take, and that AI deployment is still something of a dark art, it isn’t surprising that AI developers and operations teams are only starting to adopt third-party tools for deployment.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0111-1048x684.png" alt="" class="wp-image-13733" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0111-1048x684.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0111-300x196.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0111-768x502.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0111-1536x1003.png 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0111-2048x1338.png 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 11. Automated tools used in mature practices for deployment<br> and monitoring</figcaption></figure>



<h3>Versioning</h3>



<p>Source control has long been a standard practice in software development. There are many well-known tools used to build source code repositories.</p>



<p>We’re confident that AI projects use source code repositories such as Git or GitHub; that’s a standard practice for all software developers. However, AI brings with it a different set of problems. In AI systems, the training data is as important as, if not more important than, the source code. So is the model built from the training data: the model reflects the training data and hyperparameters, in addition to the source code itself, and may be the result of hundreds of experiments.</p>



<p>Our survey shows that AI developers are only starting to use tools for data and model versioning. For data versioning, 35% of the respondents are using homegrown tools, while 46% responded “none of the above,” which we take to mean they’re using nothing more than a database. 9% are using <a href="https://dvc.org/">DVC</a>, 8% are using tools from <a href="https://wandb.ai/site">Weights &amp; Biases</a>, and 5% are using <a href="https://www.pachyderm.com/">Pachyderm</a>.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0112-1048x575.png" alt="" class="wp-image-13734" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0112-1048x575.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0112-300x165.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0112-768x422.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0112-1536x843.png 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0112-2048x1124.png 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 12. Automated tools used for data versioning</figcaption></figure>



<p>Tools for model and experiment tracking were used more frequently, although the results are fundamentally the same. 29% are using homegrown tools, while 34% said “none of the above.” The leading tools were MLflow (27%) and Kubeflow (18%), with Weights &amp; Biases at 8%.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0113-1048x826.png" alt="" class="wp-image-13735" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0113-1048x826.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0113-300x237.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0113-768x606.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0113-1536x1211.png 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/04/aadv_0113-2048x1615.png 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption>Figure 13. Automated tools used for model and experiment tracking</figcaption></figure>



<p>Respondents who are considering or evaluating AI are even less likely to use data versioning tools: 59% said “none of the above,” while only 26% are using homegrown tools. Weights &amp; Biases was the most popular third-party solution (12%). When asked about model and experiment tracking, 44% said “none of the above,” while 21% are using homegrown tools. It’s interesting, though, that in this group, MLflow (25%) and Kubeflow (21%) ranked above homegrown tools.</p>



<p>Although the tools available for versioning models and data are still rudimentary, it’s disturbing that so many practices, including those that have AI products in production, aren’t using them. You can’t reproduce results if you can’t reproduce the data and the models that generated the results. We’ve said that a quarter of respondents considered their AI practice mature—but it’s unclear what maturity means if it doesn’t include reproducibility.</p>



<h2>The Bottom Line</h2>



<p>In the past two years, the audience for AI has grown, but it hasn’t changed much: Roughly the same percentage of respondents consider themselves to be part of a “mature” practice; the same industries are represented, and at roughly the same levels; and the geographical distribution of our respondents has changed little.</p>



<p>We don’t know whether to be gratified or discouraged that only 50% of the respondents listed privacy or ethics as a risk they were concerned about. Without data from prior years, it’s hard to tell whether this is an improvement or a step backward. But it’s difficult to believe that there are so many AI applications for which privacy, ethics, and security aren’t significant risks.</p>



<p>Tool usage didn’t present any big surprises: the field is dominated by scikit-learn, TensorFlow, PyTorch, and Keras, though there’s a healthy ecosystem of open source, commercially licensed, and cloud native tools. AutoML has yet to make big inroads, but respondents representing less mature practices seem to be leaning toward automated tools and are less likely to use scikit-learn.</p>



<p>The number of respondents who aren’t addressing data or model versioning was an unwelcome surprise. These practices should be foundational: central to developing AI products that have verifiable, repeatable results. While we acknowledge that versioning tools appropriate to AI applications are still in their early stages, the number of participants who checked “none of the above” was revealing—particularly since “the above” included homegrown tools. You can’t have reproducible results if you don’t have reproducible data and models. Period.</p>



<p>In the past year, AI in the enterprise has grown; the sheer number of respondents will tell you that. But has it matured? Many new teams are entering the field, while the percentage of respondents who have deployed applications has remained roughly constant. In many respects, this indicates success: 25% of a bigger number is more than 25% of a smaller number. But is application deployment the right metric for maturity? Enterprise AI won’t really have matured until development and operations groups can engage in practices like continuous deployment, until results are repeatable (at least in a statistical sense), and until ethics, safety, privacy, and security are primary rather than secondary concerns. Mature AI? Yes, enterprise AI has been maturing. But it’s time to set the bar for maturity higher.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/ai-adoption-in-the-enterprise-2021/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>InfoTribes, Reality Brokers</title>
		<link>https://www.oreilly.com/radar/infotribes-reality-brokers/</link>
				<comments>https://www.oreilly.com/radar/infotribes-reality-brokers/#respond</comments>
				<pubDate>Tue, 23 Mar 2021 14:40:55 +0000</pubDate>
		<dc:creator><![CDATA[Hugo Bowne-Anderson]]></dc:creator>
				<category><![CDATA[AI & ML]]></category>
		<category><![CDATA[Research]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=13687</guid>
				<description><![CDATA[It seems harder than ever to agree with others on basic facts, let alone to develop shared values and goals: we even claim to live in a post-truth era1. With anti-vaxxers, QAnon, Bernie Bros, flat earthers, the intellectual dark web, and disagreement worldwide as to the seriousness of COVID-19 and the effectiveness of masks, have [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>It seems harder than ever to agree with others on basic facts, let alone to develop shared values and goals: we even claim to live in a post-truth era<sup>1</sup>. With anti-vaxxers, QAnon, Bernie Bros, flat earthers, the intellectual dark web, and disagreement worldwide as to the seriousness of COVID-19 and the effectiveness of masks, have we lost our shared reality? For every piece of information X somewhere, you can likely find “not X” elsewhere. There is a growing disbelief and distrust in basic science and government. All too often, conversations on social media descend rapidly to questions such as &#8220;<a href="https://twitter.com/januszczak/status/1285266873483919365">What planet are you from?</a>&#8221; </p>



<h2>Reality Decentralized</h2>



<p>What has happened? Reality has once again become decentralized. Before the advent of broadcast media and mass culture, individuals&#8217; <a href="https://en.wikipedia.org/wiki/Mental_model">mental models</a> of the world were generated locally, along with their sense of reality and what they considered ground truth. With broadcast media and the culture industries came the ability to forge top-down, national identities that could be pushed into the living rooms of families at prime time, completing the project of the press and newspapers in nation-forming<sup>2</sup>. The creation of the TV dinner was perhaps one of the most effective tools in carving out a sense of shared reality at a national level (did the TV dinner mean fewer people said Grace?).</p>



<p>The rise of the Internet, Search, social media, apps, and platforms has resulted in an information landscape that bypasses the centralized knowledge/reality-generation machine of broadcast media. It is, however, driven by the incentives (both visible and hidden) of significant power structures, such as Big Tech companies. With the degradation of top-down knowledge, we&#8217;ve seen the return of locally-generated shared realities, where local now refers to proximity in cyberspace. Content creators and content consumers are connected, share information, and develop mental models of the world, along with shared or distinct realities, based on the information they consume. They form communities and shared realities accordingly and all these interactions are mediated by the incentive systems of the platforms they connect on.</p>



<p>As a result, the number of possible realities has proliferated and the ability to find people to share any given reality with has increased. This InfoLandscape we all increasingly occupy is both novel and shifting rapidly. In it, we are currently finding people we can share some semblance of ground truth with: we&#8217;re forming our own InfoTribes, and shared reality is splintering around the globe.</p>



<p>To understand this paradigm shift, we need to comprehend:</p>



<ul><li>the initial vision behind the internet and the InfoLandscapes that have emerged,</li><li>how we are forming InfoTribes and how reality is splintering, </li><li>that large-scale shared reality has merely occupied a blip in human history, ushered in by the advent of broadcast media, and</li><li>who we look to for information and knowledge in an InfoLandscape that we haven&#8217;t evolved to comprehend.</li></ul>



<h2>The InfoLandscapes</h2>



<blockquote class="wp-block-quote"><p>&#8220;Cyberspace. A consensual hallucination experienced daily by billions of legitimate operators, in every nation, by children being taught mathematical concepts&#8230; A graphic representation of data abstracted from the banks of every computer in the human system. Unthinkable complexity. Lines of light ranged in the nonspace of the mind, clusters, and constellations of data. Like city lights, receding.&#8221;</p><cite>— <em>Neuromancer, </em>William Gibson (1984)</cite></blockquote>



<p>There are several ways to frame the origin story of the internet. One is how it gave rise to new forms of information flow: the vision of a novel space in which anybody could publish anything and everyone could find it. Much of the philosophy of early internet pioneers was couched in terms of the potential to &#8220;flatten organizations, globalize society, decentralize control, and help harmonize people&#8221; (<a href="https://web.media.mit.edu/~nicholas/Wired/WIRED3-02.html">Nicholas Negraponte, MIT</a>)<sup>3</sup>.</p>



<p>As John Perry Barlow (of Grateful Dead fame) wrote in <em>A Declaration of the Independence of Cyberspace </em>(1996):</p>



<blockquote class="wp-block-quote"><p>We are creating a world that all may enter without privilege or prejudice accorded by race, economic power, military force, or station of birth. We are creating a world where anyone, anywhere may express his or her beliefs, no matter how singular, without fear of being coerced into silence or conformity. Your legal concepts of property, expression, identity, movement, and context do not apply to us. They are all based on matter, and there is no matter here.</p></blockquote>



<p>This may have been the world we wanted but not the one we got. We are veering closer to an online and app-mediated environment similar to Deleuze&#8217;s <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwir47Gz-LjuAhU0zTgGHff9CPkQFjAIegQIAxAC&amp;url=https%3A%2F%2Fwww.jstor.org%2Fstable%2F778828&amp;usg=AOvVaw0SN92kTpl6SwpB2o1OM11f"><em>Societies of Control</em></a>, in which we are increasingly treated as our data and what Deleuze calls “dividuals”: collections of behavior and characteristics, associated with online interactions, passwords, spending, clicks, cursor movements, and personal algorithms, that can be passed into statistical and predictive models and guided and incentivized to behave and spend in particular ways. Put simply, we are reduced to the inputs of an algorithm. On top of this, pre-existing societal biases are being reinforced and promulgated at previously unheard of scales as <a href="https://www.oreilly.com/radar/when-models-are-everywhere/">we increasingly integrate machine learning models into our daily lives</a>.</p>



<p>Prescient visions of society along these lines were provided by William Gibson and Neal Stephenson&#8217;s 1992 <em>Snow Crash</em>: societies increasingly interacting in virtual reality environments and computational spaces, in which the landscapes were defined by information flows<sup>4</sup>. Not only this, but both authors envisioned such spaces being turned into marketplaces and segmented and demarcated by large corporations, only a stone&#8217;s throw from where we find ourselves today. How did we get here?</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_168604806-1048x604.jpeg" alt="" class="wp-image-13688" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_168604806-1048x604.jpeg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_168604806-300x173.jpeg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_168604806-768x443.jpeg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_168604806-1536x885.jpeg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_168604806-2048x1180.jpeg 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /></figure>



<h3>Information Creation</h3>



<p>In the early days of the internet, you needed to be a coder to create a website. The ability to publish material was relegated to the technical. It was only in walled gardens such as CompuServe and AOL or after the introduction of tools like Blogger that regular punters were able to create their own websites with relative ease. The participatory culture and user-generated content of Web 2.0 opened up the creative space, allowing anyone and everyone to create content, as well as respond to, rate, and review it. Over the last decade, two new dynamics have drastically increased the amount of information creation, and, therefore, the &#8220;raw material&#8221; with which the landscape can be molded:</p>



<ol><li>Smartphones with high-resolution video cameras and</li><li>The transformation of the attention economy by &#8220;social media&#8221; platforms,  which incentivize individuals to digitize more of their experiences and broadcast as much as possible.</li></ol>



<p>And it isn&#8217;t only the generation of novel content or the speed at which information travels. It is also the vast archives of human information and knowledge that are being unearthed, digitized, and made available online. This is the space of content creation.</p>



<h3>Information Retrieval</h3>



<p>The other necessary side of information flow is discoverability, how it is organized, and where it’s surfaced. When so much of the world’s information is available, what is the method for retrieval? Previously the realm of chat rooms and bulletin boards, this question eventually gave rise to the creation of search engines, social media platforms, streaming sites, apps, and platforms.</p>



<p>Platforms that automate the organizing and surfacing of online content are necessary, given the amount of content currently out there and how much is being generated daily. And they also require interrogating, as we humans base our mental models of how the world works on the information we receive, as we do our senses of reality, the way we make decisions, and the communities we form. Platforms such as Facebook have erected walled gardens in our new InfoLandscape and locked many of us into them, as predicted by both Gibson and Stephenson. Do we want such corporatized and closed structures in our networked commons?</p>



<h2>InfoTribes, Shared Reality</h2>



<p><em>A by-product of algorithmic polarization and fragmentation has been the formation of more groups that agree within their own groups and disagree far more between groups, not only on what they value but on ground truth, about reality.</em></p>



<p>Online spaces are novel forms of community: people who haven&#8217;t met and may never meet in real life interacting in cyberspace. As scholars such as danah boyd have <a href="https://www.danah.org/papers/TakenOutOfContext.html">made clear</a>, &#8220;social network sites like MySpace and Facebook are <em>networked publics</em>, just like parks and other outdoor spaces can be understood as publics.&#8221;</p>



<p>One key characteristic of any community is a sense of <em>shared reality</em>, something agreed upon. Communities are based around a sense of shared reality, shared values, and/or shared goals. Historically, communities have required geographical proximity to coalesce, whereas online communities have been able to form outside the constraints of <a href="https://www.urbandictionary.com/define.php?term=meatspace">meatspace</a>. Let’s not make the mistake of assuming online community formation doesn’t have constraints. The constraints are perhaps more hidden, but they exist: they’re both technological and the result of how the InfoLandscapes have been carved out by the platforms, along with their technological and economic incentives<sup>5</sup>. Landscapes and communities have co-evolved, although, for most of history, on different timescales: mountain ranges can separate parts of a community and, conversely, we build tunnels through mountains; rivers connect communities, cities, and commerce, and humans alter the nature of rivers (an extreme example being <a href="https://en.wikipedia.org/wiki/Chicago_River#Reversing_the_flow">the reversal of the Chicago River</a>!).</p>



<p>The past two decades have seen the formation of several new, rapidly and constantly shifting landscapes that we all increasingly interact with, along with the formation of new information communities, driven and consolidated by the emergent phenomena of filter bubbles and echo chambers, among many others, themselves driven by the platforms’ drive for engagement. What the constituents of each of these communities share are mental models of how the world works, senses of reality, that are, for the most part, reinforced by the algorithms that surface content, either by 1) showing content you agree with to promote engagement or 2) showing content you totally disagree with to the same end. Just as the newspaper page has historically been a mish-mash collection of movie ads, obituaries, and opinions stitched together in a way that made the most business and economic sense for any given publisher, your Facebook feed is driven by a collection of algorithms that, in the end, are optimizing for growth and revenue<sup>6</sup>. These incentives define the InfoLandscape and determine the constraints under which communities form. It just so happens that dividing people increases engagement and makes economic sense. As Karen Hao <a href="https://www.technologyreview.com/2021/03/11/1020600/facebook-responsible-ai-misinformation/">wrote recently</a> in the MIT Technology Review, framing it as a result of &#8220;Zuckerberg’s relentless desire for growth,&#8221; which is directly correlated with economic incentives:</p>



<blockquote class="wp-block-quote"><p>The algorithms that underpin Facebook’s business weren’t created to filter out what was false or inflammatory; they were designed to make people share and engage with as much content as possible by showing them things they were most likely to be outraged or titillated by.</p></blockquote>



<p>The consequence? As groups of people turn inward, agreeing more amongst their in-group, and disagreeing more fervently with those outside of it, the common ground in between, the shared reality, which is where perhaps the truth lies, is slowly lost. Put another way, a by-product of algorithmic polarization and fragmentation has been the formation of more groups that agree within their own groups and disagree far more with other groups, not only on what they value but on ground truth, about reality. </p>



<p>We’ve witnessed the genesis of information tribes or <em>InfoTribes</em> and, as these new ideological territories are carved up, those who occupy InfoLandscapes hold that ground as a part of an InfoTribe<sup>7</sup>. Viewed in this way, the online flame wars we’ve become all too accustomed to form part of the initial staking out of territory in these new InfoLandscapes. Anthropologists have long talked about tribes as being formed around symbols of group membership, symbols that unite a people, like totem animals, flags, or&#8230; online content.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_123489033-1048x667.jpeg" alt="" class="wp-image-13689" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_123489033-1048x667.jpeg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_123489033-300x191.jpeg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_123489033-768x489.jpeg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_123489033-1536x978.jpeg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_123489033-2048x1303.jpeg 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /></figure>



<h2>Reality Brokers, Reality Splintering</h2>



<p><em>The platforms that “decide” what we see and when we see it are reality brokers in a serious sense: they guide how individuals construct their sense of the world, their own identities, what they consider ground truth, and the communities they become a part of.</em></p>



<p>Arguably, many people aren’t particularly interested in the ground truth per se, they’re interested in narratives that support their pre-existing mental models of the world, narratives that help them sleep at night. This is something that 45 brilliantly, and perhaps unwittingly, played into and made starkly apparent, by continually sowing seeds of confusion, gaslighting the global community, and questioning the reality of anything that didn’t serve his own purposes.</p>



<p>This trend isn’t confined to the US. The rise of populism more generally in the West can be seen as the result of diverging senses of reality, the first slice splitting people across ideological and party lines. Why are these divergences in a sense of shared reality becoming so exacerbated and apparent now? The unparalleled velocity at which we receive information is one reason, particularly as we likely haven’t evolved to even begin to process the vast amounts we consume. But it isn&#8217;t only the speed and amount, it&#8217;s the structure. The current media landscape is highly non-linear, as opposed to print and television. Our sense-making and reality-forming faculties are overwhelmed daily by the fractal-like nature of (social) media platforms and environments that are full of overlapping phenomena and patterns that occur at many different frequencies<sup>8</sup>. Moreover, the information we’re served is generally driven by opaque and obscure economic incentives of platforms, which are protected by even more obscure legislation in the form of <a href="https://www.theverge.com/21273768/section-230-explained-internet-speech-law-definition-guide-free-moderation">Section</a> <a href="https://www.amazon.com/exec/obidos/ASIN/1501714414/">230</a> in the US (there are other incentives at play, themselves rarely surfaced, in the name of “trade secrets”). </p>



<p>But let&#8217;s be careful here: it isn’t tech all the way down. We’re also deep in a several decades-long erosion of institutional knowledge, a mistrust in both science and government being the two most obvious. Neoliberalism has carved out the middle class while the fruits of top-down knowledge have left so many people unserved and behind. On top of this, ignorance has been actively cultivated and produced. Look no further than the recent manufacturing of ignorance from the top down with the goals of chaos creation, sowing the seeds of doubt, and delegitimizing the scientific method and data reporting (the study of culturally induced ignorance is known as <em>agnotology </em>and Proctor and Scheibinger&#8217;s book <a href="https://www.sup.org/books/title/?id=11232"><em>Agnotology: The Making and Unmaking of Ignorance</em></a> is canonical). On top of this, we&#8217;ve seen the impact of bad actors and foreign influence (not mutually exclusive) on the dismantling of shared reality, such as Russian interference around the 2016 US election.</p>



<p>This has left reality up for grabs and, in an InfoLandscape exacerbated by a global pandemic, those who control and guide the flow of information also control the building of InfoTribes, along with their shared realities. Viewed from another perspective, the internet is a space in which information is created and consumed, a many-sided marketplace of supply-and-demand in which the dominant currency is information, albeit driven by a shadow market of data, marketing collateral, clicks, cash, and crypto. The platforms that “decide” what we see and when we see it are <em>reality brokers </em>in a serious sense: they guide how individuals construct their sense of the world, their own identities, what they consider ground truth, and the communities they become a part of. In some cases, these reality brokers may be doing it completely by accident. They don&#8217;t necessarily care about the ground truth, just about engagement, attention, and profit: the breakdown of shared reality as collateral damage of a globalized, industrial-scale incentive system. In this framework, the rise of conspiracy theories is an artefact of this process: the reality brokered and formed, whether it be a flat earth or a cabal of Satan-worshipping pedophiles plotting against 45, is a direct result of the bottom-up sense-making of top-down <em>reality splintering</em>, the dissolution of ground truth and the implosion of a more general shared reality. Web 2.0 has had a serious part to play in this reality splintering but the current retreat away into higher signal and private platforms such as newsletters, Slack, Discord, WhatsApp, and Signal groups could be more harmful, in many ways.</p>



<p>Shared reality is breaking down. But was it even real in the first place?</p>



<h2>Shared Reality as Historical Quirk</h2>



<p>Being born after World War Two could lead one to believe that shared reality is foundational for the functioning of the world and that it’s something that always existed. But there’s an argument that shared reality, on national levels, was really ushered in by the advent of broadcast media, first the radio, which was in over 50% of US households by the mid-1930s, and then the television, nuclear suburban families, and TV dinners. The hegemonic consolidation of the American dream was directly related to the projection of ABC, CBS, and NBC into each and every household.  When cable opened up TV to more than three major networks, <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1460-2466.2005.tb02677.x">we began to witness the fragmentation and polarization of broadcast media into more camps</a>, including those split along party lines, modern exemplars being Fox News and CNN.  It is key to recognize that there were distinct and differing realities in this period, split along national lines (USA and Soviet Russia), ideological lines (pro- and anti-Vietnam), and scientific lines (the impact of smoking and asbestos). Even then, it was a large number of people with a small number of shared realities.</p>



<p>The spread of national identity via broadcast media didn&#8217;t come out of the blue. It was a natural continuation of similar impacts of &#8220;The Printed Word,&#8221; which Marshall McLuhan refers to as an &#8220;Architect of Nationalism&#8221; in <a href="https://mitpress.mit.edu/books/understanding-media"><em>Understanding Media</em></a>:</p>



<blockquote class="wp-block-quote"><p>Socially, the typographic extension of man brought in nationalism, industrialism, mass markets, and universal literacy and education. For print presented an image of repeatable precision that inspired totally new forms of extending social energies.</p></blockquote>



<p>Note that the shared realities generated in the US in the 20th century weren&#8217;t only done so by national and governmental interests, but also by commercial and corporate interests: mass culture, the culture industries, culture at scale as a function of the rise of the corporation. There were strong incentives for commercial interests to create shared realities at scale across the nation because it&#8217;s easier to market and sell consumer goods, for example, to a homogeneous mass: one size fits all, one shape fits all. This was achieved through the convergence of mass media, modern marketing, and PR tactics.</p>



<p>Look no further than Edward Bernays, a double nephew of Freud who was referred to in his obituary as &#8220;the Father of Public Relations.&#8221; Bernays famously &#8220;<a href="https://www.npr.org/templates/story/story.php?storyId=4612464">used his Uncle Sigmund Freud&#8217;s ideas to help convince the public, among other things, that bacon and eggs was the true all-American breakfast</a>.&#8221; In the abstract of his 1928 paper &#8220;<a href="https://web.archive.org/web/20170407053758/http://w.truty.org/PDFs/Media/BERNAYS-ManipulatingPublicOpinion.pdf">Manipulating Public Opinion: The Why and the How</a>,&#8221; Bernays wrote:</p>



<blockquote class="wp-block-quote"><p>If the general principles of swaying public opinion are understood, a technique can be developed which, with the correct appraisal of the specific problem and the specific audience, can and has been used effectively in such widely different situations as changing the attitudes of whites toward Negroes in America, changing the buying habits of American women from felt hats to velvet, silk, and straw hats, changing the impression which the American electorate has of its President, introducing new musical instruments, and a variety of others.</p></blockquote>



<p>The Century of Marketing began, in some ways, with psychoanalytical tools, marketing as a mode of reality generation, societal homogenization, and behavioral modification. A paradigm of this is how <a href="https://www.theatlantic.com/international/archive/2015/02/how-an-ad-campaign-invented-the-diamond-engagement-ring/385376/">DeBeers convinced the West to adopt diamonds as the necessary gem for engagement rings</a>. A horrifying and still relevant example is <a href="https://www.newyorker.com/magazine/2017/10/30/the-family-that-built-an-empire-of-pain">Purdue Pharma and the Sackler dynasty&#8217;s marketing of OxyContin</a>.</p>



<p>The channels used by marketers were all of the culture industries, including broadcast media, a theme most evident in the work of <a href="https://en.wikipedia.org/wiki/Frankfurt_School">the Frankfurt School</a>, notably in that of Theodor Adorno and Max Horkheimer. Look no further than Adorno&#8217;s 1954 essay &#8220;<a href="https://users.clas.ufl.edu/burt/I%27mnotcrazy!/AdornoHowtoLookatTelevision.pdf">How to Look at Television</a>&#8220;:</p>



<blockquote class="wp-block-quote"><p>The old cultured elite does not exist any more; the modern intelligentsia only partially corresponds to it. At the same time, huge strata of the population formerly unacquainted with art have become cultural &#8220;consumers.&#8221;</p></blockquote>



<p>Although it was all the culture industries of the 20th century that worked to homogenize society at the behest of corporate interests, television was the one that we brought into our living rooms and that we eventually watched with family over dinner. Top-down reality-generation was centralized and projected into nuclear suburban homes.</p>



<p>Fast forward to today, the post-broadcast era, in which information travels close to the speed of light, in the form of lasers along fiber-optic cables and it’s both multi-platformed and personalized and everyone is a potential creator: reality, <em>once again</em>, is decentralized. In this frame, the age of shared reality was the anomaly, the exception rather than the rule. It’s perhaps ironic that one of the final throes of the age of shared reality was the advent of reality TV, a hyper-simulation of reality filtered through broadcast media. So now, in a fractured and fractal InfoLandscape, who do we look to in our efforts to establish some semblance of ground truth?</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_84736851-1048x704.jpeg" alt="" class="wp-image-13690" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_84736851-1048x704.jpeg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_84736851-300x202.jpeg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_84736851-768x516.jpeg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_84736851-1536x1032.jpeg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2021/03/AdobeStock_84736851-2048x1376.jpeg 2048w" sizes="(max-width: 1048px) 100vw, 1048px" /></figure>



<h2>Verified Checkmarks and Village Elders</h2>



<p><em>If our online communities are our InfoTribes, then the people we look to for ground truth are our village elders, those who tell stories around the campfire. </em></p>



<p>When COVID-19 hit, we were all scrambling around for information about reality in order to make decisions, and not only were the stakes a matter of life and death but, for every piece of information somewhere, you could find the opposite somewhere else. The majority of information, for many, came through social media feeds. Even when the source was broadcast media, a lot of the time it would be surfaced in a social media feed. Who did I pay attention to? Who did I believe? How about you? For better or for worse, I looked to my local (in an online sense) community, those whom I considered closest to me in terms of shared values and shared reality. On top of this, I looked to those respected in my communities. On Twitter, for example, I paid attention to <a href="https://twitter.com/EpiEllie">Dr Eleanor Murray</a> and <a href="https://twitter.com/NAChristakis">Professor Nicholas Christakis</a>, among many others. And why? They’re both leaders in their fields with track records of deep expertise, for one. But they also have a lot of Twitter followers and have the coveted blue verified checkmarks: in an InfoLandscape of such increasing velocity, we use rules of thumbs and heuristics around what to believe and what to not, including the validity and verifiability of the content creator, signaled by the number of followers, who the followers are (do I follow any of them? And what do I think of them?), and whether or not the platform has verified them.</p>



<p>If our online communities are our InfoTribes, then the people we look to for ground truth are our village elders, those who tell stories around the campfire. In the way they have insight into the nature of reality, we look to them as our illiterate ancestors looked to those who could read or as Pre-Reformation Christians looked to the Priests who could read Biblical Latin. With the emergence of these decentralized and fractured realities, we are seeing hand-in-hand those who rise up to define the realities of each InfoTribe. It’s no wonder the term <em>Thought Leader</em> rose to prominence as this landscape clarified itself. We are also arguably in the midst of a paradigm shift from content being the main object of verification online to content creators themselves being those verified. As Robyn Caplan points out astutely in <em><a href="https://slate.com/technology/2020/12/pornhub-verified-users-twitter.html">Pornhub Is Just the Latest Example of the Move Toward a Verified Internet</a></em>:</p>



<blockquote class="wp-block-quote"><p>It is often said that pornography drives innovation in technology, so perhaps that’s why many outlets have framed Pornhub’s verification move as “unprecedented.” However, what is happening on Pornhub is part of a broader shift online: Many, even most, platforms are using “verification” as a way to distinguish between sources, often framing these efforts within concerns about safety or trustworthiness.</p></blockquote>



<p>But mainstream journalists are more likely to be verified than independent journalists, men more likely than women, and, as Caplan points out “there is a dearth of publicly available information about the demographics of verification in general—for instance, whether BIPOC users are verified at the same rates as white users.” And it is key to note that many platforms are increasingly verifying and surfacing content created by “platform partners,“ an approach also driven by business incentives. Who decides who we listen to? And, as <a href="https://www.theguardian.com/books/2019/oct/04/shoshana-zuboff-surveillance-capitalism-assault-human-automomy-digital-privacy">Shoshana Zuboff</a> continually asks, <em>Who decides who decides?</em></p>



<p>This isn&#8217;t likely to get better anytime soon, with the retreat to private and higher signal communication channels, the next generation of personalized products, the advent of deep fakes, the increasing amount of information we&#8217;ll be getting from voice assistants over the coming 5-10 years, the proportion of information consumed via ephemeral voice-only apps such as Clubhouse, and the possibility of augmented reality playing an increasing role in our daily lives.</p>



<p>So what to do? Perhaps instead of trying to convince people of what we believe to be true, we need to stop asking &#8220;What planet are you from?&#8221; and start looking for shared foundations in our conversations, a sense of shared reality. We also have a public awareness crisis on our hands as the old methods of media literacy and education have stopped working. We need to construct new methods for people to build awareness, educate, and create the ability to dissent. Public education will need to bring to light the true contours of the emergent InfoLandscapes, some key aspects of which I have attempted to highlight in this essay. It will also likely include developing awareness of all our information platforms as multi-sided marketplaces, a growing compendium of all the informational dark patterns at play, the development of informational diets and new ways to count InfoCalories, and bringing antitrust suits against the largest reality brokers. Watch these spaces.</p>



<hr class="wp-block-separator" />



<p><em>Many thanks to Angela Bowne, Anthony Gee, Katharine Jarmul, Jamie Joyce, Mike Loukides, Emanuel Moss, and Peter Wang for their valuable and critical feedback on drafts of this essay along the way.</em></p>



<hr class="wp-block-separator" />



<h3>Footnotes</h3>



<p>1. A term first coined in 1990 by the playwright Steve Teisch and that was the Oxford Dictionaries 2016 Word of the Year (source: <a href="https://www.thenation.com/article/archive/post-truth-and-its-consequences-what-a-25-year-old-essay-tells-us-about-the-current-moment">Post-Truth and Its Consequences: What a 25-Year-Old Essay Tells Us About the Current Moment)</a><br>2. See Benedict Anderson&#8217;s <a href="https://www.versobooks.com/books/2259-imagined-communities"><em>Imagined Communities</em></a> for more about the making of nations through shared reading of print media and newspapers.<br>3. I discovered this reference in Fred Turner&#8217;s startling book&nbsp;<a rel="noreferrer noopener" href="https://press.uchicago.edu/ucp/books/book/chicago/F/bo3773600.html" target="_blank"><em>From Counterculture to Cyberculture</em></a>, which traces the countercultural roots of the internet to movements such as the New Communalists, leading many tech pioneers to have a vision of the web as &#8220;a collaborative and digital utopia modeled on the communal ideals&#8221; and &#8220;reimagined computers as tools for personal [and societal] liberation.&#8221;<br>4. There is a growing movement recognizing the importance of information flows in society. See, for example, <a href="https://courses.openmined.org/">OpenMined&#8217;s free online courses</a> which are framed around the theme that <a href="https://blog.openmined.org/society-runs-on-information-flows/">&#8220;Society runs on information flows.&#8221;</a><br>5. Think Twitter, for example, which builds communities by surfacing specific tweets for specific groups of people, a surfacing that’s driven by economic incentives, among others; although do note that TweetDeck, owned by Twitter, does not show ads, surface tweets, or recommend follows: perhaps the demographic that mostly uses TweetDeck doesn&#8217;t click on ads?<br>6. Having said this, there are some ethical constraints in the physical publishing business, for example, you can&#8217;t run an ad for a product across from an article or review of the product; there are also forms of transparency and accountability in physical publishing: we can all see what any given broadsheet publishes, discuss it, and interrogate it collectively.<br>7. Related concepts are the <a href="https://en.wikipedia.org/wiki/Tribe_(internet)">digital tribe</a>, a group of people who share common interests online, and the <a href="https://medium.com/s/world-wide-wtf/memetic-tribes-and-culture-war-2-0-14705c43f6bb">memetic tribe</a>, &#8220;a group of agents with a meme complex, or memeplex, that directly or indirectly seeks to impose its distinct map of reality—along with its moral imperatives—on others.&#8221;<br>8. Is it a coincidence that we&#8217;re also currently seeing the rise of non-linear note-taking, knowledge base, and networked thought tools, such as <a href="https://roamresearch.com/">Roam Research</a> and <a href="https://obsidian.md/">Obsidian</a>?</p>



<p></p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/infotribes-reality-brokers/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>The Next Generation of AI</title>
		<link>https://www.oreilly.com/radar/the-next-generation-of-ai/</link>
				<comments>https://www.oreilly.com/radar/the-next-generation-of-ai/#respond</comments>
				<pubDate>Tue, 09 Mar 2021 13:46:41 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides]]></dc:creator>
				<category><![CDATA[AI & ML]]></category>
		<category><![CDATA[Research]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=13676</guid>
				<description><![CDATA[Programs like AlphaZero and GPT-3 are massive accomplishments: they represent years of sustained work solving a difficult problem. But these problems are squarely within the domain of traditional AI. Playing Chess and Go or building ever-better language models have been AI projects for decades. The following projects have a different flavor: In February, PLOS Genetics [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>Programs like <a href="https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go">AlphaZero</a> and <a href="https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/">GPT-3</a> are massive accomplishments: they represent years of sustained work solving a difficult problem. But these problems are squarely within the domain of traditional AI. Playing Chess and Go or building ever-better language models have been AI projects for decades. The following projects have a different flavor:</p>



<ul><li>In February, PLOS Genetics published an article by researchers who are using GANs (Generative Adversarial Networks) to <a href="https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1009303">create artificial human genomes</a>.</li></ul>



<ul><li>Another group of researchers published an article about using NLP (natural language processing) to analyze viral genomes and, specifically, to <a href="https://www.technologyreview.com/2021/01/14/1016162/ai-language-nlp-coronavirus-hiv-flu-mutations-antinbodies-immune-vaccines/">predict the behavior of mutations</a>. They were able to distinguish between errors in &#8220;syntax&#8221; (which make the gene non-viable), and changes in semantics (which result in a viable virus that functions differently).</li></ul>



<ul><li>Yet another group of researchers modelled a small portion of a fruit fly&#8217;s brain (the part used for smell), and were able to train that to <a href="https://www.discovermagazine.com/the-sciences/fruit-fly-brain-network-hacked-for-language-processing">create a model for natural language processing</a>. This new model appears to be orders of magnitude more efficient than state-of-the-art models like GPT-3.</li></ul>



<p>The common thread through these advances is applying work in one field to another area that’s apparently unrelated—not sustained research at cracking a core AI problem. Using NLP to analyze mutations? That&#8217;s brilliant—and it&#8217;s one of those brilliant things that sounds so obvious once you think about it. And it&#8217;s an area where NLP may have a real significant advantage because it <em>doesn&#8217;t</em> actually understand language, any more than humans understand DNA.</p>



<p>The ability to create artificial human genomes is important in the short term because the human genome data available to researchers is limited by privacy laws. Synthetic genomes aren&#8217;t subject to privacy laws, because they don&#8217;t belong to any person. Data limitations aren’t a new problem; AI researchers frequently face the problem of finding sufficient data to train a model. So they have developed a lot of techniques for generating &#8220;synthetic&#8221; data: for example, cropping, rotating, or distorting pictures to get more data for image recognition. Once you’ve realized that it’s possible to create synthetic data, the jump to creating synthetic genomes isn’t far-fetched; you just have to make the connection. Asking where it might lead in the long term is even more important.</p>



<p>It&#8217;s not hard to come up with more examples of surprising work that comes from bringing techniques from one field into another. <a href="https://openai.com/blog/dall-e/">DALL-E</a> (which combines NLP with image analysis to create a new image from a description) is another example. So is <a href="https://dl.acm.org/doi/10.1145/3432202">ShadowSense</a>, which uses image analysis to let robots determine when they are touched.</p>



<p>These results suggest that we&#8217;re at the start of something new. The world isn&#8217;t a better place because computers can play Go; but it may become a better place if we can understand how our genomes work. Using adversarial techniques outside of game play or NLP techniques outside of language will inevitably lead to solving the problems we <em>actually</em> need to solve. </p>



<p>Unfortunately, that&#8217;s really only half the story. While we may be on the edge of making great advances in applications, we aren&#8217;t making the same advances in fairness and justice. Here are some key indicators:</p>



<ul><li>Attempts to train models to predict the pain that Black patients will suffer as a result of medical procedures have largely failed. Recently, research discovered that the models were more successful if they got their training data by <a href="https://www.technologyreview.com/2021/01/22/1016577/ai-fairer-healthcare-patient-outcomes/">actually listening to Black patients</a>, rather than just using records from their doctors.</li></ul>



<ul><li>A study by MIT discovered that training predictive crime models on crime reports rather than arrests doesn&#8217;t make them less racist. </li></ul>



<p>Fortunately, the doctors modeling medical pain decided to listen to their Black patients; unfortunately, that kind of listening is still rare. Listening to Black patients shouldn&#8217;t be a breakthrough akin to using NLP to analyze DNA. Why weren’t we listening to the patients in the first place? And why are the patients’ assessments of their pain so different from the doctors’?&nbsp; This is clearly progress, but more than that, it’s a sign of how much progress has yet to be made in treating minorities fairly.</p>



<p>And I&#8217;m afraid that MIT has only discovered that there aren&#8217;t any historical data sources about crime that aren&#8217;t biased, something we already knew. If you look at so-called &#8220;white collar&#8221; crime, Midtown Manhattan is the most dangerous neighborhood in New York. But that&#8217;s not where the police are spending their time.&nbsp; The only somewhat tongue-in-cheek <a href="https://whitecollar.thenewinquiry.com/static/whitepaper.pdf">paper</a> accompanying the map of <a href="https://whitecollar.thenewinquiry.com/">White Collar Crime Risk Zones</a> suggests that their next step will be using “facial features to quantify the ‘criminality’ of the individual.”&nbsp; That would clearly be a joke if such techniques weren’t already <a href="https://www.bbc.com/news/technology-53165286">under development</a>, and not just in China.</p>



<p>It looks like we&#8217;re at the cusp of some breakthroughs in AI—not new algorithms or approaches, but new ways to use the algorithms we already have. But the more things change, the more they stay the same. Our ability to think about our responsibilities of ethics and justice—and, more specifically, to put&nbsp; in place mechanisms to redress harms caused by unfair decisions–are slow to catch up.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/the-next-generation-of-ai/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>Product Management for AI</title>
		<link>https://www.oreilly.com/radar/product-management-for-ai/</link>
				<comments>https://www.oreilly.com/radar/product-management-for-ai/#respond</comments>
				<pubDate>Fri, 26 Feb 2021 19:40:39 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides]]></dc:creator>
				<category><![CDATA[AI & ML]]></category>
		<category><![CDATA[Deep Dive]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=13646</guid>
				<description><![CDATA[A couple of years ago, Pete Skomoroch, Roger Magoulas, and I talked about the problems of being a product manager for an AI product. We decided that would be a good topic for an article, and possibly more. After Pete and I wrote the first article for O’Reilly Radar, it was clear that there was [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>A couple of years ago, Pete Skomoroch, Roger Magoulas, and I talked about the problems of being a product manager for an AI product. We decided that would be a good topic for an article, and possibly more.</p>



<p>After Pete and I wrote the first article for O’Reilly Radar, it was clear that there was “more”–a lot more.&nbsp; We then added Justin Norman, VP of Data Science at Yelp, to the team.&nbsp; Justin did the lion’s share of the work from that point on.&nbsp; He has a great perspective on product management and AI, with deep practical experience with real-world products: not just building and deploying them, but shepherding them through the process from the initial idea to maintaining them after employment–including interfacing with management.</p>



<p>Many organizations start AI projects, but relatively few of those projects make it to production.&nbsp; These articles show you how to minimize your risk at every stage of the project, from initial planning through to post-deployment monitoring and testing.&nbsp; We’ve said that AI projects are inherently probabilistic. That’s true at every stage of the process.&nbsp; But there’s no better way to maximize your probability of success than to understand the challenges you’ll face.</p>



<h2>Report</h2>



<p><a href="https://learning.oreilly.com/library/view/product-management-for/9781098104207/">Product Management for AI </a></p>



<h2>Articles</h2>



<p><a href="https://www.oreilly.com/radar/what-you-need-to-know-about-product-management-for-ai/">What you need to know about product management for AI</a><br><a href="https://www.oreilly.com/radar/practical-skills-for-the-ai-product-manager/">Practical Skills for the AI Product Manager</a><br><a href="https://www.oreilly.com/radar/bringing-an-ai-product-to-market/">Bringing an AI Product to Market</a></p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/product-management-for-ai/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
	</channel>
</rss>
