<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"

	>

<channel>
	<title>Innovation &amp; Disruption &#8211; Radar</title>
	<atom:link href="https://www.oreilly.com/radar/topics/innovation-and-disruption/feed/" rel="self" type="application/rss+xml" />
	<link>https://www.oreilly.com/radar</link>
	<description>Now, next, and beyond: Tracking need-to-know trends at the intersection of business and technology</description>
	<lastBuildDate>Thu, 23 Dec 2021 18:28:26 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.3.13</generator>
	<item>
		<title>Remembering Freeman Dyson</title>
		<link>https://www.oreilly.com/radar/remembering-freeman-dyson/</link>
				<comments>https://www.oreilly.com/radar/remembering-freeman-dyson/#respond</comments>
				<pubDate>Wed, 04 Mar 2020 14:31:30 +0000</pubDate>
		<dc:creator><![CDATA[Tim O’Reilly]]></dc:creator>
				<category><![CDATA[Innovation & Disruption]]></category>
		<category><![CDATA[Commentary]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=12303</guid>
				<description><![CDATA[Freeman Dyson died last week at the age of 96 after injuring himself in a fall in the cafeteria at the Institute of Advanced Studies in Princeton, where he had continued to work right up to the end. I can&#8217;t resist adding to the outpouring of appreciation and love that has ensued. He has an [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>Freeman Dyson died last week at the age of 96 after injuring himself in a fall in the cafeteria at the Institute of Advanced Studies in Princeton, where he had continued to work right up to the end. I can&#8217;t resist adding to the outpouring of appreciation and love that has ensued. He has an outsized place in my mind and in my heart for someone whom I met in person fewer than a half-dozen times.</p>
<p>When I <a href="http://conferences.oreillynet.com/cs/os2004/view/e_sess/5498">interviewed Freeman on stage at OSCON in 2004</a>, along with his son George, the subject strayed to digital preservation. I lamented how much would be lost due to incompatible standards for information storage, and he said, &#8220;Oh no, forgetting is so important! It is what gives room for new ideas to come in.&#8221; This was such a typical Freeman moment: bringing a profoundly fresh perspective to any discussion. Perhaps the most famous example is <a href="https://journals.aps.org/pr/abstract/10.1103/PhysRev.75.486">the paper he wrote in 1949</a> at the age of 25 making the case that the visualizations of Richard Feynman were mathematically equivalent to the calculations of the more conventional physicists Julian Schwinger and Shin&#8217;ichirō Tomonaga, a paper that led to Feynman, Schwinger, and Tomonaga receiving the 1965 Nobel Prize in Physics for the theory of quantum electrodynamics.</p>
<p>This talent Freeman had for seeing to the heart of things was apparent even earlier, when he was working as a statistician in the operations research section of the Royal Air Force Bomber Command during World War II. As recounted in the first of his numerous volumes of autobiography, <a href="https://www.amazon.com/Disturbing-Universe-Sloan-Foundation-Science/dp/0465016774"><em>Disturbing the Universe</em></a>, he had been asked to study the pattern of bullet holes on the bombers returning to Britain from their forays overseas with an eye to reinforcing the areas with the most anti-aircraft damage. No, no, Freeman argued, reinforcement may be more effective in areas that show little damage in returning planes, because hits to the most vital regions will have caused the planes to be lost! The essential information was to be found in what was missing.</p>
<p>After George sent an email to a group of friends about Freeman&#8217;s death, Danny Hillis replied with a story that seems to perfectly encapsulate this gift of Freeman&#8217;s for seeing things that others missed. &#8220;I visited him recently,&#8221; Danny wrote, &#8220;and we got into a conversation about self-organizing systems. After lunch we climbed up the long stairs to his office, and when we sat down he seemed a bit distracted. I asked him what was wrong. Well, he said, what <strong>seemed</strong> wrong was that self-gravitating systems have negative specific heat capacity. The thing to do, he said, was to figure out why that was <strong>right</strong>.&#8221; When the world doesn&#8217;t quite make sense, don&#8217;t brush the offending observations under the rug. Think harder.</p>
<p>My earliest memory of Freeman, from well before I met him, echoes the distracted air that Danny noticed. In Kenneth Brower&#8217;s book <a href="https://www.amazon.com/Starship-Canoe-Kenneth-Brower/dp/0060910305"><em>The Starship and the Canoe</em></a>, about Freeman&#8217;s work on Project Orion and George&#8217;s work reconstructing early native American canoes in the Pacific Northwest, there was an account of the way Freeman would go so deep inside that he wouldn&#8217;t notice anyone around him—his interior world of thought was too vivid—and how sometimes, he would even break off in the middle of a conversation. Rude? No, he explained (as I recall it). Would someone think it rude if there were the sound of a car accident outside, and you broke off conversation to run to the window? Sometimes thoughts too are so compelling that you must immediately take notice. For those of us who are cursed (or gifted) with the experience that our interior world is sometimes more real than the outside, demanding all our concentration, this was a heady acknowledgement.</p>
<p>Another moment that I treasure was a fragment of an overheard conversation at the first <a href="https://en.wikipedia.org/wiki/Science_Foo_Camp">Science Foo Camp</a> in 2004. He and another physicist were discussing Michael Crichton&#8217;s novel <em>Prey</em>, in which the monster <em>du jour</em> was a swarm of nanoparticles. He said something like: isn&#8217;t it ridiculous how the nanoparticles are chasing people? Anyone knows that a particle moving in a viscous medium such as air moves at a velocity proportional to its length. (Or some such; I believe he offhandedly referenced a particular formula, which I don&#8217;t recall.) I was immediately struck by how differently the world appears to someone so deeply mathematical. We see the world through the lens of our received ideas, but for most of us, words predominate. Freeman had a gift for seeing with both words and numbers, and for throwing both away when needed, to see the world afresh.</p>
<p>Forgetting may be important, but so is remembering and honoring. Freeman was good at that, too. One of my favorite pieces of his writing is his foreword to a collection of Richard Feynman&#8217;s essays, <a href="https://www.amazon.com/Pleasure-Finding-Things-Out-Richard/dp/0465023959/"><em>The Pleasure of Finding Things Out</em></a>. It is so good that over the years, I&#8217;ve managed to get only about halfway through the book, since each time I pick it up, I first re-read Freeman&#8217;s foreword. His account of how he came to write his seminal paper on quantum electrodynamics is the most glorious appreciation of another human being that I have ever read, and shows Freeman at his best: curious, deep, original, humble, kind, thoughtful, and remarkably literary. You can read about Freeman&#8217;s life and career in one of the many obituaries, like <a href="https://www.nytimes.com/2020/02/28/science/freeman-dyson-dead.html">the one in the <em>New York Times</em></a> last week, but if you want to experience the man himself, you still can, through <a href="https://www.sns.ias.edu/sites/default/files/files/Dyson_Bibliography(1).pdf">the many books and articles he left behind</a>. This is as good <a href="https://books.google.com/books?id=WO9D_BaDDhkC&amp;pg=PT6&amp;lpg=PT6#v=onepage&amp;q&amp;f=false">a place</a> as any to start:</p>
<blockquote><p>“I did love the man this side idolatry as much as any,” wrote Elizabethan dramatist Ben Jonson. “The man” was Jonson’s friend and mentor, William Shakespeare. Jonson and Shakespeare were both successful playwrights. Jonson was learned and scholarly, Shakespeare was slapdash and a genius. There was no jealousy between them. Shakespeare was nine years older, already filling the London stage with masterpieces before Jonson began to write. Shakespeare was, as Jonson said, “honest and of an open and free nature,” and gave his young friend practical help as well as encouragement. The most important help that Shakespeare gave was to act one of the leading roles in Jonson’s first play, “Every Man in His Humour,” when it was performed in 1598. The play was a resounding success and launched Jonson’s professional career. Jonson was then aged 25, Shakespeare 34. After 1598, Jonson continued to write poems and plays, and many of his plays were performed by Shakespeare’s company. Jonson became famous in his own right as a poet and scholar, and at the end of his life he was honored with burial in Westminster Abbey. But he never forgot his debt to his old friend. When Shakespeare died, Jonson wrote a poem, “To the Memory of My Beloved Master, William Shakespeare,” containing the well-known lines: “He was not of an age, but for all time.” &#8230;</p>
<p>What have Jonson and Shakespeare to do with Richard Feynman? Simply this. I can say as Jonson said, “I did love this man this side idolatry as much as any.” Fate gave me the tremendous luck to have Feynman as a mentor. I was the learned and scholarly student who came from England to Cornell University in 1947 and was immediately entranced by the slapdash genius of Feynman. With the arrogance of youth, I decided that I could play Jonson to Feynman’s Shakespeare. I had not expected to meet Shakespeare on American soil, but I had no difficulty in recognizing him when I saw him.</p>
<p>Before I met Feynman, I had published a number of mathematical papers, full of clever tricks but totally lacking in importance. When I met Feynman, I knew at once that I had entered another world. He was not interested in publishing pretty papers. He was struggling, more intensely than I had ever seen anyone struggle, to understand the workings of nature by rebuilding physics from the bottom up&#8230;.I seized every opportunity to listen to Feynman talk, to learn to swim in the deluge of his ideas. He loved to talk, and he welcomed me as a listener. So we became friends for life.</p>
<p>For a year I watched as Feynman perfected his way of describing nature with pictures and diagrams, until he had tied down the loose ends and removed the inconsistencies. Then he began to calculate numbers, using his diagrams as a guide. With astonishing speed he was able to calculate physical quantities that could be compared directly with experiment. The experiments agreed with his numbers. In the summer of 1948 we could see Jonson’s words coming true: “Nature herself was proud of his designs, and joyed to wear the dressing of his lines.” During the same year when I was walking and talking with Feynman, I was also studying the work of the physicists Schwinger and Tomonaga, who were following more conventional paths and arriving at similar results. Schwinger and Tomonaga had independently succeeded, using more laborious and complicated methods, in calculating the same quantities that Feynman could derive directly from his diagrams. Schwinger and Tomonaga did not rebuild physics. They took physics as they found it, and only introduced new mathematical methods to extract numbers from the physics. When it became clear that the results of their calculations agreed with Feynman, I knew that I had been given a unique opportunity to bring the three theories together&#8230;. My paper was published in the Physical Review in 1949, and launched my professional career as decisively as “Every Man in His Humour” launched Jonson’s. I was then, like Jonson, 25 years old. Feynman was 31, three years younger than Shakespeare had been in 1598. I was careful to treat my three protagonists with equal dignity and respect, but I knew in my heart that Feynman was the greatest of the three and that the main purpose of my paper was to make his revolutionary ideas accessible to physicists around the world. Feynman actively encouraged me to publish his ideas, and never once complained that I was stealing his thunder. He was the chief actor in my play.</p></blockquote>
<p>Freeman undersells himself. He too was a genius. We will all miss him.</p>
<figure class="center"><img alt="Freeman Dyson at his 90th birthday celebration at the Institute for Advanced Studies in 2013." src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/03/FJD-at-90-at-IAS-scaled.jpg"><figcaption>Freeman Dyson at his 90th birthday celebration at the Institute for Advanced Studies in 2013. I was honored to be invited but my photos were from a far greater distance. This photo was taken by George Dyson, and is used courtesy of the Dyson family.</figcaption></figure>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/remembering-freeman-dyson/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>5 key areas for tech leaders to watch in 2020</title>
		<link>https://www.oreilly.com/radar/oreilly-2020-platform-analysis/</link>
				<comments>https://www.oreilly.com/radar/oreilly-2020-platform-analysis/#respond</comments>
				<pubDate>Tue, 18 Feb 2020 11:00:00 +0000</pubDate>
		<dc:creator><![CDATA[Roger Magoulas and Steve Swoyer]]></dc:creator>
				<category><![CDATA[AI & ML]]></category>
		<category><![CDATA[Innovation & Disruption]]></category>
		<category><![CDATA[Next Architecture]]></category>
		<category><![CDATA[Platform Analysis]]></category>
		<category><![CDATA[Research]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=11698</guid>
				<description><![CDATA[O’Reilly online learning contains information about the trends, topics, and issues tech leaders need to watch and explore. It’s also the data source for our annual usage study, which examines the most-used topics and the top search terms.[1] This combination of usage and search affords a contextual view that encompasses not only the tools, techniques, [&#8230;]]]></description>
								<content:encoded><![CDATA[<p id="F1"><a href="https://www.oreilly.com/online-learning/">O’Reilly online learning</a> contains information about the trends, topics, and issues tech leaders need to watch and explore. It’s also the data source for our annual usage study, which examines the most-used topics and the top search terms.<a href="#_ftn1"><sup>[1]</sup></a></p>
<p>This combination of usage and search affords a contextual view that encompasses not only the tools, techniques, and technologies that members are actively using, but also the areas they’re gathering information about.</p>
<p>Current signals from usage on the O’Reilly online learning platform reveal:</p>
<ul>
<li><strong>Python is preeminent</strong>. It’s the single most popular programming language on O’Reilly, and it accounts for 10% of <em>all</em> usage. This year’s growth in Python usage was buoyed by its increasing popularity among data scientists and machine learning (ML) and artificial intelligence (AI) engineers.</li>
<li><strong>Software architecture, infrastructure, and operations are each changing rapidly</strong>. The shift to cloud native design is transforming both software architecture and infrastructure and operations. Also: <a href="https://conferences.oreilly.com/infrastructure-ops">infrastructure and operations</a> is trending up, while DevOps is trending down. Coincidence? Probably not, but only time will tell.</li>
<li><strong>ML + AI are up, but passions have cooled</strong>. Up until 2017, the ML+AI topic had been amongst the fastest growing topics on the platform. Growth is still strong for such a large topic, but usage slowed in 2018 (+13%) and cooled significantly in 2019, growing by just 7%. Within the data topic, however, ML+AI has gone from 22% of all usage to 26%.</li>
<li><strong>Still cloud-y, but with a possibility of migration</strong>. Strong usage in cloud platforms (+16%) accounted for most cloud-specific growth. But sustained interest in cloud migrations—usage was up almost 10% in 2019, on top of 30% in 2018—gets at another important emerging trend.</li>
<li><strong>Security is surging.</strong> Aggregate security usage spiked 26% last year, driven by increased usage for two security certifications: CompTIA Security (+50%) and CompTIA CySA+ (+59%). There’s plenty of security risks for business executives, sysadmins, DBAs, developers, etc., to be wary of.</li>
</ul>
<figure class="center"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/02/1-top-search-terms.png" alt="Normalized search frequency of top terms on the O’Reilly online learning platform in 2019 (left) and the rate of change for each term (right)."><figcaption>Figure 1 (above). Normalized search frequency of top terms on the O’Reilly online learning platform in 2019 (left) and the rate of change for each term (right).</figcaption></figure>
<figure class="center"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/02/2-high-level-topic-usage-e1581084720697.png" alt="High-level topics on the O’Reilly online learning platform with the most usage in 2019 (left) and the rate of change for each topic (right)."><figcaption>Figure 2. High-level topics on the O’Reilly online learning platform with the most usage in 2019 (left) and the rate of change for each topic (right).</figcaption></figure>
<h2>In programming, Python is preeminent</h2>
<p>In 2019, as in 2018, Python was the most popular language on O’Reilly online learning. Python-related usage grew at a solid 6% pace in 2019, a slight drop from 2018 (+10%). After several years of steady climbing—and after outstripping Java in 2017—Python-related interactions now comprise almost 10% of all usage.</p>
<p>But Python is a special case: this year, more than in year’s past, its growth was buoyed by interest in ML. Usage specific to Python as a programming language grew by just 4% in 2019; by contrast, usage that had to do with Python and ML—be it in the context of AI, deep learning, and natural language processing, or in combination with any of several popular ML/AI frameworks—grew by 9%. The laggard use case was Python-based web development frameworks, which grew by just 3% in usage, year over year.</p>
<figure class="center"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/02/3-top-languages-search.png" alt="Normalized search frequency of top programming languages on the O’Reilly online learning platform in 2019 (left) and the rate of change for each language (right)."><figcaption>Figure 3 (above). Normalized search frequency of top programming languages on the O’Reilly online learning platform in 2019 (left) and the rate of change for each language (right).</figcaption></figure>
<figure class="center"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/02/4-top-languages-usage.png" alt="Programming languages on the O’Reilly online learning platform with the most usage in 2019 (left) and the rate of change for each language (right)."><figcaption>Figure 4. Programming languages on the O’Reilly online learning platform with the most usage in 2019 (left) and the rate of change for each language (right).</figcaption></figure>
<p>This is consistent with <a href="https://www.oreilly.com/radar/whats-driving-open-source-software-in-2019/">what we’ve observed elsewhere</a>: Python has acquired new relevance amid strong interest in AI and ML. Along with <a href="https://en.wikipedia.org/wiki/R_(programming_language)">R</a>, Python is one of the most-used languages for data analysis. From pre-built libraries for linear or logistic regressions, decision trees, naïve Bayes, k-means, gradient-boosting, etc., there’s a Python library for virtually anything a developer or data scientist might need to do. (Python libraries are no less useful for manipulating or engineering data, too.)</p>
<p>Interestingly, <a href="https://trends.google.com/trends/explore?date=all&amp;geo=US&amp;q=%2Fm%2F0212jm">R itself continues to decline</a>. R-related usage on O’Reilly online learning fell by 8% between 2017-18 and by 6%, year-over-year, in 2019. It’s likely that R—much like Scala (-33% in usage in 2018-19; -19% in usage 2017-18)—is a casualty of Python. True, it might seem difficult to reconcile R’s decline with strong interest in AI and ML, but consider two factors: first, ML and statistics are not the same thing, and, second, R is not, primarily, a developer-oriented language. R was designed for use in academic, scientific, and, more recently, commercial use cases. As statistics and related techniques become more important in software development, more programmers are encountering stats in programming classes. In this context, they’re more likely to use Python than R.</p>
<p>Interest in some languages seems to be trending up, and interest in others, down. Exhibit A: Java-related usage dropped by a noteworthy 13% between 2018 and 2019. Is this the harbinger of a trend? Not necessarily: Java-related searches increased by 5% between 2017 and 2018. On the other hand, Java’s cousin, JavaScript, also appears to be in decline. True, theirs is only a conceptual relation, but interest in JavaScript, too, <a href="https://trends.google.com/trends/explore?date=all&amp;geo=US&amp;q=%2Fm%2F02p97">really does seem to be waning</a>: JS-related usage dropped on O’Reilly online learning by 4% between 2017-2018 and by 7% between 2018-19. It’s possible that microservices architecture is hastening the move to other languages (such as Go, Rust, and Python) for web properties.</p>
<p>Among the JavaScript-based web application frameworks, React gained in popularity (+4% in usage) as Angular (-12% in usage) slipped between 2018 and 2019. Vue.js—a competitor to both React and Angular—settled down to steady growth (+8% in usage) in 2018-19, after almost doubling in usage (+97%) between 2017-18.</p>
<p id="F2">One possible trend-in-the-making is that of a slowing Go, which—following several years of rapid growth in usage (including +14% from 2017 to 2018)—cooled down last year, with usage growing by a mere 2%. But Go is now the sixth most-used programming language, trailing only Python, Java, .NET, and C++. Drop .NET from the tally on methodological grounds<a href="#_ftn2"><sup>[2]</sup></a>, and Go cracks the top five.</p>
<h2>Trends in software architecture, infrastructure, and operations</h2>
<p><a href="https://www.oreilly.com/radar/how-companies-adopt-and-apply-cloud-native-infrastructure/">Cloud native</a><a href="https://www.oreilly.com/radar/how-companies-adopt-and-apply-cloud-native-infrastructure/"> design</a> is a new way of thinking about software and architecture. But the shift to cloud native has implications not only for <a href="https://www.oreilly.com/radar/the-topics-to-watch-in-software-architecture/">software architecture</a><u>,</u> but for <a href="https://conferences.oreilly.com/infrastructure-ops/io-ca">infrastructure and operations</a>, too. It exploits new design patterns (microservices) and adapts existing techniques (service orchestration) with the goal of achieving cloud-like elasticity and resilience in <em>all</em> environments, cloud or on-premises. O’Reilly Radar uses the term “<a href="https://www.oreilly.com/radar/what-is-next-architecture/">Next Architecture</a><u>”</u> to describe this shift.</p>
<p>It’s against this backdrop that what’s happening in both software architecture and infrastructure and ops must be understood. In the generic software architecture topic, usage in the containers topic increased in our 2019 analysis, growing by 17%. This was just a fraction of its 2018 growth rate (+56% in usage), but impressive nonetheless. Kubernetes has emerged as the <em>de facto</em> solution for orchestrating services and microservices in cloud native design patterns. Usage in Kubernetes surged by 211% in 2018—and grew at a 40% clip in 2019. Kubernetes’ parent topic, container orchestrators, also posted strong usage growth: 151% in 2018, 36% this year—almost all due to interest in Kubernetes itself.</p>
<figure class="center"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/02/5-software-architecture-usage-e1581084431270.png" alt="Software architecture topics on the O’Reilly online learning platform with the most usage in 2019 (left) and the rate of change for each topic (right)."><figcaption>Figure 5. Software architecture topics on the O’Reilly online learning platform with the most usage in 2019 (left) and the rate of change for each topic (right).</figcaption></figure>
<p>This also helps explain increased usage in the microservices topic, which grew at a 22% clip in 2019. True, you don’t necessarily <em>need</em> microservices to “do” cloud native design; at this point, however, it’s difficult to disentangle the two. Most cloud native design patterns involve microservices.</p>
<p>These trends are also implicated in the rise of infrastructure and ops, which reflects both the limitations of DevOps and the challenges posed by the shift to cloud native design. Infrastructure and ops usage was the fastest growing sub-topic under the generic systems administration topic. Surging interest in infrastructure and ops also explains declining usage in the configuration management (CM) and DevOps topic areas. The most popular CM tools are DevOps focused, and, like DevOps itself, they’re declining: usage in the CM topic dropped significantly (-18%) in 2019, as did virtually all CM tools. Ansible was least affected (-4% in usage), but Jenkins, Puppet, Chef, and Salt each dropped off by 25% or more in usage. It can’t be a coincidence that DevOps usage declined again (-5%) in 2019, following a 20% decline in 2018.</p>
<figure class="center"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/02/6-infrastructure-ops-usage-e1581084489821.png" alt="Infrastructure and operations topics on the O’Reilly online learning platform with the most usage in 2019 (left) and the rate of change for each topic (right)."><figcaption>Figure 6. Infrastructure and operations topics on the O’Reilly online learning platform with the most usage in 2019 (left) and the rate of change for each topic (right).</figcaption></figure>
<p>The emergence of infrastructure and ops suggests that organizations might be having trouble <em>scaling</em> DevOps. DevOps aims to produce programmers who can work competently in each of the layers in a system “<a href="https://www.facebook.com/note.php?note_id=461505383919">stack</a>.” In practice, however, developers tend to be less committed to DevOps’ operations component, a fact that gave birth to the idea of <a href="https://en.wikipedia.org/wiki/Site_Reliability_Engineering">site reliability engineering</a> (SRE). Even if the “full stack” developer isn’t a unicorn, she certainly isn’t commonplace. Organizations see infrastructure and ops as a pragmatic, ops-focused complement that picks up precisely where DevOps tends to fail.</p>
<h2>A drill-down into data, AI, and ML topics</h2>
<p>The results for data-related topics are both predictable and—there’s no other way to put it—confusing. Starting with data engineering, the backbone of all data work (the category includes titles covering data management, i.e., relational databases, Spark, Hadoop, SQL, NoSQL, etc.). In aggregate, data engineering usage declined 8% in 2019. This follows a 3% drop in 2018. Both years were driven by declining usage of data management titles.</p>
<p>When we look more specifically at data engineering topics, <em>excluding</em> data management, we see a small share, but solid growth in usage, up 7% in 2018 and 15% in 2019 (see Figure 7).</p>
<p>Within the broad “data” topic, data engineering (including data management) continues as the topic with the most share, garnering about one-twelfth of all usage on the platform. This is almost double the usage share of the data science topic, which recorded an uptick in usage (+5%) in 2019, following a decline (-2%) in 2018.</p>
<p>Elsewhere, interest in ML and AI keeps growing, albeit at a diminished rate. To wit: the combined ML/AI topic was up 7% in usage in 2019, about half its growth (+13%) in 2018.</p>
<figure class="center"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/02/7-data-usage-e1581084542209.png" alt="Data topics on the O’Reilly online learning platform with the most usage in 2019 (left) and the rate of change for each topic (right)."><figcaption>Figure 7. Data topics on the O’Reilly online learning platform with the most usage in 2019 (left) and the rate of change for each topic (right).</figcaption></figure>
<p>Ironically, the strength of ML/AI might be less evident in data-specific topics than in other topic areas, such as programming languages, where growing Python usage is—to a large degree—being driven by that language’s usefulness for and applicability to ML. But ML/AI-related topics such as natural language processing (NLP, +22% in 2019) and neural networks (+17%) recorded strong growth in usage, too.</p>
<p id="F3">Data engineering <em>as a task</em> certainly isn’t in decline. Interest in data engineering probably isn’t declining, either. If anything, data engineering <em>as a practice area</em> is being subsumed by both data science and ML/AI<a href="#_ftn3"><sup>[3]</sup></a>. We know from <a href="https://www.oreilly.com/radar/the-state-of-data-quality-in-2020/">other research</a> that data scientists, ML and AI engineers, etc., spend an outsized proportion of their time discovering, preparing, and engineering data for their work. We’ve seen that popular tools and frameworks usually incorporate data engineering capabilities, either in the form of automated/guided self-service features or (in the case of Jupyter and other notebooks) an ability to build and orchestrate data engineering pipelines that invoke Python, R (via Python), etc., libraries to run data engineering jobs concurrently or, if possible, in parallel.</p>
<p>Terms that correspond with old-school data engineering—e.g., “relational database,” “Oracle database solutions,” “Hive,” “database administration,” “data models,” “Spark”—declined in usage, year-over-year, in 2019. Some of this decline was a function of larger, market-driven factors. We know from our research that Hadoop and its ecosystem of related projects (such as Hive) <a href="https://www.oreilly.com/radar/topics-to-watch-at-the-strata-data-conference-in-new-york-2019/">are in the midst of a protracted, years-long decline</a>. This decline is borne out in our usage numbers: Hadoop (-34%), Hive (also -34%), and even Spark (-21%) were all down, significantly, year-over-year.</p>
<p>We discuss likely reasons for this decline in more detail <a href="https://www.oreilly.com/radar/topics-to-watch-at-the-strata-data-conference-in-new-york-2019/">in our analysis of O’Reilly Strata Conference speaker proposals</a>.</p>
<h2>Cloud continuing to climb</h2>
<p>Interest in cloud-related concepts and terms continues to increase on O’Reilly online learning, albeit at a slower rate. Cloud-related usage surged by 35% between 2017 and 2018; it grew at less than half that rate (17%) between 2018 and 2019. This slowdown suggests that cloud as a category has achieved such a large share that (mathematically) any additional growth <em>must</em> occur at a slower rate. In cloud’s case, while growth is slower, it’s still strong.</p>
<figure class="center"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/02/8-cloud-usage-e1581084605868.png" alt="Cloud topics on the O’Reilly online learning platform with the most usage in 2019 (left) and the rate of change for each topic (right)."><figcaption>Figure 8. Cloud topics on the O’Reilly online learning platform with the most usage in 2019 (left) and the rate of change for each topic (right).</figcaption></figure>
<p>Interest in cloud service provider platforms mirrors that of the industry as a whole: Amazon and AWS-related usage increased by 14%, year-over-year; Azure usage, on the other hand, grew at a speedier 29% clip, while Google Compute Platform (GCP) surged by 39%. Amazon controls a little less than half (<a href="https://www.gartner.com/en/newsroom/press-releases/2019-07-29-gartner-says-worldwide-iaas-public-cloud-services-market-grew-31point3-percent-in-2018">per Gartner’s 2018 numbers</a>) of the overall market for cloud infrastructure-as-a-service (IaaS) offerings. It, too, has reached the point at which rapid growth becomes mathematically prohibitive. Both Azure and GCP are growing much faster than AWS, but they’re also much smaller: Azure notched nearly 61% growth in 2018 (per Gartner), good for more than 15% of the IaaS market; GCP, at around 60% growth, accounts for 4% of IaaS share.</p>
<p>Also intriguing: cloud-specific interest in microservices and <a href="https://en.wikipedia.org/wiki/Kubernetes">Kubernetes</a> grew significantly last year on O’Reilly. Microservices-related usage was up 22%, year over year, following a decline in 2018. Kubernetes usage was up by 38%, year over year, following a period of explosive growth (+190%) from 2017 to 2018. Both trends mirror what we’re seeing via user surveys and other <a href="https://www.oreilly.com/radar/tag/research/">research efforts</a>: namely, that microservices has emerged as an important component of cloud native design and development.</p>
<p>The bigger takeaway is that the essential tendency of modern software architecture—namely, the priority it gives to loose coupling in emphasizing abstraction, isolation, and atomicity—is eliding the boundaries between what we think of as “cloud” versus “on-premises” contexts. We see this via sustained interest in microservices and Kubernetes in both on-premises <em>and</em> cloud deployments.</p>
<p>This is the logic of cloud native design: specific deployment contexts will still matter, of course—which features or constraints do developers need to take into account when they’re developing for AWS? For Azure? For GCP? But the clear boundaries that used to demarcate the public cloud from the private cloud are starting to disappear, just as those that distinguish on-premises private clouds from conventional on-premises systems are falling away, too.</p>
<h2>Surging interest in security</h2>
<p>Security usage (+26%) grew significantly in 2019 (see Figure 2). Some of this was driven by increased usage in the CompTIA Security+ (50%) and CompTIA Cyber Security Analyst (CySA+, 59%) topics.</p>
<p>Security+ is an entry-level security certification, so its growth could be attributed to increased usage by sysadmins, DBAs, software developers, and other non-specialists. Whether it’s to flesh out their full-stack bona fides, address new job (or <a href="https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-181.pdf?trackDocs=NIST.SP.800-181.pdf">regulatory</a>) requirements, or simply to make themselves more marketable, Security+ is a pretty straightforward certification process: pass the exam and you’re certified. CySA+, on the other hand, is relatively new. This <em>could</em> explain the explosion of CySA+-related usage in 2018 (+128%), as well as last year’s strong growth. Unlike <a href="https://en.wikipedia.org/wiki/List_of_computer_security_certifications">the CISSP and other popular certifications</a>, CySA+ recommends, but doesn’t require, real-world experience. Like Security+, it’s another certification sys admins, DBAs, developers, and others can pick up to burnish their bona fides.</p>
<p>Certifications weren’t the only thing driving security-related usage on O’Reilly in 2019. A rash of vulnerabilities and potential exploits, too, had some impact. If 2018 (+5% growth in security usage; +22% growth in search) gave us <a href="https://en.wikipedia.org/wiki/Meltdown_(security_vulnerability)">Meltdown</a> and <a href="https://en.wikipedia.org/wiki/Spectre_(security_vulnerability)">Spectre</a>, 2019 gave us sobering information about <a href="https://www.pandasecurity.com/mediacenter/security/spectre-making-comeback-processors/">the far-reaching </a><a href="https://www.pandasecurity.com/mediacenter/security/spectre-making-comeback-processors/"><em>implications</em></a><a href="https://www.pandasecurity.com/mediacenter/security/spectre-making-comeback-processors/"> of Meltdown</a> and, <a href="https://www.technologyreview.com/s/612989/chips-may-be-inherently-vulnerable-to-spectre-and-meltdown-attacks/">especially, Spectre</a>. For 2019, security-specific usage (+26%) and search (+25%) increased accordingly. System and database administrators, CSAs, CISSPs, and others were keen to acquire expert, detailed information specific to patching and hardening their vulnerable systems to protect against <a href="https://arxiv.org/pdf/1811.05441.pdf">no less than 13 different Spectre and 14 different Meltdown variants</a>, as well as to mitigate <a href="https://www.phoronix.com/scan.php?page=article&amp;item=spec-melt-8way&amp;num=1">the potentially huge performance impacts associated with these patches</a>. Developers and software architects had questions about rewriting, refactoring, or optimizing their code to address these same concerns. Against this backdrop, the spike in security-related usage makes sense.</p>
<p>There was a great deal going on with respect to information security and data privacy, too. After all, not only was 2019 the first full year for which the EU’s omnibus GDPR regime was binding, but—as of January 1, 2019—updates to Canada’s GDPR-like PIPEDA regime officially kicked in, too. The sweeping <a href="https://en.wikipedia.org/wiki/California_Consumer_Privacy_Act">California Consumer Privacy Act</a> (CCPA), which has been called California’s GDPR, went into effect on January 1, &nbsp;2020.</p>
<p>Taken together, an analysis of these trends seems to support a glass-half-full assessment of the state of security today. If the sustained growth in security usage on O’Reilly is a reliable indicator, it’s <em>possible</em> that security may, finally, be getting the attention it deserves in an increasingly digital world. It&#8217;s <em>possible</em> that organizations have accepted that the financial and reputational penalties entailed by a data breach or high-profile hack are just too costly to risk, and that money spent on information security is, on balance, money well spent.</p>
<p>The same analysis also lends itself to a glass-half-empty assessment, however: namely, that security spending is cyclical; that a confluence of circumstances has helped to boost security spending; and that—let’s be honest—<a href="https://www.varonis.com/blog/company-reputation-after-a-data-breach/">organizations tend to bounce back from high-profile security incidents</a>. Only time (or future installments of this survey) will tell.</p>
<h2>Concluding thoughts</h2>
<p>It’s hard to imagine that the hottest trends of 2019 won’t be reprising their roles, in more or less the same pecking order, in next year’s analysis. Programming languages come into and go out of vogue, but Python appears poised to keep growing at a steady rate because it’s at once protean, adaptable, and easy to use. We see this in the widespread use of Python in ML and AI, where it has supplanted R as the <em>lingua franca</em> of data engineering and analysis.</p>
<p>The same is true of ML and AI. Even if (as some naysayers warn) the next <a href="https://en.wikipedia.org/wiki/AI_winter">AI winter</a> <a href="https://www.ft.com/content/47111fce-d0a4-11e8-9a3c-5d5eac8f1ab4">is nigh upon us</a>, it’s hard to imagine interest in ML and AI petering out anytime soon. The same could be said about trends in software architecture and, especially, infrastructure and operations. They’re each sites of ceaseless innovation. Their practitioners will be hard-pressed to keep up with what’s happening.</p>
<p>It’s helpful to think of what’s hot and what’s not in terms of a modified “<a href="https://en.wikipedia.org/wiki/Overton_window">Overton Window</a>.” The Overton Window circumscribes the <em>human cognitive bandwidth</em> that’s available in a certain place at a certain time. No combination of policies—or issues, or trends—can exceed more than 100% of available bandwidth. This is true, to a degree, of the activity on O’Reilly online learning, too. A decline in usage doesn’t have to correlate with a decline in use (or usefulness) in practice. It’s just being crowded out by other, emergent trends.</p>
<p>This also underscores why the decline in a bellwether topic such as JavaScript might be hugely significant. Even if these topics are no longer sites of rapid and sustained innovation, they are likewise important for day-to-day use cases, especially with respect to general-purpose information-gathering or more specialized problem solving. It isn’t that JavaScript is less important than it used to be; after all, React, Angular, and Vue.js are sites of development and innovation, and all three are based on JavaScript. It’s that we’re coming to understand and relate to JavaScript—or data engineering, or Docker, or DevOps and change management—in a different way. We’re appropriating them differently.</p>
<p>It’s this difference that Radar aims to capture. Not the change that’s obvious for everyone to see, but the coalescence of change itself as it’s happening.</p>
<hr>
<aside data-type="sidebar">
<p id="_ftn1"><a href="#F1"><sup>[1]</sup></a> This article is based on non-personally-identifiable information about the top search terms and most-used topics on O’Reilly online learning. We compared aggregated data for the last three years; a full year of data for 2017 and 2018, and through the end of October for 2019.</p>
<p id="_ftn2"><a href="#F2"><sup>[2]</sup></a> A reasonable enough decision. .NET isn’t so much a language as a software framework: i.e., a superset of C# and a few related languages, including Visual Basic .NET, J#, and, C++/CLI, the latter of which is a .NET-specific implementation of C++.</p>
<p id="_ftn3"><a href="#F3"><sup>[3]</sup></a> ML and AI aren’t in any sense the same thing, either. We combine them here for simplicity’s sake.</p>
</aside>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/oreilly-2020-platform-analysis/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>AI meets operations</title>
		<link>https://www.oreilly.com/radar/ai-meets-operations/</link>
				<comments>https://www.oreilly.com/radar/ai-meets-operations/#respond</comments>
				<pubDate>Mon, 03 Feb 2020 05:01:00 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides]]></dc:creator>
				<category><![CDATA[Innovation & Disruption]]></category>
		<category><![CDATA[Next Architecture]]></category>
		<category><![CDATA[Radar Column]]></category>
		<category><![CDATA[Commentary]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=11603</guid>
				<description><![CDATA[One of the biggest challenges operations groups will face over the coming year will be learning how to support AI- and ML-based applications. On one hand, ops groups are in a good position to do this; they&#8217;re already heavily invested in testing, monitoring, version control, reproducibility, and automation. On the other hand, they will have [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>One of the biggest challenges operations groups will face over the coming year will be learning how to support AI- and ML-based applications. On one hand, ops groups are in a good position to do this; they&#8217;re already heavily invested in testing, monitoring, version control, reproducibility, and automation. On the other hand, they will have to learn a lot about how AI applications work and what&#8217;s needed to support them. There&#8217;s a lot more to AI Operations than Kubernetes and Docker. The operations community has the right language, and that&#8217;s a great start; I do <em>not</em> mean that in a dismissive sense. But on inspection, AI stretches the meanings of those terms in important but unfamiliar directions.</p>
<p>Three things need to be understood about AI.</p>
<p>First, the behavior of an AI application depends on a <em>model</em>, which is built from source code and <em>training data</em>. A model isn’t source code, and it isn’t data; it’s an artifact built from the two. Source code is relatively less important compared to typical applications; the training data is what determines how the model behaves, and the training process is all about tweaking parameters in the application so that it delivers correct results most of the time.</p>
<p>This means that, to have a history of how an application was developed, you have to look at more than the source code. You need a repository for models and for the training data. There are many tools for managing source code, from git back to the venerable SCCS, but we&#8217;re only starting to build tools for <a href="https://dvc.org/">data versioning</a>. And that&#8217;s essential: if you need to understand how your model behaves, and you don&#8217;t have the training data, you&#8217;re sunk. The same is true for the models themselves; if you don&#8217;t have the artifacts you produced, you won&#8217;t be able to make statements about how they performed. Given source code and the training data, you could re-produce a model, but it almost certainly wouldn&#8217;t be the same because of randomization in the training process.</p>
<p>Second, the behavior of AI systems changes over time. Unlike a web application, they aren&#8217;t strictly dependent on the source. Models almost certainly react to incoming data; that&#8217;s their point. They may be retrained automatically. They almost certainly grow stale over time: users change the way they behave (often, the model is responsible for that change) and grow outdated.</p>
<p>This changes what we mean by &#8220;monitoring.&#8221; AI applications need to be monitored for staleness—whatever that might mean for your particular application. They also need to be monitored for fairness and bias, which can certainly creep in after deployment. And these results are inherently statistical. You need to collect a large number of data points to tell that a model has grown stale. It&#8217;s not like pinging a server to see if it&#8217;s down; it&#8217;s more like analyzing long-term trends in response time. We have the tools for that analysis; we just need to learn how to re-deploy them around issues like fairness.</p>
<p>We should also ask what &#8220;observability&#8221; means in a context where even &#8220;explainability&#8221; is always an issue. Is it important to observe what happens on each layer of a neural network? I don&#8217;t know, but that&#8217;s a question that certainly needs answering. Charity Majors&#8217; emphasis on <a href="https://thenewstack.io/observability-a-3-year-retrospective/">cardinality</a> and inferring the internal states of a system from its outputs is certainly the right direction in which to be looking, but in AI systems, the number of internal states grows by many, many orders of magnitude.</p>
<p>Last, and maybe most important: AI applications are, above all, probabilistic. Given the same inputs, they don&#8217;t necessarily return the same results each time. This has important implications for testing. We can do unit testing, integration testing, and acceptance testing—but we have to acknowledge that AI is not a world in which testing whether 2 == 1+1 counts for much. And conversely, if you need software with that kind of accuracy (for example, a billing application), you shouldn&#8217;t be using AI. In the last two decades, a tremendous amount of work has been done on testing and building test suites. Now, it looks like that&#8217;s just a start. How do we test software whose behavior is fundamentally probabilistic? We will need to learn.</p>
<p>That’s the basics. There are other issues lurking. Collaboration between AI developers and operations teams will lead to growing pains on both sides, especially since many data scientists and AI researchers have had limited exposure to, or knowledge of, software engineering. The creation and management of data pipelines isn’t something that operations groups are responsible for–though, despite the proliferation of new titles like “data engineer” and “data ops,” in the future I suspect these jobs will be subsumed into “operations.”</p>
<p>It’s going to be an interesting few years as operations assimilates AI. The operations community is asking the right questions; we&#8217;ll learn the right answers.</p>
<hr>
<h2>Upcoming events</h2>
<p><a href="https://www.oreilly.com/conferences">O’Reilly conferences</a> combine expert insights from industry leaders with hands-on guidance about today’s most important technology topics.</p>
<p>We hope you’ll join us at our upcoming events:</p>
<p><a href="https://conferences.oreilly.com/software-architecture/sa-ny">O’Reilly Software Architecture Conference</a>, New York, February 23-26</p>
<p><a href="https://conferences.oreilly.com/strata-data-ai/stai-ca">O’Reilly Strata Data &amp; AI Conference</a>, San Jose, March 15-18</p>
<p><a href="https://oreilly.formulated.by/scme-phoenix-2020/">Smart Cities &amp; Mobility Ecosystems Conference</a>, Phoenix, April 15-16</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/ai-meets-operations/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>Where programming languages are headed in 2020</title>
		<link>https://www.oreilly.com/radar/where-programming-languages-are-headed-in-2020/</link>
				<comments>https://www.oreilly.com/radar/where-programming-languages-are-headed-in-2020/#respond</comments>
				<pubDate>Mon, 13 Jan 2020 11:30:00 +0000</pubDate>
		<dc:creator><![CDATA[Zan McQuade and Amanda Quinn]]></dc:creator>
				<category><![CDATA[Innovation & Disruption]]></category>
		<category><![CDATA[Commentary]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=11305</guid>
				<description><![CDATA[As we enter a new decade, we asked programming experts⁠—including several of our own O&#8217;Reilly authors and instructors⁠—for their thoughts on what&#8217;s in store for some established players and fast-growing languages. Python The biggest news this year in Python is that creator and &#8220;benevolent dictator for life&#8221; Guido van Rossum retired, leaving Python in the [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>As we enter a new decade, we asked programming experts⁠—including several of our own O&#8217;Reilly authors and instructors⁠—for their thoughts on what&#8217;s in store for some established players and fast-growing languages.</p>
<h2>Python</h2>
<p>The biggest news this year in Python is that creator and &#8220;benevolent dictator for life&#8221; <a href="https://blog.dropbox.com/topics/company/thank-you--guido">Guido van Rossum retired</a>, leaving Python in the hands of the <a href="https://www.python.org/dev/peps/pep-8100/">Python Steering Council</a>. So far, it&#8217;s been a painless shift in power, which as Eric Matthes, author of <a href="https://learning.oreilly.com/library/view/python-crash-course/9781492071266/"><em>Python Crash Course</em></a>, argues, should come as no surprise, since &#8220;Guido has carried himself, and his role in the community, with such poise for so long.&#8221; 2020 will also see <a href="https://pythonclock.org/">the end of support for Python 2.7</a>, which will likely <a href="https://www.wired.com/story/think-app-updates-suck-try-upgrading-programming-language/">cause its share of headaches</a> among holdouts. Meanwhile, Python continues to be <a href="https://learning.oreilly.com/playlists/4d43bad2-2fcf-45f4-b8e4-a15f58442933/">the language of choice for data science</a>.</p>
<p>For Matthes, one exciting aspect of Python has been &#8220;the variety of interesting and critical projects that have come out of a community where diversity has so intentionally been built for so long.&#8221; <a href="https://www.willingconsulting.com/">Carol Willing</a>, a member of the Python Steering Council and a core developer of CPython, also celebrates these projects—like the Binder service, which promotes reproducible research by creating an executable environment from your Jupyter Notebooks—particularly as they expand beyond their initial aims. Binder, she notes, &#8220;was widely used last year for teaching workshops and tutorials at many Python conferences.&#8221; Willing also offered a shout-out to the CircuitPython and Mu projects, asking, &#8220;Who doesn&#8217;t love hardware, blinking LEDs, sensors, and using Mu, a user-friendly editor that is fantastic for adults and kids?&#8221;</p>
<h2>Java</h2>
<p>It&#8217;s mostly good news on the Java front. <a href="https://learning.oreilly.com/search/?query=Ben%20Evans&amp;extended_publisher_data=true&amp;highlight=true&amp;include_assessments=false&amp;include_case_studies=true&amp;include_courses=true&amp;include_orioles=true&amp;include_playlists=true&amp;include_collections=true&amp;include_notebooks=true&amp;is_academic_institution_account=false&amp;source=user&amp;sort=relevance&amp;facet_json=true&amp;page=0">Java Champion Ben Evans</a> explains, &#8220;Once again, rumours of Java&#8217;s demise have proved to be little more than wishful thinking on the part of the platform&#8217;s detractors.&#8221; But it hasn&#8217;t all been smooth sailing. As we noted last year, <a href="https://openjdk.java.net/projects/jdk/11/">the release of Java 11</a> in September 2018 brought a raft of new features, including many that give the release a significant, clear advantage for using containers. However, wide adoption of this latest release <a href="https://www.baeldung.com/java-in-2019">hasn&#8217;t followed suit</a>, with more than 80% of developers still on Java 8, according to <a href="https://www.jetbrains.com/lp/devecosystem-2019/java/">this JetBrains survey</a>. Evans wonders, &#8220;Does this mean that people aren&#8217;t running Java in containers as much as we&#8217;re told they are? Or do people just not know about the benefits of 11 in containers?&#8221;</p>
<p>Despite the slow rate of adoption, Java&#8217;s <a href="https://www.infoq.com/news/2017/09/Java6Month/">six-month release cadence</a> has been trucking along—Java 12 dropped in March 2019, with Java 13 following in September. And according to <a href="https://learning.oreilly.com/videos/moving-to-java/9780134857664">Java Champion Trisha Gee</a>, it&#8217;s really starting to show its value:</p>
<blockquote><p>Each release is quite small but predictable. And although they don&#8217;t all have exciting new language changes, you can see the language moving forward steadily. In addition, it enables this idea of preview features, which I think we saw working really well for switch expressions—developers got to try out the feature and give real feedback based on how it feels to work with, instead of feedback on abstract, conceptual ideas. In response, there was a small change to the syntax of switch expressions, which was possible due to it being a preview feature and not set in stone, in Java 13. Now this updated syntax is scheduled to be part of JDK 14 as a production-ready feature.</p></blockquote>
<p>2019 brought another surprise when Oracle moved Java SE to a subscription-based model. But as Marc Loy, coauthor of <a href="https://learning.oreilly.com/library/view/learning-java-5th/9781492056263/"><em>Learning Java</em>, fifth edition</a> (now in early release), points out, &#8220;The Java community at large has approached this unfortunate change with <a href="https://medium.com/@javachampions/java-is-still-free-c02aef8c9e04">increased enthusiasm for the OpenJDK</a>.&#8221;</p>
<p>As for the coming year, Evans suggests that 2020 will be about watching the 2019 trends play out:</p>
<blockquote><p>How close to a production version of <a href="https://wiki.openjdk.java.net/display/valhalla/Main">Project Valhalla</a> will we be? Will the incremental strategy of delivering pattern matching and algebraic data types (<a href="https://openjdk.java.net/projects/amber/">Project Amber</a>) pay off? Will <a href="https://developers.redhat.com/blog/2019/03/07/quarkus-next-generation-kubernetes-native-java-framework/">Quarkus</a> bear out its promise and the faith of its early fans? Will 2020 be the year that Kotlin makes a significant beachhead beyond Android? These are exciting times—we&#8217;re in transition toward something new, and there&#8217;s a lot going on.</p></blockquote>
<h2>Kotlin</h2>
<p>Google announced in May 2019 that Kotlin is now <a href="https://techcrunch.com/2019/05/07/kotlin-is-now-googles-preferred-language-for-android-app-development/">its preferred language for Android app developers</a>, boosting the language’s already strong adoption. Although many Android developers are still in the process of making the move to Kotlin, those who have already transitioned know the benefits it offers. Dawn and David Griffiths, authors of <a href="https://learning.oreilly.com/library/view/head-first-kotlin/9781491996683/"><em>Head First Kotlin</em></a>, share a few reasons behind Kotlin&#8217;s ascendance:</p>
<blockquote><p>For a language created by an IDE company, it&#8217;s no surprise that Kotlin has a healthy level of tooling support. The experimental DSL for code contracts gives developers the ability to provide guarantees about the ways that code behaves. Does your function have side effects? Is it guaranteed to return a non-null value? Code contracts allow you to make these promises, and the compiler can use them to loosen compile-time checks.</p>
<p>The barriers between different Kotlin platforms are now also breaking down. The &#8220;expect&#8221;/&#8221;actual&#8221; qualifiers allow developers to more easily write code that is compatible across Java/Native/JS environments. And serialization support now means that it&#8217;s even easier to convert JSON data into Kotlin objects, and vice versa.</p></blockquote>
<p>Expect to see Kotlin continue its impressive growth—and not just in Android. <a href="https://learning.oreilly.com/playlists/dd42ec64-a539-4bab-ad55-3473db4c7177/">Hadi Hariri</a>, leader of the developer advocacy team at JetBrains, points to the success of Kotlin/Everywhere—a series of community-led events where you can learn the essentials and best practices of Kotlin in Android, Google Cloud Platform, and multiplatform development—as proof: &#8220;From May to November, we&#8217;ve managed to reach close to 30,000 people in 86 countries. <a href="https://kotlinconf.com/">KotlinConf</a> sold out three years in a row with more than 1,700 attendees in 2019. This really shows, amongst other things, that interest and adoption of the language is growing.&#8221;</p>
<h2>Go</h2>
<p>When Gophers think back on 2019, they&#8217;ll likely remember the saga of the try proposal. Go developer and writer Jon Bodner explains:</p>
<blockquote><p>One of the most common complaints about Go is that error handling is too verbose. So in early June, the Go core developers proposed adding <a href="https://github.com/golang/proposal/blob/master/design/32437-try-builtin.md">a new built-in function called try</a>. <a href="https://github.com/golang/go/issues/32437">A GitHub issue was opened</a> to discuss this new feature. Within a month, there were nearly 800 comments, most of them negative. The people who were against the new feature felt that this change made code too &#8220;magical&#8221; and obscured the logic flow. After reviewing the feedback, the Go team marked the proposal as closed and rejected on July 16.</p></blockquote>
<p>What&#8217;s notable about this process wasn&#8217;t the failure of the feature but rather, as Bodner describes it, &#8220;the way the process happened: a feature was proposed, the discussion was respectful, but many felt that the change was inconsistent with Go&#8217;s style. In the end, the people who steward the language decided to respect the majority opinion. That&#8217;s what developers mean when they talk about community.&#8221;</p>
<p>2020 should bring more clarity to Go&#8217;s Contracts specification, better known as <a href="https://go.googlesource.com/proposal/+/master/design/go2draft-contracts.md">the Generics proposal</a>. According to Bodner, &#8220;It looks like Go is going to implement generics using an approach that is a bit different from other languages, but which fits nicely into the idioms of Go.&#8221; It will hopefully allow Go to keep its idiomatic style while adding a feature that developers have found useful in other languages.</p>
<h2>Rust</h2>
<p>We checked in with <a href="https://learning.oreilly.com/search/?query=author%3A%22Jim%20Blandy%22&amp;extended_publisher_data=true&amp;highlight=true&amp;include_assessments=false&amp;include_case_studies=true&amp;include_courses=true&amp;include_orioles=true&amp;include_playlists=true&amp;include_collections=true&amp;include_notebooks=true&amp;is_academic_institution_account=false&amp;source=user&amp;sort=relevance&amp;facet_json=true&amp;page=0">Jim Blandy</a>, coauthor of <a href="https://learning.oreilly.com/library/view/programming-rust/9781491927274/"><em>Programming Rust</em></a>, to see how his vision of Rust&#8217;s progress changed over the course of 2019. Last year, he noted that, “Rust has supported asynchronous programming in one form or another for a long time, but async functions provide a syntax for this sort of code that is a major improvement over what Rust has had before.” Did his hope for improvements to the Rust syntax come to fruition? Yes, eventually: Blandy explained that async/await syntax didn&#8217;t become stable until version 1.39, which was released November 7, 2019. &#8220;Originally, we were hoping async/await syntax could be part of the 2018 edition of Rust, but it took longer to get things right.&#8221; Still, he has high hopes for what async will mean for Rust in 2020: &#8220;Integrating async into the language lets the borrow checker understand what you&#8217;re doing, so asynchronous code can look like idiomatic Rust.&#8221; And as Blandy points out, the Rust ecosystem is acting quickly to take advantage of the language&#8217;s new expressiveness.</p>
<p>The Rust community is also excited about WebAssembly, which this year became a theoretical replacement to C/FFI for ecosystems that need portable, high-performance modules. And as Rust expert <a href="https://learning.oreilly.com/videos/oscon-2019/9781492050643/9781492050643-video325934">Nathan Stocks</a> notes, &#8220;You get light sandboxing as well!&#8221; What impressed Stocks most was &#8220;how much of the theory had been prototyped and demonstrated successfully.”</p>
<blockquote><p>I had previously thought of WebAssembly purely as a compilation target to run code from non-JS languages in the browser. The addition of the ability to consume web assembly from any language outside the browser was mind-bending.</p></blockquote>
<h2>Swift</h2>
<p>The biggest stories in Swift last year were the releases of <a href="https://developer.apple.com/xcode/swiftui/">SwiftUI</a>, Apple&#8217;s newest framework for designing user interfaces across all Apple devices, and <a href="https://www.tensorflow.org/swift">Swift for TensorFlow</a>, a platform for deep learning and differentiable programming integrating Google&#8217;s TensorFlow framework with Swift. SwiftUI, as <a href="https://learning.oreilly.com/videos/oscon-2019/9781492050643/9781492050643-video325927">Timirah James</a> explains, &#8220;has already gained so much traction amongst developers (rightfully so) with its declarative nature and is already being seen as a possible future successor to UIKit.&#8221; As for Swift for TensorFlow, <a href="https://secretlab.institute/2019/11/08/first-steps-swift-for-tensorflow/">Paris Buttfield-Addison</a> calls it &#8220;a radical new use for Swift.&#8221; He explains, &#8220;Swift has always been a great app development and systems programming language, and is a great up-and-coming web and back-end development language, but now, with Swift for TensorFlow, it&#8217;s a powerful ML framework, too.&#8221; Here&#8217;s why:</p>
<blockquote><p>Swift for TensorFlow is developed by a team that includes the original creator of Swift, Chris Lattner, and provides (or will, when it&#8217;s done) everything you need for machine learning and numerical computing. Most surprisingly is the full first-class support for <a href="https://en.wikipedia.org/wiki/Differentiable_programming">differentiable programming</a> with automatic differentiation, which is made possible by Swift&#8217;s underlying compiler framework and design.</p>
<p>Full in-language differentiable programming will make a whole collection of previously impossible things possible: the best example is being able to use a standard programming debugger to step through backpropagation and debug derivatives when you&#8217;re building a neural network.</p>
<p>Swift for TensorFlow also brings full Python support to Swift, allowing data scientists to mix and match the useful and familiar Python frameworks they need, with clean expressive Swift code.</p></blockquote>
<p>Looking ahead, both James and Buttfield-Addison are excited to see the new directions Swift takes, with James pointing to &#8220;Swift adoption across different communities and stacks beyond mobile, especially in the serverless realm,&#8221; and Buttfield-Addison calling out &#8220;amazing web development frameworks, like <a href="https://www.kitura.io/">Kitura</a>, and all sorts of amazing frameworks for niche areas&#8230;such as <a href="https://github.com/KarthikRIyer/swiftplot">SwiftPlot</a>, which is a Swift native version of the ubiquitous Matplotlib from Python.&#8221;</p>
<h2>What lies ahead?</h2>
<p>Change is inevitable, and as programming languages continue to lean in to optimization for new trends in the cloud, microservices, big data, and machine learning, each language and its ecosystem will continue to adapt in its own unique way. Big releases may be on the horizon in 2020 for certain languages—C++20 will be released this summer and Scala 3.0 is expected in late 2020—but what’s clear is even the smallest changes can cause huge waves in the daily lives of programmers.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/where-programming-languages-are-headed-in-2020/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>Rethinking programming</title>
		<link>https://www.oreilly.com/radar/rethinking-programming/</link>
				<comments>https://www.oreilly.com/radar/rethinking-programming/#respond</comments>
				<pubDate>Mon, 06 Jan 2020 12:00:00 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides]]></dc:creator>
				<category><![CDATA[Innovation & Disruption]]></category>
		<category><![CDATA[Radar Column]]></category>
		<category><![CDATA[Commentary]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=11275</guid>
				<description><![CDATA[We need to rethink the role of the programmer Look for the industry to become more stratified and specialized. The programming world will increasingly be split between highly trained professionals and people who don’t have a deep background but have a lot of experience building things. The former group builds tools, frameworks, languages, and platforms; [&#8230;]]]></description>
								<content:encoded><![CDATA[
<h2>We need to rethink the role of the programmer</h2>



<p>Look for the industry to become more stratified and specialized. The programming world will increasingly be split between highly trained professionals and people who don’t have a deep background but have a lot of experience building things. The former group builds tools, frameworks, languages, and platforms; the latter group connects things and builds websites, mobile apps, and the like. These two types of programmers have always existed, mixing fluidly. We just haven’t recognized the distinction, and that’s going to change. A good analogy is plumbing. If you need to install a toilet, you call a plumber: they know how to connect things together. There are jobs for people who design plumbing fixtures, but you wouldn’t want them working in your bathroom.</p>



<h2>We need to think about how programming is taught</h2>



<p>Like reading, some people learn how to code with little training, and others don’t. But as with reading, we shouldn’t accept a world in which some people enter primary school programming-literate, and those that don’t have to wait until high school. We’ll need teachers who are trained in teaching programming—specifically, teaching programming in the early grades. We already have programming environments that are optimized for teaching children, including <a href="https://scratch.mit.edu/">Scratch</a>, <a href="https://www.alice.org/">Alice</a>, and their relatives. And don’t discount the role gaming could play. <em>Minecraft</em> has unwittingly taught a generation of grade-schoolers how to program in Java.</p>



<p>We also need to build bridges for people with great programming skills but without a deep computer science background—the plumbers—to enter the professional market. Some of those bridges exist already; they include the many boot camps and schools like <a href="https://generalassemb.ly/">General Assembly</a> and <a href="https://www.holbertonschool.com/">Holberton</a>. These are distinct from college degree-granting programs (the traditional computer science major) and serve a different purpose. They’re more like vocational education programs: They’re focused on practice, with minimal emphasis on theory. They’re about learning to program in a professional context—working with a web platform, a database, or even an AI platform—but not about developing those platforms or databases. They’re for those who say, “Why should I know how to program quicksort? If I want to sort something, I’ll call a library function.” That’s just fine, and we shouldn’t pretend that it isn’t.</p>



<p>In contrast, CS majors should continue to be exposed to and work with theory and algorithms—not because they’re going to write their own quicksort but because we need people who can develop and implement new algorithms, and the best way to learn is to practice on algorithms we already understand. You don’t need to be good at math to program, but you do need math to push computing forward—particularly if you’re interested in data science or artificial intelligence.</p>



<h2>We need new, more sophisticated programming tools</h2>



<p>In “<a href="https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf">Hidden Technical Debt in Machine Learning Systems</a>,” the authors—a group of researchers and engineers from Google—argue that machine learning is a relatively small part of any application. Much of the rest is wiring things together: building data pipelines, connecting the application to the serving infrastructure, providing for monitoring. It&#8217;s not glamorous, but it needs to be done, and done correctly. I&#8217;d bet that much more downtime results from bad plumbing than from bad implementations of ML algorithms. Rather than relying on our current crop of languages, I wonder whether or not there are better languages for this part of the enterprise. It’s long seemed strange to me that programming languages aren’t all that different from what they were in the 1960s and 1970s: line-oriented, alphanumeric texts, most often in fixed-width type. <a href="https://en.wikipedia.org/wiki/Functional_programming">Functional languages</a> date back to the 1950s, and the earliest roots of <a href="https://en.wikipedia.org/wiki/Object-oriented_programming">object-oriented programming</a> aren’t much later. What would it mean to imagine other kinds of languages? Work is already being done on this front. There have been a surprising number of visual languages, which let users create programs using symbols or other graphic elements rather than text—although most have been unsuccessful. But even in popular languages like Scratch, we’re dealing with a simple mapping of visual objects to a traditional programming language: a “clamp” is a “loop,” a “variable” is a “box,” and so on. Is it possible to push even further beyond traditional programming languages? What would a programming language designed for plumbing look like? And would it give us better and more fruitful ways to think about the interconnections between systems? <em>— Mike Loukides</em></p>


<hr>
<h2>Upcoming events</h2>
<p><a href="https://www.oreilly.com/conferences">O’Reilly conferences</a> combine expert insights from industry leaders with hands-on guidance about today’s most important technology topics.</p>]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/rethinking-programming/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>5 industries that demonstrate how blockchains go beyond finance</title>
		<link>https://www.oreilly.com/radar/5-industries-that-demonstrate-how-blockchains-go-beyond-finance/</link>
				<comments>https://www.oreilly.com/radar/5-industries-that-demonstrate-how-blockchains-go-beyond-finance/#respond</comments>
				<pubDate>Mon, 16 Dec 2019 11:00:00 +0000</pubDate>
		<dc:creator><![CDATA[Alison McCauley]]></dc:creator>
				<category><![CDATA[Innovation & Disruption]]></category>
		<category><![CDATA[Blockchain]]></category>
		<category><![CDATA[Blockchain Enterprise Series]]></category>
		<category><![CDATA[Deep Dive]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=11177</guid>
				<description><![CDATA[Although blockchain technology is still in its early days, momentum has been building in the enterprise. While there has been much focus on blockchains in banking and payments, its impact has already extended far beyond finance. Here is a glimpse of just some of this work and the kind of intense learning that can be [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>Although blockchain technology is still in its early days, <a href="https://www.oreilly.com/radar/how-social-forces-could-drive-blockchain-demand/">momentum</a> has been building in the enterprise. While there has been much focus on blockchains in banking and payments, its impact has already extended far beyond finance. Here is a glimpse of just some of this work and the kind of intense learning that can be leveraged across industries.</p>



<h2>Health care</h2>



<p>With its high bar for data privacy and security, lessons learned in health care could inform development and practice in government and other regulation-heavy industries. Regulatory requirements make health care one of the more challenging industries to tackle, so change will take time to trickle down to patients in a way they really notice. But development has been underway for years already. Visionaries are hoping to leverage blockchains to provide a secure, universal data sharing infrastructure that can help solve decades-long problems endemic to the health care system. Because providers today don’t have a way to securely share data, there is hope that blockchain-driven collaboration could ultimately increase accuracy of diagnosis and coordination of treatment.</p>



<p>For example, MedRec is a system in development at MIT that seeks to make it possible for patients to manage their own records, giving permission to different providers to access and update data. The system is being tested with anonymized data, and could be used, for example, as a trusted central repository for vaccination, even if those vaccines were given by different providers. Gem and Tierion are startups working different aspects of data storage, verification and sharing (both partnered with Philips Healthcare), while Hu-manity.co partnered with IBM to give patients more control of their data and enable them to profit directly from sharing their data. MetLife can now pay claims instantly to expectant mothers who test positive for gestational diabetes—while this represents just a small sliver of business, it is easy to project how this learning could be extensible to other populations.</p>



<h2>Real estate</h2>



<p>Pioneers are working to increase liquidity, ease administrative burden, and open up the possibility of new business models in real estate. Their work could inspire innovations in other spaces with similar characteristics. They are using the technology to record, track, and transfer land titles, deed, and liens, and to facilitate payments, leasing, and sales. They are even exploring new ways to fractionalize real estate through tokenization (programmable representations of value).</p>



<p>The Swedish land registry authority, Lantmäteriet, has, with a diverse team of partners, been testing land registry on a private blockchain. Today, it can take months from the signing of a contract to register a sale, even though Lantmäteriet is already paperless and highly digitized. The registry is automating transactions on a blockchain, enabling buyers and sellers to digitally sign a bill of sale, with signatures verified automatically instead of at an agent’s office. When a land title changes hands, it would be verified via the blockchain, and recorded again. It’s still a long way until a blockchain-driven registry is widely available in Sweden, but estimates put taxpayers’ potential savings at over $100 million a year. In the U.S., the city of South Burlington in Vermont has also run pilots with land title and registry on a blockchain.</p>



<p>Platforms have emerged to facilitate the tokenization of real estate, which can help make an illiquid asset more liquid. Buyers purchase tokens, each representing a fraction of the asset. Last year, the owners of the St. Regis Aspen Resort in Colorado raised $18 million in this way, with each token valued at $1 USD.</p>



<h2>Consumer packaged goods</h2>



<p>A great deal of investment in blockchain technology has poured in from consumer packaged goods companies attracted by the promise of more supply chain transparency. Their challenging work—how, for example, do you digitize a strawberry?—could lay important groundwork for any industry looking to achieve more visibility of goods moving through their ecosystem.</p>



<p>More than 100 companies are involved with IBM’s Food Trust network, including many consumer packaged goods companies and grocery retailers. Consumers, retailers, and food safety organizations are demanding more transparency, and blockchain is looking to be a promising solution for this complex ecosystem.</p>



<p>Bumble Bee Foods is using a blockchain to achieve transparency in its supply chain all the way from the moment a tuna is caught by fishermen to arriving on grocery store shelves. The company is aiming to demonstrate provenance of its product to influence consumer preference and even pricing. Nestle has been testing blockchains in more than 10 projects, including IBM Food Trust, where it is using the technology to track provenance of ingredients in products like Gerber baby food.</p>



<p>Anheuser-Busch InBev is exploring a range of pilots. The brewing company is giving some consumers an option to upload their driver’s license information to a blockchain, which verifies age so they can then buy beer at a vending machine simply by scanning their phone. Anheuser-Busch InBev also has a partnership with BanQu that uses a blockchain to facilitate payments to farmers who don’t have bank accounts. In fast growing markets, like Africa, this could enable the company to meet demand more efficiently by making it possible to work with more farmers.</p>



<h2>Advertising</h2>



<p>Digital advertising depends on a complex ecosystem of advertisers, publishers, and middlemen, and is plagued by widespread fraud. Blockchain provides hope for smarter, more secure advertising that generates higher yields. Entrepreneurs and intrapreneurs are exploring many angles, including using blockchains for higher quality data, targeting, and reducing costs. This work could inform a range of industries that are operating with large ecosystems characterized by fraud or high-toll intermediaries.</p>



<p>For example, Comcast and competitors Viacom and Spectrum Reach have partnered to launch Blockgraph, which seeks to enable more precise targeting without disclosing viewers’ personal information to advertisers. Brave is a new browser that sees to attack each layer of inefficiency in the browser ecosystem. This new model uses a blockchain to eliminate the middlemen that today take more than half the revenue. Instead of going to Google or Facebook, for example, advertisers will list directly onto Brave’s blockchain-based browser. Both advertisers and publishers get more value, and consumers get fewer but better targeted ads.</p>



<h2>Insurance</h2>



<p>The application of blockchain in the insurance industry is an area of great exploration. The technology is being targeted as a way to detect fraud, improve the efficiency of claims processing, and simplify the flow of data and payments between insurers and reinsurers. But it is also being explored as a key mechanism for innovative approaches to insurance. Blockchains (along with a new flood of IoT-driven data that can be used to calculate risk and monitor assets) could make it easier for insurers to offer granular protection that flexes and changes with individual needs. It drives costs down and enables unique identifiers, immutable tracking, and contracts that can be executed without a human—making it possible to make insurance work for smaller and smaller things or periods of time. This may also make it easier for non-insurers to bundle insurance with their products and services, creating new, innovative packaging that drives customer preference in a broad range of industries.</p>



<p>Insurance giant Allianz SE has been testing a range of blockchain applications. For example, through a joint venture, the company is testing flight-delay insurance that could be automatically initiated as soon as a flight is delayed by the insured delay time. It is also involved with a project to speed insurance payments to health providers. State Farm is working on a blockchain to speed the process in which insurers seek partial reimbursements for claims from other insurers involved with the claim, today a painful process with high administrative costs.</p>



<h2>Momentum takes shape across the business landscape</h2>



<p>These industries provide just a glimpse of the kinds of projects that are building momentum. Just about every industry has both established and disruptive players looking to use blockchain technology to evolve the way business is done. According to Deloitte in their <a href="https://www2.deloitte.com/insights/us/en/topics/understanding-blockchain-potential/global-blockchain-survey.html">2019 Global Blockchain Survey</a>, “the question for executives is no longer, ‘will blockchain work?’ but, ‘how can we make blockchain work for us?”. While many enterprise projects tend to be lodged in the deep back office, they create a foundation that could be leveraged to one day impact the way we work and play. As blockchain savvy teams around the globe build and <a href="https://www.forbes.com/sites/alisonmccauley/2019/05/23/is-collaboration-the-new-competition-blockchains-flip-conventional-mba-wisdom/#7aa2ab8c52da">gather allies</a>, projects could build, over time, to something pervasive and difficult to outrun.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/5-industries-that-demonstrate-how-blockchains-go-beyond-finance/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>A 5G future</title>
		<link>https://www.oreilly.com/radar/a-5g-future/</link>
				<comments>https://www.oreilly.com/radar/a-5g-future/#respond</comments>
				<pubDate>Mon, 02 Dec 2019 10:00:00 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides]]></dc:creator>
				<category><![CDATA[Innovation & Disruption]]></category>
		<category><![CDATA[Radar Column]]></category>
		<category><![CDATA[Commentary]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=11075</guid>
				<description><![CDATA[For the past year, 5G cell technology has generated a lot of excitement–and a lot of hype. The specifications are impressive: 5G will provide a peak data rate of up to 20 Gbps (with 100 Mbps of “user experienced data rate”) to mobile devices: cell phones, smart cars, and a lot of devices that haven’t [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>For the past year, 5G cell technology has generated a lot of excitement–and a lot of hype. The specifications are impressive: 5G will provide a <a href="https://www.etsi.org/technologies/5g">peak data rate</a> of up to 20 Gbps (with 100 Mbps of “<a href="https://5gobservatory.eu/info-deployments/5g-performance/">user experienced data rate</a>”) to mobile devices: cell phones, smart cars, and a lot of devices that haven’t been invented yet. It’s difficult to imagine mobile applications that will require that much data, and 5G’s proponents seem willing to promise just about anything. What will 5G mean in practice? If it’s going to make any real difference, we’ll need to think that through.</p>



<p>The most obvious change 5G might bring about isn’t to cell phones but to local networks, whether at home or in the office. Back in the 1980s, Nicholas Negroponte <a href="https://books.google.com/books?id=McBUx67rw7wC&amp;pg=PA338&amp;lpg=PA338&amp;dq=negroponte+everything+wired+will+become+wireless,+and+everything+wireless+will+become+wired.&amp;source=bl&amp;ots=cJTyDDyWhR&amp;sig=ACfU3U14LnXEcU_lGE6lm5jHFk30-mCQ1w&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwiRsZOD7-DlAhWsuFkKHZg9CqwQ6AEwBXoECAkQAQ#v=onepage&amp;q=negroponte%20everything%20wired%20will%20become%20wireless%2C%20and%20everything%20wireless%20will%20become%20wired.&amp;f=false">said</a> everything wired will become wireless, and everything wireless will become wired. What happens to “last mile” connectivity, which seems to be stuck somewhere around 50 Mbps for homes and several times that for business service? It would be great to have an alternative to the local cable monopoly for high-bandwidth connectivity. We were supposed to have fiber to the home by now. I don’t, do you? High-speed networks through 5G may represent the next generation of <a href="https://en.wikipedia.org/wiki/Cord-cutting">cord cutting</a>. Can 5G replace wired broadband, allowing one wireless service for home and mobile connectivity? I don’t need more bandwidth for video conferences or movies, but I would like to be able to download operating system updates and other large items in seconds rather than minutes. Anyone who has ever built a Docker container has experienced “now we wait for some giant things to download and be uncompressed.” Those waits can be significant, even if you’re on a corporate network. They could disappear.</p>



<p>Rural connectivity is a persistent problem; many rural users (and some urban users) are still limited to dial-up speeds. Although the industry <a href="https://www.zdnet.com/article/the-realities-of-rural-5g-deployment-in-the-us/">claims</a> that 5G will provide better connectivity for rural areas, I’m skeptical. Because 5G uses higher frequencies than 4G, and higher frequencies are more subject to path loss, 5G cells have to be smaller than 4G/LTE cells. If carriers won’t build cell towers for current technology, they aren’t likely to build even more towers for 5G. I suspect rural communities will be left in the dark–again.</p>



<p>As far as mobile and embedded devices go, I don’t see why I need a gigabit on my phone, except perhaps to serve as a Wi-Fi hub when traveling. Phones are a painful way to watch movies–more about that later. 5G enthusiasts frequently say it’s an enabling technology for autonomous vehicles (AV), which will need high bandwidth to download maps and images, and perhaps even to communicate with each other: AV heaven is a world in which all vehicles are autonomous and can therefore collaboratively plan traffic. That may well require 5G–though again, I wonder who is going to make the investment in building out rural networks. Autonomous vehicles that only work in urban or suburban areas are less useful. For applications like communication between AVs, latency–how long it takes to get a response–is more likely to be a bigger limitation than raw bandwidth, and is subject to limits imposed by physics. There are impressive <a href="http://www.techplayon.com/5g-nr-control-plane-latency-calculations/">estimates</a> for latency for 5G, but reality has a tendency to be harsh on such predictions. Reliability will be an even bigger problem than latency. Remember your last trip to New York or San Francisco? Cell service in major cities is often poor because signals are reflected from buildings and attenuated (weakened) as they pass through. Those problems get worse as you go higher in frequency, as 5G does. Whether you’re interested in AVs or some other applications, making mobile connections more reliable is more important than making them faster. 5G intends to do so by trading off congestion against signal quality. That’s a plausible tradeoff, but it remains to be seen whether it works.</p>



<p>Pete Warden, who is working on <a href="https://learning.oreilly.com/library/view/tinyml/9781492052036/">machine learning for very low power devices</a>, says 5G is only marginally useful for the applications he cares about. When you’re trying to build a device that will run for months on a coin battery, you realize that the radio takes much more power than the CPU. You have to keep the radio off as much as possible, transmitting data in short, brief bursts. So what about industrial IoT (IIoT), and sensors that can be built into a <a href="https://www.oreilly.com/radar/tinyml-the-challenges-and-opportunities-of-low-power-ml-applications/">sticker</a> and slapped on to machinery? That might be a 5G application–but as Warden has said, the real win here is eliminating batteries and power cords, which in turn requires careful use of low-power networking. 5G isn’t ideal for that, and the first indications are that it will require more power than current technologies.</p>



<p>Regardless of power consumption, I’m not convinced we’ll have lots of IoT devices shipping data back to their respective motherships. We’ve seen the reaction to news that Amazon’s Echo and Google Home send recordings of conversations back to the server. And we’re already seeing devices like smart thermostats and light bulbs being used for <a href="https://www.nytimes.com/2018/06/23/technology/smart-home-devices-domestic-abuse.html">harassment</a>. As privacy regulation takes hold and techniques like <a href="https://ai.googleblog.com/2017/04/federated-learning-collaborative.html">federated learning</a> become more widespread, the need–and desire–for shipping our data far and wide will inevitably decrease.</p>



<p>So where is 5G useful? Let’s get back to home networking. I’d gladly give up my 50 Mbps wired connection for gigabit wireless. Again, that’s the ultimate cable cutting, and it creates significant new possibilities. I might not want to watch 4K video on my phone (given current screen technology, to say nothing of our eyes’ angular resolution, high-resolution video on a phone is meaningless), but I might want to send video from my phone to my television using Chromecast.</p>



<p>I’m satisfied with my current Wi-Fi deployment, but I wonder whether I’d even need Wi-Fi in a 5G world. Perhaps, for security and privacy reasons, it makes sense to separate a local network from the rest of the world. But that’s also a problem that 5G vendors could solve; virtual LANs (<a href="https://en.wikipedia.org/wiki/Virtual_LAN">VLANs</a>) are hardly a new concept. Gigabit connectivity to laptops, with the cell network providing a VLAN, could also replace office networks. In either case, some hard guarantees about privacy and security would be needed. Given <a href="https://www.privacypolicies.com/blog/isp-tracking-you/">service providers’ records on user tracking</a>, that may be too much to ask.</p>



<p>If we can get some enforceable guarantees about privacy and security on ISP-provided VLANs, I can imagine bigger changes. I’ve long thought it makes little sense to maintain disk drives (whether rust-based or solid-state) that periodically fail and need to be backed up. I do regular backups, but I know I’m the exception. What would the world look like if all of our storage was in the cloud, and access to that storage was so fast we didn’t care? What if all of your documents were in Google Docs, all of your music was in your favorite streaming service? That vision isn’t entirely new; Sun Microsystems had the idea back in the 1990s, and that’s essentially the vision behind Google’s Chromebooks.</p>



<p>How would our usage patterns change with 5G? I have 30 or 40 GB of photos. I could upload them all to Google Photos or some other service, but at 50 Mbps down and 10 Mbps up, that’s not something you want to think about. At a gigabit, you don’t have to think twice. I’ve always been unimpressed by streaming services for music and video, at least partly because they’re least available when you most want them: when you’re flying or on a train, in at a technical conference with 3,000 attendees maxing out the hotel’s network. (Someone once told me “so download everything you’re likely to want to listen to before leaving.”&nbsp; Really.)&nbsp; But with gigabit microcells, this suddenly makes sense. Maybe not on flights, which are out of range of cell towers and where WoeFi will remain the order of the day, and maybe not when you’re driving through rural areas, but if I can get a gigabit network to my phone, why should I care about Amtrak’s slow Wi-Fi or network congestion in my hotel? If an office can get that kind of bandwidth to my laptop, with adequate guarantees for cloud security, why should we worry about office LANs?</p>



<p>Whether that excites you or not, that strikes me as a significantly new pattern: we won’t care where our data is. We won’t need to worry about backups. We won’t need to worry (as much) about outages. We can bring our networks with us. We won’t even need to worry as much about security; Google, Amazon, and Microsoft all do better backups than I ever will, are much better at surviving network disruption, and know an awful lot more about how to protect my data. If Google can push their users to <a href="https://authy.com/what-is-2fa/">two-factor authentication</a> (2FA) or the use of a <a href="https://www.extremetech.com/computing/274067-google-eliminated-phishing-by-giving-all-85000-employees-usb-security-keys">security dongle</a>, that’s a huge step toward safe computing. Those cloud providers will, of course, have to guarantee this data remains private–as private as it is when it lives on a personal disk drive or an office fileserver. That’s a problem that’s <a href="https://en.wikipedia.org/wiki/FBI%E2%80%93Apple_encryption_dispute">eminently solvable</a>.</p>



<p>The implications for business are even more important. Home users think in gigabytes; businesses are increasingly involved with tera- or petabytes. It’s a lot easier to move large datasets when you have ubiquitous gigabit networks. Whether that’s training data for AI applications or just lots of transaction records, businesses move data, and lots of it. With our current technology, the best way to move a huge amount of data is, all too often, to put <a href="https://aws.amazon.com/snowmobile/">disk drives on a truck</a>. 5G brings us a lot closer to solving that problem–if we can get hard guarantees about security and privacy. Businesses are even less likely than users to appreciate some third party using their data for their own purposes.</p>



<p>I’m sure that 5G will also lead to a new generation of smart devices that can use the bandwidth–devices we haven’t imagined yet. But I’m more interested in something I can imagine, decoupling myself from my data: having access to it any time, any where, without carrying it around, or stashing it on some kind of machine in the closet. That’s the real promise of 5G. <em>— Mike Loukides</em></p>



<hr class="wp-block-separator" />



<h2>Radar data points: Recent research and analysis</h2>



<p>We recently conducted a <a href="https://www.oreilly.com/radar/oreilly-serverless-survey-2019-concerns-what-works-and-what-to-expect/">survey on serverless architecture adoption</a>. We were pleasantly surprised at the high level of response: more than 1,500 respondents from a wide range of locations, companies, and industries participated. The high response rate tells us that serverless is garnering significant mindshare in the community.</p>



<p>Key findings from the serverless survey include:</p>



<ul><li>40% of respondents work at organizations that have adopted serverless architecture in some form or another. Reduced operational costs and automatic scaling are the top serverless benefits cited by this group.</li><li>Of the 60% of respondents whose companies haven’t adopted serverless, the leading concerns about the paradigm are security and fear of the unknown.</li><li>About 50% of respondents who adopted serverless three-plus years ago consider their implementations successful or extremely successful, a contrast to the 35% of those adopting serverless a year or less ago experiencing a successful or extremely successful implementation—a gap that suggests serverless experience pays off.</li><li>Respondents who have implemented serverless made custom tooling the top tool choice—implying that vendors’ tools may not fully address what organizations need to deploy and manage a serverless infrastructure.</li></ul>



<p>Read “<a href="https://www.oreilly.com/radar/oreilly-serverless-survey-2019-concerns-what-works-and-what-to-expect/">O’Reilly serverless survey 2019: Concerns, what works, and what to expect</a>” for full results. Also be sure to check out our <a href="https://www.oreilly.com/radar/tag/research/">archive of Radar research and analysis</a>.</p>



<hr class="wp-block-separator" />



<h2>Upcoming events</h2>



<p><a href="https://www.oreilly.com/conferences">O’Reilly conferences</a> combine expert insights from industry leaders with hands-on guidance about today’s most important technology topics.</p>



<p>We hope you’ll join us at our upcoming events:</p>



<p><a href="https://conferences.oreilly.com/software-architecture/sa-ny">O’Reilly Software Architecture Conference in New York</a>, February 23-26, 2020</p>



<p><a href="https://conferences.oreilly.com/strata/strata-ca">Strata Data Conference in San Jose</a>, March 15-18, 2020</p>



<p><a href="https://conferences.oreilly.com/artificial-intelligence/ai-ca">O’Reilly Artificial Intelligence Conference in San Jose</a>, March 15-18, 2020</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/a-5g-future/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>Why you should care about robotic process automation</title>
		<link>https://www.oreilly.com/radar/why-you-should-care-about-robotic-process-automation/</link>
				<comments>https://www.oreilly.com/radar/why-you-should-care-about-robotic-process-automation/#respond</comments>
				<pubDate>Thu, 21 Nov 2019 12:00:00 +0000</pubDate>
		<dc:creator><![CDATA[Sunil Ranka and Roger Magoulas]]></dc:creator>
				<category><![CDATA[Innovation & Disruption]]></category>
		<category><![CDATA[Deep Dive]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=11021</guid>
				<description><![CDATA[In a classic 1983 paper, cognitive psychologist Lisanne Bainbridge drew attention to a curious irony: so-called “automated” systems were, in practice, usually attended by one or more human operators. The more advanced the system, she observed, “the more crucial…the contribution of the human operator.” Bainbridge believed that automation designers were in denial about this, however. [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>In <a href="https://www.ise.ncsu.edu/wp-content/uploads/2017/02/Bainbridge_1983_Automatica.pdf">a classic 1983 paper</a>, cognitive psychologist <a href="https://en.wikipedia.org/wiki/Lisanne_Bainbridge">Lisanne Bainbridge</a> drew attention to a curious irony: so-called “automated” systems were, in practice, usually attended by one or more human operators.</p>
<p>The more advanced the system, she observed, “the more crucial…the contribution of the human operator.” Bainbridge believed that automation designers were in denial about this, however. As she saw it, designers approached the problem of automation as if the human factor <em>were not</em>, in fact, a factor. This resulted in systems that left their (inevitable) human operators “with an arbitrary collection of tasks” with respect to which “little thought may have been given to providing support.”</p>
<p>This is precisely the kind of problem that robotic process automation (RPA) aims to address.</p>
<h2>RPA explained</h2>
<p>To understand what RPA is, why it matters, and <a href="https://cloudblogs.microsoft.com/dynamics365/bdm/2019/11/04/announcing-rpa-enhanced-security-no-code-virtual-agents-and-more-for-microsoft-power-platform/">why it’s garnered so much interest</a>, you have to understand the case of “Maddie.” Full disclosure: Maddie is a fictional person. She’s a composite of hundreds of thousands (perhaps even millions) of smart, industrious people who are tasked with the IT equivalent of <a href="https://en.wikipedia.org/wiki/Sisyphus">rolling a large boulder up a hill</a>. One of Maddie’s responsibilities—she has plenty of others—is to manipulate a mouse and click the appropriate buttons (“OK,” “Yes,” “Import”) when prompted to do so by a legacy Windows application. Once an hour, on the hour, Maddie sits in front of an old-school beige computer and clicks a mouse button several hundred times. She’s the vestigial human link in a process—insurance claims processing—that has a mostly automated workflow.</p>
<p>Maddie has other responsibilities that challenge her ingenuity, but it’s the drudgery of pointing-and-clicking a mouse that defines her job for her. RPA is a beacon of hope for the Maddies of the world. It enlists software “robots” to automate the rote, repetitive, or tedious tasks that bridge virtual gaps, or facilitate virtual transfers or exchanges, in and between common business processes.</p>
<p>Whether it’s copying and pasting text, scraping screens, pointing and clicking (or dragging and dropping) with a mouse, saving changes in one program and importing them into another, etc., software robots are able to interpret inputs, events, and gestures, and trigger responses and communicate with other systems—just as humans do. In fact, robots arguably do these things <em>better</em> than humans do them. Unlike humans, a software robot doesn’t get tired, bored, or distracted, especially when its performing a tedious or repetitive task. Unlike humans, a software robot performs tasks precisely the same way each and every time. Unlike humans, a software robot generates records for each and every task in a workflow. Most important, unlike humans, a robot does not have a capacity for creativity or ingenuity. Robots cannot reflect on and derive innovative solutions to hard problems. They can do only what they’re programmed to do. Core to RPA is the idea that the software robot is the perfect candidate for a range of tasks that neither tax human ingenuity nor engage human passion.</p>
<h2>Workflow and back-office automation</h2>
<p>Virtual automation has a long history. Prior to RPA, enterprises used several different techniques to automate workflows and back-office processes. Scripting, for example, is the ur-IT automation technology. Virtually anything can be scripted—including keyboard, mouse, and GUI actions. Speaking of mice, another automation technology—screen-scraping—dates from before the widespread use of graphical user interfaces (GUI), mice, and other <a href="https://en.wikipedia.org/wiki/Tablet_computer">even newer types of human interface devices</a>. Even though mainframe technologies evolved—IBM’s System/360 mainframe gave way to System/370, which was superseded by System/380, and so on—the programs that ran on these systems often did not.</p>
<p>In the late 1960s, following the shift from punch cards to magnetic tape, enterprises used optical character recognition (OCR) technologies to digitize their punch-card archives. In the 1980s, they used screen-scraping techniques to capture data from legacy mainframe and minicomputer programs and expose it in different contexts—such as on then-new PCs and Macintoshes. Two decades later, enterprises used screen-scraping to web-enable legacy mainframe, minicomputer, and client-server applications. Speaking of client-server, the succession of paradigm shifts from host-based to client-server to cloud-based architectures helped substantively automate many common workflows and processes. Yes, automation has usually eliminated a small proportion of human jobs; at the same time, however, it has created new opportunities for those well-versed in software, databases, architecture, and operations.</p>
<p>But RPA is different. <strong>First</strong>, RPA automates tasks that a human being must perform in a <em>GUI</em>, as distinct to a text-only terminal. These are tasks that comprise gaps in the midst of critical workflows or processes; tasks that (as often as not) connect or span separate programs, systems, or networks.</p>
<p><strong>Second</strong>, RPA is smart; it uses machine learning (ML) techniques that learn on the basis of human action in the visual interface. Just by manipulating the mouse and clicking a sequence of “Yes,” “OK,” etc., prompts, a user like Maddie generates a training data set for a software robot to work with and learn from. The Achilles heel of scripts, mouse macros, and other established practices is that they all depend on human prognostication. Imagine a developer writes a script that triggers an action in response to one or more events: say, for example, some combination of function calls and GUI events. For the script to work reliably, however, the developer must anticipate not only expected events (<em>first</em>, the script moves the cursor to the designated X,Y coordinates; <em>second</em> it checks to make sure the designated window is activated; <em>only then</em> does it simulate a mouse click) but <em>exceptions</em>, too—<a href="https://en.wikipedia.org/wiki/Edge_case">edge cases</a>, as when a dialog box captures control of the screen and obscures the “OK” prompt. RPA technology uses ML to discover, understand, and adapt to edge cases that a human user manipulating a mouse could easily deal with—but which would bring a dumb script grinding to a halt.</p>
<p><strong>Third</strong>, RPA doesn’t assume that the primary goal of automation is to replace human beings. RPA technologies automate the kinds of tasks that human beings <em>don’t like doing</em>. RPA targets tedious, repetitive, rote, time-consuming tasks. Proponents like to claim that RPA frees human actors (like Maddie) to focus on responsibilities and activities that engage their ingenuity and creativity.</p>
<p><strong>Fourth</strong>, RPA expects program, system, and even network heterogeneity. RPA evolved to eliminate gaps in workflows or processes that span disparate GUI-based systems. A history lesson might be helpful here. In the 1990s, <a href="https://en.wikipedia.org/wiki/Software_suite">packaged software suites</a> emerged to displace fit-for-purpose GUI and text-based applications. An insurance company might once have depended on a mix of custom-built and commercial systems to support key processes such as enrollment, billing, claim filing, and claim adjustment; by the late-1990s, however, packaged applications were able to replicate many (if not most) of the features and functions these systems provided. <em>But not all of them</em>. More important, some subset of function-specific systems just couldn’t be replaced. The upshot was that even as enterprises restructured their business processes to accommodate packaged suites, they kept some of these processes (and their supporting IT systems) intact, too. The neat thing about RPA is that its software bots run <em>alongside</em> the GUI-based program(s) on the existing system. RPA does not modify the code of an existing system; it doesn’t even require that someone (a developer, engineer, etc.) <em>understand</em> this code. Software bots on one system can also interoperate with software bots on other systems; this permits RPA to knit together disparate systems into a single (managed) process flow.</p>
<p>All of this helps account for <a href="https://trends.google.com/trends/explore?date=all&amp;geo=US&amp;q=robotic%20process%20automation">keen and trending interest</a> in RPA. In most cases, RPA is a less expensive alternative to conventional practices: to approximate its efficacy (and to account for most possible problems, hiccups, or exceptions) a developer would have to observe what a user like Maddie does over a protracted period of time.</p>
<p>RPA also <a href="https://www.mckinsey.com/business-functions/mckinsey-digital/our-insights/the-next-acronym-you-need-to-know-about-rpa">delivers return on investment</a> via increased employee productivity, improvements in process efficiency, built-in auditability, and superior accuracy. (Humans grow bored, get distracted, and don’t notice when they make mistakes. When a software bot makes a “mistake,” it triggers an exception.)</p>
<aside data-type="sidebar">
<h2>Exception handling</h2>
<p>One critical factor for success with any RPA project is exception handling. In practice, the problem of anticipating and controlling for exceptions—i.e., anomalous events—constitutes the most important and challenging aspect of implementing an RPA project. Software robots must learn to identify and control for scenarios that human beings take for granted, such as slow, non-responsive, and frozen applications—or, worse, application errors or crashes. But controlling for application performance, availability, and dependency issues is extremely complicated.</p>
<p>One approach is for software robots to poll for resource availability and collect detailed statistics about the host operating system’s state. The robot will use this data to train an ensemble of machine learning (ML) models to identify and handle exceptions. In most cases, however, <em>human beings</em>, not ML models, devise the rules that robots will use to determine what to do when a program is unresponsive or unavailable. How long should the robot wait for a response? Does the robot kill and re-spawn unresponsive programs or processes? These are just a few of the questions a designer must anticipate and control for.</p>
<p>The edge cases can quickly multiply, especially in scenarios in which critical programs and operating system libraries are implicated. And that’s the thing: all conceivable exceptions must be accounted for (and built into) the RPA design. An event that might cause a human being to do a double take can bring an RPA process to a complete halt.<br />
</aside>
<h2>What should be automated?</h2>
<p id="f1">RPA automates actions, inputs, behaviors, etc., <em>in the user interface</em>. It is ideal for integration projects that require human manipulation of user interface elements and involve heterogeneous systems. RPA technologies exploit standard<a href="#_ftn1"><sup>[1]</sup></a> APIs, so integrators won’t have to tinker with existing applications, workflows, processes, or system architecture. In this way, RPA can help reduce the amount of work (along with much of the risk) necessary to integrate heterogeneous applications and systems. This is one of the biggest factors driving the cost-effectiveness of RPA implementations.</p>
<p>RPA is not a panacea. Some tasks, even if tedious or repetitive, will require manual human oversight and control. As always, careful analysis of in-process workflows enables you to determine the best overall candidates for robotic automation.</p>
<p>In general, the following are good places to start with RPA:</p>
<ul>
<li><strong>Rote and repetitive tasks</strong>. Pointing-and-clicking a mouse. Copying-and-pasting text.</li>
<li><strong>Testing and validation</strong>. Some visual interfaces require substantial time and effort to test prior to deployment. RPA can radically accelerate this process, improve testing, and reduce costs.</li>
<li><strong>Redundant tasks</strong>. Basic tasks that multiple users tend to perform in parallel in an organization.</li>
<li><strong>Manual tasks with limited variability and few exceptions</strong>. Tasks that are consistent, repeatable, and/or highly predictable are excellent candidates for robotic automation.</li>
<li><strong>Human-orchestrated “integrations</strong>.” A user manually copies data from one visual interface and pastes it into another, or a user manually imports the output of one program into another.</li>
<li><strong>Any tedious or time-consuming task that looks like it might be a good candidate for automation</strong>. Automating the task should free one or more humans to do more productive work.</li>
</ul>
<p>The above benefits can lower costs, improve efficiency and quality, and can economically scale, all adding up to significant return on investment for the right RPA projects.</p>
<p>The following are probably <em>not</em> good candidates for RPA, however:</p>
<ul>
<li><strong>One-off tasks</strong>. A task or process that only needs to run once is not worth an investment in RPA.</li>
<li><strong>Unpredictable tasks</strong>. Tasks involving ad hoc or context-sensitive input are not viable for RPA.</li>
<li><strong>Turing test-like tasks</strong>. Similarly, tasks that require some kind of visual confirmation—such as, for example, <a href="https://en.wikipedia.org/wiki/CAPTCHA">CAPTCHA</a>s—are not always suitable for RPA. This isn’t to preclude <em>all</em> such tasks, however; for example, you could use RPA to automate a process that tests for and validates images of scanned checks. You could even use RPA to test and validate that a CAPTCHA is working. RPA rule of thumb: if it’s repeatable and predictable, it can be automated.</li>
<li><strong>Decision-laden processes. </strong>Processes that require human decision making and that involve multiple decision-making stakeholders aren’t usually viable candidates for RPA. If you aren’t comfortable automating some or all of the decisions in a process flow, RPA isn’t a good fit.</li>
<li><strong>KISS tasks</strong>. Keep it simple, stupid: if it’s something a human can do quickly, easily, and non-onerously, it doesn’t make sense to automate it. Ditto for tasks/processes that offer low ROI.</li>
</ul>
<h2>“Doing” RPA</h2>
<p>Fundamentally, RPA is an integration and interoperability technology. There is not a one-size-fits-all blueprint for “doing” RPA. There can’t be: every company and every integration or interoperability scenario is different. However, thanks to RPA’s extensive use in the health care, insurance, oil and gas, and government verticals, would-be adopters can avail themselves of resources—e.g., case studies and ROI studies, along with industry-specific consulting and integration services—that will assist them as they plan their RPA implementations. As RPA sees widespread use across <em>all</em> verticals, adopters should expect to encounter complex, unpredictable, and challenging scenarios.</p>
<p>Experience with and analysis of the RPA space leads to a formalized list of “Do’s and Don’ts” adopters can use to troubleshoot their RPA implementation projects.</p>
<h3>RPA Dos</h3>
<ul>
<li>An RPA initiative must have the backing of one or more C-level executives; support from a single line of business (or from IT alone) isn’t sufficient.</li>
<li>An RPA initiative must be aligned with the organization’s overall digital transformation strategy.</li>
<li>Engaging experts and vendor tools can help speed RPA adoption for those new to the topic.</li>
<li>Once a project is up and running, establishing an RPA <a href="https://en.wikipedia.org/wiki/Center_of_excellence">center of excellence</a> can help standardize and disseminate best practices across an organization to foster and speed adoption of RPA where appropriate.</li>
</ul>
<h3>RPA Don’ts</h3>
<ul>
<li>Don&#8217;t pursue RPA as a silo-ed, independent, or skunkworks project; align RPA with your organization’s overall vision.</li>
<li>Don’t automate a process just for the sake of automating it; RPA gives you a chance to <em>optimize</em> existing processes.</li>
<li>Don’t rush into automating a process; plan, test, and validate to achieve success with RPA.</li>
</ul>
<p>Teams play a very important role in the success of an RPA implementation. Two common RPA-specific job roles are RPA developer and RPA solutions architect; these specialists will augment and liaison with an internal IT team in implementing RPA.</p>
<h2>Conclusion</h2>
<p>Over the last two to three years, RPA has emerged as a promising technology for workflow and back-office automation. Unlike older automation technologies—such as scripting and screen-scraping—RPA is designed to automate the kinds of tasks that are performed by human users who must interact with and manipulate elements in a graphical user interface. These include custom or proprietary applications that expose basic prompts (buttons, dialog boxes, etc.) or other prompts that expect a user to perform an action of some kind (enter text or number values, left- or right-click a mouse, press a sequence of keys on a keyboard) in response to a visual prompt.</p>
<p>The return on investment for RPA projects can be substantial; this alone helps explain the keen interest in the topic. This interest shows no signs of abating. And with Microsoft’s announcement of a new RPA-oriented cloud offering, <a href="https://flow.microsoft.com/en-us/ui-flows/">UI Flows</a>, RPA seems poised for mainstream adoption. “Adoption” doesn’t necessarily equal success, however. Like any new technology, RPA’s potential rewards are offset by nontrivial risks. If you understand what RPA can and can’t do, you can make an informed decision about whether RPA is a viable option for you.</p>
<hr>
<aside data-type="sidebar">
<div id="_ftn1">
<a href="#f1"><sup>[1]</sup></a> GUI/windowing system, kernel/operating system, and, if documented, application APIs.</p>
</div>
</aside>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/why-you-should-care-about-robotic-process-automation/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>Unraveling the mystery of code</title>
		<link>https://www.oreilly.com/radar/unraveling-the-mystery-of-code/</link>
				<comments>https://www.oreilly.com/radar/unraveling-the-mystery-of-code/#respond</comments>
				<pubDate>Thu, 21 Nov 2019 05:10:01 +0000</pubDate>
		<dc:creator><![CDATA[Jenn Webb]]></dc:creator>
				<category><![CDATA[Innovation & Disruption]]></category>
		<category><![CDATA[Software Engineering]]></category>
		<category><![CDATA[Deep Dive]]></category>
		<category><![CDATA[Foo Camp '19]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=10236</guid>
				<description><![CDATA[In this interview from O&#8217;Reilly Foo Camp 2019, Adventures in Coderland author Andrew Smith discusses the journey he&#8217;s taken while learning to code. Having started from a place of not knowing anything about coding, Smith says he went much deeper into it than he thought he would. Highlights from the interview include: Smith decided to [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>In this interview from O&#8217;Reilly Foo Camp 2019, <a href="https://andrewsmithauthor.com/books/adventures-in-coderland/"><em>Adventures in Coderland</em></a> author Andrew Smith discusses the journey he&#8217;s taken while learning to code. Having started from a place of not knowing anything about coding, Smith says he went much deeper into it than he thought he would.</p>



<p>Highlights from the interview include:</p>



<p>Smith decided to write a book about what he&#8217;s learned, a sort of &#8220;I&#8217;m learning to code so you don&#8217;t have to&#8221; kind of tome. &#8220;I understood nothing about [coding] at all, about how it works, or what a line of code means,&#8221; he explained. &#8220;How does this thing that looks like algebra make something happen over there? To most of us, that&#8217;s a complete mystery. Given that we&#8217;re beginning to have a lot of conversations about what we think code should do, and, more crucially, what it shouldn&#8217;t do, I felt I needed to be informed about it to participate in those conversations. I thought, &#8216;Well, actually, so does everyone.&#8217; People aren&#8217;t going to all learn to code, but I could do it on their behalf, so they don&#8217;t have to.&#8221; (<a href="#t=15">00:15</a>)</p>



<p>In learning to code, Smith discovered he worked best with Python because he found it very approachable and clear. The biggest challenge, he noted, wasn&#8217;t so much the language itself, but having to learn a new way of thinking. &#8220;What you&#8217;re trying to do is think the way a computer thinks. That&#8217;s the really big hurdle, and that&#8217;s the difficult thing. &#8230; I&#8217;m anthropomorphizing, but trying to get into the mindset of a computer, something that we would do in our analog way very quickly by saying a sentence might take three, four, five, or six steps to communicate to the computer.&#8221; (<a href="#t=385">06:25</a>)</p>



<p>Smith has also discovered a cross-over between code and his interest in music. &#8220;Recently when I was in the UK, I got involved with live coding—the program I use is called Sonic Pi, which basically turns your laptop into a musical instrument. &#8230; In the UK, &#8216;algorave&#8217; has become a small underground movement in nightclubs. The performer live codes the music and there&#8217;s a screen behind them that shows the code.&#8221; (<a href="#t=1084">18:04</a>)</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/unraveling-the-mystery-of-code/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>Improving the military UX</title>
		<link>https://www.oreilly.com/radar/improving-the-military-ux/</link>
				<comments>https://www.oreilly.com/radar/improving-the-military-ux/#respond</comments>
				<pubDate>Tue, 12 Nov 2019 05:10:49 +0000</pubDate>
		<dc:creator><![CDATA[Jenn Webb]]></dc:creator>
				<category><![CDATA[Innovation & Disruption]]></category>
		<category><![CDATA[Deep Dive]]></category>
		<category><![CDATA[Foo Camp '19]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=9575</guid>
				<description><![CDATA[In this interview from O&#8217;Reilly Foo Camp 2019, Noah Firth, director at the Air Force Digital Service, talks about the challenges of making the US military more user-friendly, the unique recruiting parameters for team members, and the success metrics that guide his team. Highlights from this interview include: Firth&#8217;s team&#8217;s main focus is on how [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>In this interview from O&#8217;Reilly Foo Camp 2019, Noah Firth, director at the Air Force Digital Service, talks about the challenges of making the US military more user-friendly, the unique recruiting parameters for team members, and the success metrics that guide his team.</p>

<p>Highlights from this interview include:</p>

<p>Firth&#8217;s team&#8217;s main focus is on how technology can be used to improve the lives of US military service members and their families. He describes the significance of a large project they&#8217;re wrapping up, move.mil. &#8220;Military service members have to move every two to four years, and the system that was enabling those moves was generally hostile toward the user experience. We&#8217;re working to use modern technologies to implement a new-and-improved system to schedule the moves to allow service members&#8217; spouses and family members to be active participants in figuring out how they&#8217;re going to uproot their lives and move across the country or around the world.&#8221; (<a href="#t=2">00:02</a>)</p>

<p>Recruiting is a challenge for any organization, but it&#8217;s particularly quirky for teams working with government organizations, Firth explains. He looks for folks who are not only at the top of their fields, but for those able to maneuver around the frustrations of slow-moving bureaucracy. &#8220;We have what we call core competencies&mdash;questions we ask and things we look for to determine whether or not someone&#8217;s going to be a good fit in our organization. &#8230; Some of those things that we call &#8217;emotional quotient&#8217; are as important if not more so than some of the technical quotient things we&#8217;re looking for. &#8230; We have to go through an enormous amount of validation of a person&#8217;s prior experiences, but then also a validation of them as a human. It gives us the ability to do the whole person concept, to make sure that, as we have a small team and we have to maintain a small team, we can work to make that small team work well together.&#8221; (<a href="#t=57">00:57</a>)

<p>One might think it wouldn&#8217;t be difficult, Firth notes, but one of the main success metrics he uses is whether or not they can convince others that technology can make their lives better. Also in Firth&#8217;s case, the military leadership he deals with turns over every two to four years; all around, he&#8217;s on a &#8220;constant education campaign.&#8221; Firth outlines how he measures overall success: &#8220;Have we made the user&#8217;s life better? Have we used technology to do so in a way that appreciates (1) the capability of the technology, but also (2), the capability of the people who are implementing that technology and will have to do so long term? Then (3), have we walked away leaving behind us a legacy of creating that understanding of what technology could be used for?&#8221; (<a href="#t=198">03:18</a>)</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/improving-the-military-ux/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>Bitcoin and the disruption of monetary oppression</title>
		<link>https://www.oreilly.com/radar/bitcoin-and-the-disruption-of-monetary-oppression/</link>
				<comments>https://www.oreilly.com/radar/bitcoin-and-the-disruption-of-monetary-oppression/#respond</comments>
				<pubDate>Fri, 08 Nov 2019 05:10:27 +0000</pubDate>
		<dc:creator><![CDATA[Jenn Webb]]></dc:creator>
				<category><![CDATA[Innovation & Disruption]]></category>
		<category><![CDATA[Deep Dive]]></category>
		<category><![CDATA[Foo Camp '19]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=9571</guid>
				<description><![CDATA[In this interview from O&#8217;Reilly Foo Camp 2019, Programming Bitcoin author Jimmy Song talks about why Bitcoin is a profound invention, the impact it&#8217;s already having on society, and its path to monetary relevance. Highlights from the interview include: In the physical world, we have centralized and decentralized scarcity—think numbered prints from artists (centralized) versus [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>In this interview from O&#8217;Reilly Foo Camp 2019, <a href="https://learning.oreilly.com/library/view/programming-bitcoin/9781492031482/"><em>Programming Bitcoin</em></a> author Jimmy Song talks about why Bitcoin is a profound invention, the impact it&#8217;s already having on society, and its path to monetary relevance.</p>

<p>Highlights from the interview include:</p>

<p>In the physical world, we have centralized and decentralized scarcity—think numbered prints from artists (centralized) versus gold or salt or glass beads (decentralized). Until Bitcoin, Song argues, there was no decentralized scarcity in the digital realm. &#8220;Decentralized digital scarcity is a profound invention,&#8221; Song explains &#8220;because it lets you transfer value without an intermediary. There is no permission involved, there&#8217;s no censorability, and that makes it an excellent, excellent [form of] money.&#8221; (<a href="#t=63">1:03</a>)</p>

<p>One of the tangible social impacts of Bitcoin can be witnessed in the human rights arena. As one example, Song offers an overview of the refugee crisis in Venezuela, explaining that Bitcoin is allowing those wishing to flee the country to sell their belongings and retain their money when crossing the border to Columbia. &#8220;There&#8217;s very clear evidence of this,&#8221; Song explains &#8220;because the price of Bitcoin in Columbia is actually lower than everywhere else in the world because there&#8217;s such a big supply. Four million Venezuelans have left. That&#8217;s 10% of their population. That&#8217;s a serious impact. Usually in refugee crises, it has gotten so bad that people were willing to leave everything behind. With this, they get to carry their wealth. It undermines the Maduro government to a large degree.&#8221; (<a href="#t=351">5:51</a>)</p> 

<p>The US&#8217;s market-driven monetary imperialism has led, Song argues, to a sort of global US dollar hegemony—the impact of which is that all global trade is settled in US dollars; if you&#8217;re in Kenya and want to trade with someone in neighboring Nigeria, you have to trade for the US dollar and then back to the Kenyan shilling. Song notes this basically gives the US dominance over everything and allows dictators, who control their economies through their central banks, to use money as a tool of oppression. &#8220;Bitcoin gives you a politically neutral money, one that&#8217;s not dominated by a single country. I say all this about the US dollar, but imagine a world where the Chinese yuan is the dominant currency.&#8221; (<a href="#t=642">10:42</a>)</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/bitcoin-and-the-disruption-of-monetary-oppression/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>Quantum computing’s potential is still far off, but quantum supremacy shows we’re on the right track</title>
		<link>https://www.oreilly.com/radar/quantum-computings-potential-is-still-far-off-but-quantum-supremacy-shows-were-on-the-right-track/</link>
				<comments>https://www.oreilly.com/radar/quantum-computings-potential-is-still-far-off-but-quantum-supremacy-shows-were-on-the-right-track/#respond</comments>
				<pubDate>Fri, 01 Nov 2019 04:05:34 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides]]></dc:creator>
				<category><![CDATA[Innovation & Disruption]]></category>
		<category><![CDATA[Radar Column]]></category>
		<category><![CDATA[Commentary]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=10154</guid>
				<description><![CDATA[One of the most exciting topics we’ve been following is the development of quantum computing. We recently learned about a major breakthrough: Google says it has achieved “quantum supremacy” with a 53-qubit computer. I will steer clear of a lengthy explanation of quantum computing, which I’m not really competent to give. Quantum supremacy itself is [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>One of the most exciting topics we’ve been following is the development of quantum computing. We recently learned about a major breakthrough: <a href="https://www.nature.com/articles/s41586-019-1666-5">Google says it has achieved “quantum supremacy”</a> with a 53-qubit computer.</p>
<p>I will steer clear of a lengthy explanation of quantum computing, which I’m not really competent to give. Quantum supremacy itself is a simple concept: it means performing a computation that could not be performed on a classical computer.</p>
<p>It’s very important to understand exactly what this means. Google performed a computation in a few minutes (3 minutes, 20 seconds to be <a href="https://www.nytimes.com/2019/10/23/technology/quantum-computing-google.html">precise</a>) that would have taken more than 10,000 years on the most powerful computers we currently have. But that’s a speedup for one specific computation, and that computation has no practical value. (<a href="https://medium.com/starts-with-a-bang/has-google-actually-achieved-quantum-supremacy-with-its-new-quantum-computer-b59d051f3d88">This explanation of the computation</a> is the best I’ve seen.) Google has verified that the result is correct—a statistical distribution that is subtly different from a Gaussian distribution.</p>
<p>This is a major breakthrough, despite <a href="https://www.ibm.com/blogs/research/2019/10/on-quantum-supremacy/">some controversy</a> (though it’s worth pointing out that researchers <a href="https://www.quantamagazine.org/john-preskill-explains-quantum-supremacy-20191002/">John Preskill</a>—who coined the term “quantum supremacy”—and <a href="https://www.scottaaronson.com/blog/?p=4372">Scott Aaronson</a> accept Google’s understanding of quantum supremacy).</p>
<p>It’s important to consider what this achievement does <em>not</em> mean. It does not mean that cryptography is broken, or that we can achieve general artificial intelligence, or anything of the sort. Remember, this result is about one specific computation with no practical value; it’s meaningless, except perhaps as a random number generator that obeys an odd distribution. To break current cryptographic techniques, we’ll need quantum computers with thousands of qubits. And qubits don’t stack up as easily as bytes in a memory chip.</p>
<p>One fundamental problem with quantum computers is that the probability they’ll return an incorrect answer is always non-zero. To do meaningful computation on a quantum computer, we’ll need to develop quantum error correction. Error correction is well understood for classical computers; error correction for quantum computers isn’t. One error-corrected qubit (a “logical” qubit) may require more than a thousand physical qubits. So breaking cryptography, which may require thousands of logical qubits, will require <a href="https://www.scottaaronson.com/blog/?p=4317">millions of physical qubits</a>. Quantum computers of that scale are still a long way off.</p>
<p>Quantum supremacy, now and in some imagined future, also doesn’t mean that digital computers become obsolete. Most of what we do on our computers—fancy graphics, email, databases, building websites, data analysis, digital signal processing—can’t be done with quantum computing. Certainly not now, and possibly never. Quantum computing is useful to speed up a relatively small number of very difficult computational problems that can’t be solved on classical computers. I suspect that quantum computers won’t be computers as such (certainly not laptops, unless you can manage a laptop that runs at temperatures close to absolute zero); they’ll be more like GPUs, specialized attachments that run certain kinds of computations.</p>
<p>I also suspect that, for quantum computers, Thomas J. Watson’s notorious (and perhaps <a href="https://en.wikipedia.org/wiki/Thomas_J._Watson#Famous_attribution">apocryphal</a>) prediction that the total market for computers would be five, might be close to the truth. But unlike Watson, I can tell you where those quantum computers will be: they will live in the cloud. Google, IBM, Amazon, and Microsoft will each have one; a few more will be scattered around at intelligence agencies and other organizations with three-letter names. The total market might end up being a few dozen—but because of the cloud, that will be all we need. Don’t expect a quantum computer on your desktop. It’s possible that some breakthrough in physics will make quantum computing units as common as GPUs—but that breakthrough isn’t even close to the horizon.</p>
<p>So, after all that cold water, why is Google’s achievement important? It’s important because it is what it says it is: A computation that would have taken more than 10,000 years on the fastest modern supercomputer has been done in a few minutes. It doesn’t matter that the computation is meaningless, and it doesn’t matter that scaling up to meaningful problems, like breaking cryptography, is likely to take another 10 to 20 years. Google has proven that it is possible to build a quantum computer that can perform computations of a complexity that isn’t conceivable for traditional computers. That’s a huge step forward; it proves that we’re on the right track.</p>
<p>Even though the computation Google has performed doesn’t have any applications, I wouldn’t be surprised if we can find useful computations that can be done on our current quantum computers, with 50 to 100 qubits. Random number generation is itself an important problem; quantum computers can be testbeds for researching quantum mechanics, and there are quantum algorithms for determining whether a message has been read by a third party. (And while these applications depend on the quantum nature of qubits, they don’t require “quantum supremacy” as such.) Utility is all a matter of perspective. I was introduced to programming in 1972, on computers that were incredibly small by modern standards—but they were still useful. And the first IBM mainframes of the 1950s were small even by the standards of 1972, but they did useful work. Scaling up took, literally, 60 years, but we did important work along the way. It’s easy to dismiss a 53-qubit quantum machine from the perspective of a laptop with 64 GB of RAM and a terabyte disk, but that’s like looking through the wrong end of a telescope and complaining about how small everything is. That’s not how the industry progresses.</p>
<p>Scaling quantum computing isn’t trivial. But the most important problem, getting these things to work in the first place, has been solved. Yes, there have been some small quantum computers available; IBM has quantum computers <a href="https://www.ibm.com/quantum-computing/technology/experience/?lnk=hm">available in the cloud</a>, including a 5-qubit machine that anyone can try for free. But a real machine that can achieve a huge speedup on an actual calculation—nobody knew, until now, that we could build such a machine and make it work.</p>
<p>That is very big news. <em>— Mike Loukides</em></p>


<hr class="wp-block-separator" />



<h2>Data points: Recent research and analysis</h2>



<p>Our analysis of <a href="https://www.oreilly.com/radar/whats-driving-cloud-native-and-distributed-systems-in-2019/">speaker proposals from the 2019 edition of the O’Reilly Velocity Conference in Berlin</a> turned up several interesting findings related to infrastructure and operations:</p>



<ul><li><strong>Cloud native is preeminent</strong>. The language, practices, and tools of cloud native architecture are prominent in Velocity Berlin proposals. From the term “cloud native” itself (No. 25 in the tally of the highest weighted proposal terms) to foundational cloud native technologies such as “Kubernetes” (No. 2), cloud native is coming on strong.</li><li><strong>Security is a source of some concern</strong>. The term “security” not only cracked the top 5 terms, it surged to No. 3, following Kubernetes. This suggests that even as cloud native comes on strong, there’s a degree of uncertainty—and perhaps also uneasiness—about how to secure the new paradigm.</li><li><strong>Performance is still paramount</strong>. Architects, engineers, and developers are using new tools, metrics, and even new concepts, to observe, manage, and optimize performance. This is as much a shift in language—with the terms “observability” rising and “monitoring” falling—as in technology.</li><li><strong>Site reliability engineering (SRE) is growing</strong>. Terms associated with SRE continue to ascend the rankings. SRE is a very different way of thinking about software development. Our analysis suggests SRE-like terms, concepts, and practices are beginning to catch on.</li><li><strong>Europe and the United States are different regions</strong>—and it isn’t just the metric system. For example, “observability” is a thing in Europe, but seems to be a slightly bigger thing in the US. It’s one of several terms that tend to be more popular on one side of the pond than on the other. Another term, oddly enough, is “cloud native,” which is more popular in the EU than the US.</li></ul>



<p>Check out “<a href="https://www.oreilly.com/radar/whats-driving-cloud-native-and-distributed-systems-in-2019/">What’s driving cloud native and distributed systems in 2019</a>” for full results from our analysis of Velocity Berlin &#8217;19 proposals.</p>



<hr class="wp-block-separator" />



<h2>Upcoming events</h2>



<p><a href="https://www.oreilly.com/conferences">O’Reilly conferences</a> combine expert insights from industry leaders with hands-on guidance about today’s most important technology topics.</p>



<p>We hope you’ll join us at our upcoming events:</p>



<p><a href="https://conferences.oreilly.com/software-architecture/sa-eu">O’Reilly Software Architecture Conference in Berlin</a>, November 4-7, 2019</p>



<p><a href="https://conferences.oreilly.com/velocity/vl-eu">O’Reilly Velocity Conference in Berlin</a>, November 4-7, 2019</p>



<p><a href="https://conferences.oreilly.com/software-architecture/sa-ny">O’Reilly Software Architecture Conference in New York</a>, February 23-26, 2020</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/quantum-computings-potential-is-still-far-off-but-quantum-supremacy-shows-were-on-the-right-track/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>Defusing propaganda feedback loops on the social web</title>
		<link>https://www.oreilly.com/radar/defusing-propaganda-feedback-loops-on-the-social-web/</link>
				<comments>https://www.oreilly.com/radar/defusing-propaganda-feedback-loops-on-the-social-web/#respond</comments>
				<pubDate>Wed, 30 Oct 2019 04:02:26 +0000</pubDate>
		<dc:creator><![CDATA[Mike Loukides]]></dc:creator>
				<category><![CDATA[Innovation & Disruption]]></category>
		<category><![CDATA[Commentary]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=10213</guid>
				<description><![CDATA[Feedback isn’t just the grit-your-teeth, high-pitched squeal you get when you put a microphone too close to a speaker, though that’s the form many of us are most familiar with. Music is what you hear at a concert when feedback is under control. The squeal is out-of-control feedback. To an electrical engineer, however, feedback systems [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>Feedback isn’t just the grit-your-teeth, high-pitched squeal you get when you put a microphone too close to a speaker, though that’s the form many of us are most familiar with. Music is what you hear at a concert when feedback is under control. The squeal is out-of-control feedback.</p>
<p>To an electrical engineer, however, feedback systems are filters and amplifiers.</p>
<p>Knowing how that control happens, and how it fails, will help us to understand the breakdown of social media.</p>
<p>Here’s what’s happening when you hear that squeal. A microphone picks up the sound waves that reach it. That sound is amplified and comes out of the speaker system. Feedback happens when the sound coming out of the speakers is added to the other sounds already present at the microphone.</p>
<p>One set of frequencies (those that arrive at the microphone in phase) is amplified at the expense of others, drowning out everything else. That’s why when you hear feedback, you only hear a single pitch, not an entire soundscape.</p>
<p>And that’s why the pitch changes if you move the microphone. The feedback system is a filter: it amplifies one set of frequencies; and everything else disappears. There’s no music, just a spine-chilling scream.</p>
<h2>Feedback loops</h2>
<p>There’s an important lesson here. By controlling the properties of the feedback loop—in this case, the distance between the speaker and the microphone—you can control the system’s output.  It takes very little energy: the feedback loop doesn’t power the amplifier. There’s no obvious point of control. The feedback loop commandeers the sound system and the engineers, with all their knobs and dials, have to fight to bring the system back under control. Until they succeed, you just have to sit back and listen to the noise.</p>
<p>Network propaganda works in much the same way as microphones and music. Propaganda isn’t a new phenomenon. It has existed for ages, probably going back to the dawn of consciousness—certainly to the dawn of political competition.</p>
<p>Only now the mechanics, the mechanisms used to spread disinformation, have changed.</p>
<p>One key to understanding these changes is to see propaganda as a feedback loop—not simply one-way propagation. Feedback is subtle. It isn’t regulated by a central authority. It can be controlled from the outside with a gentle touch. And once started, it can maintain itself. Control the feedback, and you control the news. You’re like the <a href="https://thehill.com/homenews/campaign/461216-harris-prompts-laughter-by-comparing-trump-to-small-dude-behind-curtain-in">man behind the curtain</a> in <i>The Wizard of Oz</i>: Nobody pays attention—unless a plucky heroine and a cowardly lion (et al) get involved.</p>
<p>We saw those feedback loops in 2016, we’ve been seeing them ever since, and we will certainly see them used in 2020. Seed a group with an opinion, use <a href="https://www.theguardian.com/books/2018/feb/23/troll-steven-poole-word-of-week">sockpuppet accounts</a> to amplify responses, and sooner or later, you have a full-fledged feedback loop, living on its own knee-jerk reflexes. Clickbait titles that encourage people to repost without actually reading the content help to get the cycle started. As posts and re-posts arrive faster and faster, the squeal of feedback rises, and it becomes impossible to pay attention to anything else.</p>
<p>For the last few years, we’ve been trapped in a nightmare of feedback loops, where abuse and fake news feed on more abuse and fake news to produce noise. The noise can be targeted precisely at people, at issues, and at organizations. It quickly makes rational discussion impossible; that’s its point.</p>
<p>This process has been weaponized by bad actors, both domestic and international, ranging from <a href="https://www.nbcnews.com/feature/digital-docs/video/fake-news-how-this-teenager-in-macedonia-is-striking-it-rich-828602947943">teenagers in Macedonia</a> to informally organized networks of “patriots” whose government ties can always be denied (a phenomenon discussed a decade ago in <a href="https://learning.oreilly.com/library/view/inside-cyber-warfare/9781449318475/">Jeffrey Carr’s <i>Inside Cyber Warfare</i></a>), to professional groups like Russia’s <a href="https://www.niemanlab.org/reading/inside-the-internet-research-agency-a-russian-troll-farm/">Internet Research Agency</a> that can mount very sophisticated attacks.</p>
<p>The content of the howl isn’t even important, except to the extent that it destroys the possibility for normal conversation. We’ve seen feedback loops built around every ideology on the right and on the left. To the bad actor feeding the loop, it isn’t the thought that counts; the only important thing is that thought can no longer take place.</p>
<p>Participants in the conversation can either leave, or they can be paralyzed.</p>
<p>While many of our systems are vulnerable to attack, social networks have proven the most vulnerable. There’s no sense in attacking a hard target when a weaker one is available. Attacks on social networks can fly under the radar as “free speech,” even when the speaker is a bot that’s been programmed to manipulate a feedback loop.</p>
<p>The question, then, is how are we to build systems that are resilient against attack, systems in which feedback loops don’t form?</p>
<h2>Don’t feed the trolls</h2>
<p>There are several solutions for managing feedback in electronics. You can change the properties of the feedback path itself, disrupting the loop before it starts. Sometimes simply moving a microphone changes the path enough to prevent feedback. Or you can introduce “negative feedback” to cancel out the feedback signal. It’s important that neither approach represses feedback frequencies entirely; they aren’t holes in the system’s frequency response. Your goal is to allow <i>all</i> voices to be heard.</p>
<p>Perhaps a more useful way to frame the question is: How do we prevent feedback in our social networks without repressing certain conversations over others?</p>
<p>Many of us have been “free speech absolutists,” and the notion of repressing speech is painful, to say the least. But we’ve also learned that too much speech is a way of repressing speech. When the sound system is howling in a feedback loop, you can’t hear anything, and you can’t say anything. You can’t hear the performers, you can’t even hear your noisy neighbor. In the world of social media especially, preserving the right and ability to pay attention is more valuable than preserving some abstract notion of “speech.” Speech is meaningless when no one can listen.</p>
<p>For the most part, we don’t have a politburo that tells state-sponsored news agencies what news is fit to print. What we do have are systems that use feedback to control what people are exposed to and that shape how they are able to respond.</p>
<p>We already have some tools for attenuating feedback in our social networks. Speech doesn’t entail the right to amplification. Bots and sockpuppets are old by now and may have passed peak usefulness, but getting them off social networks is an important first step.</p>
<p>Limiting likes, <a href="https://www.buzzfeednews.com/article/alexkantrowitz/how-the-retweet-ruined-the-internet">retweets</a>, and similar tools would also be useful. Instagram is experimenting with <a href="https://www.sciencealert.com/instagram-has-a-trial-to-hide-the-number-of-likes-in-the-hope-that-it-saves-self-esteem">eliminating likes</a>, saying users should focus on the videos and pictures, not on peer approval. On Twitter, I use retweets as much as anyone, but I’d gladly sacrifice that to create room for others to think. Structuring social media to require participating in discussions, rather than simply reflecting and amplifying others’ opinions, would be progress.</p>
<p>“<a href="http://theconversation.com/dont-feed-the-trolls-really-is-good-advice-heres-the-evidence-63657">Don’t feed the trolls</a>” is advice that goes back to <a href="https://en.wikipedia.org/wiki/Usenet">Usenet</a> and the 1980s, but it’s still applicable here: Responding might feel satisfying, but it accomplishes nothing besides helping that feedback loop to start. And once it starts, it’s very hard to stop.</p>
<p>Defending people who are victimized by the feedback loop is helpful and necessary, but these responses have to be careful. Even mentioning trolls does nothing but encourage them. <a href="https://academic.oup.com/jcmc/article/10/4/JCMC1048/4614533">Research</a> on <a href="https://www.cmu.edu/joss/content/articles/volume8/Welser/">Usenet</a> has shown that participants can be classified as questioners, helpers, trolls, and flamers based merely on their posting patterns.</p>
<p>De-platforming people whose behavior is unacceptable is an age-old form of feedback control. Ages ago, it was called shunning or exile. Denying abusers a voice: It isn’t pleasant, but it is necessary.</p>
<h2>Platform alternatives</h2>
<p>Alternative venues, such as <a href="https://www.greatamericandebate.org/">The Great American Debate</a>, are another way of defusing the feedback loop. The Great American Debate is an experiment in providing a platform for reasoned, fact-based, non-partisan political discussion. The goal is to provide maps of important issues, starting with climate change. These maps will present all aspects of all sides the discussion, making it impossible to listen to one voice without hearing the others. If a feedback loop is a filter that eliminates all voices but one from the discussion, this platform is an anti-filter that disarms the loop before it starts.</p>
<p>Projects like <a href="https://www.scuttlebutt.nz/getting-started">Scuttlebutt</a> attempt to reinvent the online experience as a safe space. If our current social media won’t adapt to manage feedback, it’s time to invent new media. That’s not an easy task, but 15 years ago, nobody thought that <a href="https://www.nytimes.com/2019/03/19/business/myspace-user-data.html">MySpace</a> and <a href="https://theprint.in/economy/brandma/orkut-the-site-where-indians-made-frandships-before-facebook-came-along/244499/">Orkut</a> would be replaced by Facebook, Twitter, and LinkedIn. It can certainly happen again.</p>
<p><a href="https://www.wired.com/story/creating-ethical-recommendation-engines/">Maximizing engagement</a> to drive advertising revenue is at the heart of the feedback problem, as media expert Renée DiResta has pointed out. It is the mechanism that makes our media so easy to subvert. Advertising has always been a key part of the feedback loop; before the invention of social media, it was the only way for media output, modulated by customer response, to become input for the channels that create new media.</p>
<h2>Congratulations, you’re engaged!</h2>
<p>Then, as now, maximizing revenue is the only metric for which media channels are held accountable, and the result is predictable. The engagement metric has become a tool for radicalizing audiences: It leads to content recommendations that are increasingly extreme, just to keep viewers watching, their adrenaline pumping, and their engagement numbers up.</p>
<p>The problem with engagement is that gaming the system is simple: inject some articles or videos with clickbait titles, create some bots to read or watch the content and make it trend, and sit back as the social networks drive the feedback loop via suggested content.</p>
<p>Chris Wiggins of Columbia University’s Data Science Institute argues that organizations need to <a href="https://datascience.columbia.edu/ethical-principles-okrs-and-kpis-what-youtube-and-facebook-could-learn-tukey">take ethical principles into account</a> when formulating their key performance metrics. That isn’t a recommendation that will be welcome to corporations or to their investors; the notion that management is responsible only for maximizing short-term shareholder value is a powerful and convenient way to avoid responsibility. Business models based on subscription, rather than advertising, relieve some (though not all) of the pressure.</p>
<p><a href="https://qz.com/1660737/deepfakes-will-influence-the-2020-election/">Deep fakes</a> are the newest tool for disrupting conversation and starting feedback loops, as we’ve already seen with a <a href="https://www.cnn.com/videos/business/2019/05/24/nancy-pelosi-doctored-video-social-media.cnn-business">doctored video of House speaker Nancy Pelosi</a>. Some <a href="https://www.technologyreview.com/s/613846/a-new-deepfake-detection-tool-should-keep-world-leaders-safefor-now/">progress</a> has been made <a href="https://techcrunch.com/2019/06/10/to-detect-fake-news-this-ai-first-learned-to-write-it/">on detecting deep fakes</a>, but this conflict will certainly escalate, and it’s fair to say that the <a href="https://www.washingtonpost.com/technology/2019/06/12/top-ai-researchers-race-detect-deepfake-videos-we-are-outgunned/?utm_term=.6fb73d85385c">fakers have the advantage</a>. The significance of the Pelosi video isn’t the technology—it’s at best a shallow fake, easily detectable—but that it started the social media feedback loop.</p>
<p>DiResta suggests that fake news (including fake videos) can be handled using the same methods that have <a href="https://www.wired.com/story/the-return-of-fake-news/">successfully tamed spam</a> email: “Nobody mourns the free speech rights of spammers.”</p>
<p>Non-technical solutions to the problem of fakes certainly involve framing. I’ve seen attempts to defend the Pelosi video as parody or satire. Growing up during the Nixon administration, I saw many TV comedians and satirists say “My Fellow Americans…” I doubt anyone was tricked into thinking they were seeing Nixon on the screen. There was always a frame that made it clear who we were seeing. When <a href="https://www.youtube.com/watch?v=3e9iWizfsm8">Nixon actually made a guest appearance on Dan Rowan’s and Dick Martin’s <i>Laugh-In</i></a>, we knew it.</p>
<p>Framing isn’t entirely reliable; a quick Google search turns up many cases where un-doctored images were distorted by a misleading caption. But becoming more aware of how an image is framed, and requiring some kind of framing that shows who posted the image and why, is a start. If nothing else, framing requires the attacker to keep the frame in synch with the fake video and the viewer’s expectations. A fake video requires a fake frame that must align properly with the video. It gives the attacker another chance to slip up.</p>
<p>We require “full disclosure” from reporters, essayists, and journalists. We should also require it from our images and videos. Likewise, as consumers we need to distrust media that lacks framing, or where the frame doesn’t match the content.</p>
<p>The struggle is not about free speech; it’s about the right to pay attention, and to <em>think</em>. If nothing else, this is why the feedback metaphor is appropriate. Feedback isn’t helping you to hear the music at a concert; it’s destroying the ability to hear by amplifying one frequency at the expense of all the others.</p>
<p>And make no mistake about this: In our world of social media, it’s not accidental. It is being managed, and the forces managing it are malignant.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/defusing-propaganda-feedback-loops-on-the-social-web/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>How social forces could drive blockchain demand</title>
		<link>https://www.oreilly.com/radar/how-social-forces-could-drive-blockchain-demand/</link>
				<comments>https://www.oreilly.com/radar/how-social-forces-could-drive-blockchain-demand/#respond</comments>
				<pubDate>Tue, 22 Oct 2019 04:03:59 +0000</pubDate>
		<dc:creator><![CDATA[Alison McCauley]]></dc:creator>
				<category><![CDATA[Innovation & Disruption]]></category>
		<category><![CDATA[Blockchain]]></category>
		<category><![CDATA[Blockchain Enterprise Series]]></category>
		<category><![CDATA[Deep Dive]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=9929</guid>
				<description><![CDATA[Technology does not make a market. People make a market. It’s only when millions of people make individual decisions to use a new product or service that a technology takes hold. That is why it’s critical to examine broader social forces when trying to understand how quickly a new technology will be adopted—and to understand [&#8230;]]]></description>
								<content:encoded><![CDATA[
<p>Technology does not make a market. People make a market. It’s only when millions of people make individual decisions to use a new product or service that a technology takes hold. That is why it’s critical to examine broader social forces when trying to understand how quickly a new technology will be adopted—and to understand why blockchain technology is <a href="https://www.oreilly.com/radar/fit-and-value-the-business-case-for-blockchain/">building momentum</a> at this particularly potent time.</p>



<p>In the 1980s, scientists at European physics lab CERN were struggling to share and collaborate on their research. Tim Berners-Lee, a contractor with the lab, provided an answer: he created the World Wide Web. He designed this new platform to be permission-less and free, an open space for creativity, innovation, and free expression that transcended geographic and cultural boundaries.</p>



<p>Our world is now 30 years into its internet-driven, digital-centric life. This has reshaped us, from how we run a household to how we do our jobs. It’s changed the architecture of our expectations—of what we expect a friend, colleague, or a business to be able to do. And it has given us unprecedented capability. This power has shaped markets, as businesses clamored to respond to the new customer we have become. Since the dawn of the web, it has moved us to a steadily escalating desire for more accountability, transparency, participation, inclusion, and openness.</p>



<p>But something else also happened, and we are increasingly aware of the implications of what we have created. Power has become concentrated in the hands of a few internet giants, who now wield undue influence. Our digital lives generate heaps of data that propagate beyond our intent and control. Malicious actors have found they can leverage our inability to distinguish real from fake in the digital world, to doctor our perception of reality with ease. How did this happen? A key driver: there was nothing in the internet’s original specs to code direct trust between two participants.</p>



<h2>Enter blockchains</h2>



<p>In this moment of increasing discontent, we’re entering the dawn of the blockchain era. While we are still in the very early days, pioneers—both in the enterprise and in blockchain-first startups—are looking to leverage the technology to address this shortcoming in trust. Blockchains show potential to address key concerns of our digitally driven lives, such as a lack of transparency, accountability, verifiable identity, and <a href="https://www.oreilly.com/radar/fit-and-value-the-business-case-for-blockchain/">control of data</a>. Themes emerging in the work of the early pioneers seeking disruption include:</p>



<ul><li><strong>Deeper transparency</strong>

<br>
Not only are consumers demanding more transparency, but there are two business reasons for it: transparency can help optimize operations, and it can increase customer preference. Blockchains enable a permanent and tamper-proof record of a good’s journey from origin to ultimate destination that anyone in the community can monitor and audit. Trust is based on code, mathematics, and cryptography, instead of having to trust in another party, or rely on central institutions to supervise or direct a relationship. This could ensure sensitive pharmaceuticals are kept at the right temperature, verify the provenance of intellectual property, identify whether fish came from a sustainable source, or find the point at which components “disappeared.” There is even work to explore whether this functionality could help fight fake news.
</li><li><strong>Self-sovereign data</strong>
<br>
There is an assumption today that organizations that collect data own it, and have unfettered access to its use, as long as it’s within the realm of privacy policies (which are rarely read by users). In the vision of many pioneers in the blockchain space, the “ownership&#8221; of data would switch from the organization that gathers it to the individual or organization that contributed it. In this vision, data essentially would be “vaulted” (as in a black box) and decisions to use or sell that data would be controlled by the contributor. Access could be rescinded at any time. There are projects that are exploring versions of social media, ride-sharing, and other platforms that use this approach. But the story extends beyond the individual. With <a href="https://www.oreilly.com/ideas/you-created-a-machine-learning-application-now-make-sure-its-secure">new cryptographic techniques</a> that can enable analysis on the data without actually “seeing” it, data may one day be accessible outside of the corporate silos in which it currently resides. Imagine how our concept of big data could get bigger, for example, if the health care industry could safely share data across organizations, or how many more organizations could benefit from AI if more training data were available.
</li><li><strong>More equitable sharing of value</strong>
<br>
Today, we freely give data, content, and other forms of value in exchange for the use of “free” products. In the vision of blockchain pioneers, the technology can be used as the plumbing to facilitate a shift to a more collaborative relationship between business and customer. We can verify a contribution has come from a specific identity, track how it’s used over time, and then compensate accordingly. This represents a paradigm shift—these (unproven) business models are no longer a zero-sum game, but about creating value together. As the space evolves, innovators are likely to eventually identify viable alternatives that embody this paradigm. As consumers are exposed to these alternatives, they will be trained to recognize the true value of their contribution and will be influenced by which brands or businesses compensate them most fairly for the value they help create.
</li></ul>



<h2>Where does change begin?</h2>



<p>On the cusp of surpassing baby boomers as the nation’s largest living adult generation, millennials are a massive force. In the United States, they are also becoming the wealthiest: over the next 30 years and as they are entering their prime earning years, millennials will inherit $30 trillion from their baby boomer parents and grandparents. Receptive and quick to adopt new technologies, millennials, through their influence and demands, have already played a huge role in shaping the way we shop, work, and live. They may very well be one of the first—and highly influential—major segments to adopt blockchain-fueled development. A recent <a href="https://www.pewsocialtrends.org/2014/03/07/millennials-in-adulthood/">Pew Research Center study</a> found that just 19% of millennials (those born from 1981 to 1996, according to Pew Research) feel that “most people can be trusted.”</p>



<p>Disruption could also be spurred by an even younger generation. <em>New York Times</em> writer David Brooks traveled to college campuses to understand how students see the world. In a story he wrote after the experience, starkly titled “<a href="https://www.nytimes.com/2018/02/26/opinion/millennials-college-hopeful.html">A Generation Emerging from the Wreckage</a>,” Brooks describes a cohort with diminished expectations. Their lived experience includes the Iraq war, the financial crisis, police brutality, political fragmentation, and the advent of fake news as a social force. In short, an entire series of important moments in which “big institutions failed to provide basic security, competence, and accountability.” To this cohort in particular, blockchains’ promise of decentralization, with its built-in ability to ensure trust, is tantalizing. To circumvent and disintermediate institutions that have failed them is a ray of hope—as is establishing trust, accountability, and veracity through technology, or even the potential to forge new connections across fragmented societies.</p>



<p>This latent demand is well aligned for the promise of blockchains. While it’s a long road to maturity, these social forces provide a receptive environment, primed and ready for the moment entrepreneurs strike the right formula.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/how-social-forces-could-drive-blockchain-demand/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>Fit and value: The business case for blockchain</title>
		<link>https://www.oreilly.com/radar/fit-and-value-the-business-case-for-blockchain/</link>
				<comments>https://www.oreilly.com/radar/fit-and-value-the-business-case-for-blockchain/#respond</comments>
				<pubDate>Tue, 24 Sep 2019 04:02:52 +0000</pubDate>
		<dc:creator><![CDATA[Alison McCauley]]></dc:creator>
				<category><![CDATA[Innovation & Disruption]]></category>
		<category><![CDATA[Blockchain]]></category>
		<category><![CDATA[Blockchain Enterprise Series]]></category>
		<category><![CDATA[Deep Dive]]></category>

		<guid isPermaLink="false">https://www.oreilly.com/radar/?p=9353</guid>
				<description><![CDATA[Blockchains have a uniquely tumultuous early history for an enterprise technology—from a mysterious origin story, to a sensational first application in bitcoin, to a swift fall from a particularly frothy hype cycle. Blockchain technology provides the encrypted distributed ledger that made the first cryptocurrency, bitcoin, possible. Over the last decade, entrepreneurs and intrapreneurs around the [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>Blockchains have a uniquely tumultuous early history for an enterprise technology—from a mysterious origin story, to a sensational first application in bitcoin, to a swift fall from a particularly frothy hype cycle. Blockchain technology provides the encrypted distributed ledger that made the first cryptocurrency, bitcoin, possible. Over the last decade, entrepreneurs and intrapreneurs around the world have taken that initial spark and reimagined blockchain by applying the technology broadly across many industries for a variety of purposes. Sitting in the second half of 2019, we see a new chapter unfolding of the blockchain story as early enterprise adopters transition pilots into production. Executives and decision makers can now better see the promise blockchain holds to improve business processes and facilitate creating innovative products, and, in some cases, entirely new markets.</p>
<h2>Enterprise momentum builds for blockchain</h2>
<p>Some examples may help. The members of Tradelens, a blockchain joint venture between IBM and the shipper Maersk, control more than 60% of the world’s containership capacity. Food Trust is a blockchain ecosystem that covers more than 100 organizations, including Carrefour and the top four grocery retailers in the U.S. And Facebook, which has almost a third of the world’s population as users, recently made waves by announcing plans for a blockchain-driven currency with 27 other brands— including Visa, Uber, and Spotify—named as members of the governing association.</p>
<p>Blockchains begin at a subterranean level, which makes progress difficult to see. Facebook’s announcement aside, the enterprise projects with more public visibility tend to be lodged in the deep back office—markedly mundane on the surface, but a foundation that could be leveraged to one day impact the way we work and live. Many other teams are working behind closed doors on complex, multi-year, and potentially disruptive projects. These organizations can now harvest key lessons on where the technology holds promise—and, importantly, where it fails. This is knowledge they can leverage to assert power in the new, more collaborative ecosystems that the technology makes possible.</p>
<h2>Where is the strategic business value?</h2>
<p>Blockchains are a multi-function, multi-use tool. Much like the Internet, how you use the technology depends on your perspective and objectives. But at a 30,000-foot level, there are two areas of strategic focus that cover the vast majority of enterprise blockchain projects:</p>
<ol>
<li>An ecosystem using blockchains to cut friction (such as cost, accuracy, or time) in business processes. What these projects ask is: <strong>what can be <em>fixed</em> by working together?</strong></li>
<li>An ecosystem using blockchains to achieve a new depth of collaboration that can enable new products and services—or even the co-creation of new markets. What these projects ask is: <strong>what can we <em>create </em>by working together?</strong></li>
</ol>
<h2>What do blockchains do for enterprises?</h2>
<p>Explanations of blockchains tend to get complicated quickly. However, in very basic language, b<em>lockchains help us certify that something is true, without someone in the middle doing checks and balances. </em></p>
<p>Today’s businesses interface with vast global ecosystems of suppliers, distributors, shippers, logistics providers, strategic partners, and financers in order to deliver their products and services. While a few may be in a trusted inner circle, businesses do not simply take each other’s word. They require proof that goods have been received, quality standards have been met, money has been sent, compliance is adhered to, data has not been used improperly, and so many other parameters (as do auditors and regulators). To achieve this, businesses have built up complex processes of checks and balances. They use middlemen like escrow services and brokers. They are still plagued by mountains of paper. And large numbers of people work to verify truth and resolve discrepancies.</p>
<p>Blockchains offer an alternative. You can think of a blockchain as a platform that no one player controls or owns, and that permanently and immutably records what has happened in an ecosystem. It is decentralized, replicated on a vast number of computers, and secured by cryptography and game theory. This enables us to replace the paper trails and middlemen that we use to verify and audit truth with code. Businesses across a vast range of industries are adapting the technology to their operating needs.</p>
<p>Because blockchain is code, it brings a rich array of attractive functionality. For example, consider what could become possible when an Internet of Things (IoT) device writes to the blockchain.; IoT devices can function as a digital nervous system, tracking conditions that goods experience on their journey from a supplier. Were they kept at the proper temperature and pressure? Was there a leak, magnetic disturbance, or moisture? This can be recorded immutably as it happens.</p>
<p>But possibly one of the most intriguing functions of blockchain is the ability to embed terms and conditions into the movement of money or other forms of value through smart contracts. For example, payment could be withheld automatically if those goods were not properly cared for during their journey. Or a bonus could be automatically released if the shipment arrives early. However, smart contracts are programs, vulnerable to bugs. And, we haven’t yet seen how the legal system will respond to code-driven contracts. While this area has captured imaginations with its widespread applicability, the space needs to mature a great deal before we can confidently tie large amounts of value to smart contracts.</p>
<h2>Where could blockchains influence business strategies?</h2>
<p>With a basic understanding of what blockchains do, it’s not too difficult to see how they could cut operational friction across industries. But how can organizations use blockchains to create new products, services, or even markets, together?</p>
<p>When technology makes it possible to dramatically cut costs, it unleashes new business models. Wide and inexpensive access to the Internet dramatically cut the cost of communication, and smartphones dramatically cut the cost of ubiquitous computers; these advancements released disruptive business models that would have been hard to imagine in the early 1990s. Over time and with increased maturity, blockchains will bring about new business models based on cutting the cost of verifying truth.</p>
<p>While it’s difficult today to really project where blockchains could go, there are themes that are starting to emerge. There are <a href="https://venturebeat.com/2019/03/10/self-driving-cars-could-soon-navigate-the-world-via-micropaymets/">new explorations in machine-to-machine micropayments</a> in exchange for data or rights of way. There is experimentation in areas like micro-insurance (insuring in smaller increments of time and space, for example, based on the number of passengers or value of goods transported) and micro-credentials (universal and easy verification of a skill learned, for example). A great deal of work is going into breaking illiquid assets (like art and real estate) into smaller parts that can be more easily used or traded—an area that could trigger new evolution not only in financial services, but in the sharing economy.</p>
<p>We are still very much in the early days of blockchain technology. Executives are faced with the difficult decision of letting others take on the significant expense of testing use cases and the market—or making investments of uncertain return. While there’s still not clarity or established best practices, more teams are making the choice to learn about and invest in the technology. And as blockchain savvy teams around the globe <a href="https://www.forbes.com/sites/alisonmccauley/2019/05/23/is-collaboration-the-new-competition-blockchains-flip-conventional-mba-wisdom/#7146a82152da">build and gather allies</a>, projects that appear to be mundane could quite quickly build to become competitive advantage.</p>
]]></content:encoded>
							<wfw:commentRss>https://www.oreilly.com/radar/fit-and-value-the-business-case-for-blockchain/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
	</channel>
</rss>
